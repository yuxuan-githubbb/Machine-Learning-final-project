{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "NcDhtxLCV8Tw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, LayerNormalization, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from numpy.random import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "vI12F-E9V8T1",
    "outputId": "869138a1-ef3d-4c37-86c8-e408129de90a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "Xl = pd.read_csv('Xl.csv',header=None)\n",
    "Xs = pd.read_csv('Xs.csv',header=None)\n",
    "R = pd.read_csv('y.csv',header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_agg=R.iloc[::-1].rolling(window=12).sum().iloc[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "Psf8guRAV8T1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_train_start = list(range(46*12))\n",
    "t_train_end =[x+120 for x in t_train_start]\n",
    "t_val_start= [x for x in t_train_end]\n",
    "t_val_end = [x+60 for x in t_val_start]\n",
    "t_test_start = [x for x in t_val_end]\n",
    "t_test_end = [x for x in t_test_start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(552,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(t_test_end) #model needed to be retrained every month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "731"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_test_end[551]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2212 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.4759 - val_loss: 0.1070\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1756 - val_loss: 0.1138\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1579 - val_loss: 0.1206\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3f751a550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2213 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.2700 - val_loss: 0.7252\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.2155 - val_loss: 0.7183\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1330 - val_loss: 0.7044\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0951 - val_loss: 0.6980\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1150 - val_loss: 0.6964\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0888 - val_loss: 0.6919\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0754 - val_loss: 0.6876\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0709 - val_loss: 0.6855\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0583 - val_loss: 0.6817\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0642 - val_loss: 0.6797\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0574 - val_loss: 0.6781\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0700 - val_loss: 0.6759\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0575 - val_loss: 0.6761\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0656 - val_loss: 0.6715\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0653 - val_loss: 0.6697\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0489 - val_loss: 0.6696\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0445 - val_loss: 0.6718\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0485 - val_loss: 0.6712\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3f4d0ee50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2214 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 1.1301 - val_loss: 0.2935\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.6684 - val_loss: 0.2758\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.5427 - val_loss: 0.2617\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.4754 - val_loss: 0.2532\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.3329 - val_loss: 0.2479\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.3676 - val_loss: 0.2427\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.2992 - val_loss: 0.2362\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2476 - val_loss: 0.2318\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.2039 - val_loss: 0.2294\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1885 - val_loss: 0.2265\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1953 - val_loss: 0.2209\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1608 - val_loss: 0.2164\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1537 - val_loss: 0.2152\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1456 - val_loss: 0.2136\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1433 - val_loss: 0.2102\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1326 - val_loss: 0.2092\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1768 - val_loss: 0.2083\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1167 - val_loss: 0.2071\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0955 - val_loss: 0.2065\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.1030 - val_loss: 0.2054\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0812 - val_loss: 0.2046\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0958 - val_loss: 0.2066\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1277 - val_loss: 0.2064\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3f18ab790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2215 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 3.1435 - val_loss: 0.2196\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 1.5210 - val_loss: 0.2088\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.9438 - val_loss: 0.1966\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.8066 - val_loss: 0.1793\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.5671 - val_loss: 0.1765\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.4600 - val_loss: 0.1747\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.4698 - val_loss: 0.1760\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.3486 - val_loss: 0.1781\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ef64e040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2216 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.1812 - val_loss: 0.0324\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1120 - val_loss: 0.0322\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0810 - val_loss: 0.0321\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0524 - val_loss: 0.0321\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0481 - val_loss: 0.0322\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0509 - val_loss: 0.0329\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ee982940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2217 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.1828 - val_loss: 0.0300\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1131 - val_loss: 0.0283\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0944 - val_loss: 0.0267\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0617 - val_loss: 0.0251\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0417 - val_loss: 0.0238\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0500 - val_loss: 0.0229\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0564 - val_loss: 0.0226\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0540 - val_loss: 0.0227\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0456 - val_loss: 0.0232\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4131aa550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2218 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.2353 - val_loss: 0.1503\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.0947 - val_loss: 0.1481\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0896 - val_loss: 0.1481\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0711 - val_loss: 0.1453\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0488 - val_loss: 0.1444\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0417 - val_loss: 0.1419\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0412 - val_loss: 0.1406\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0358 - val_loss: 0.1388\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0298 - val_loss: 0.1371\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0378 - val_loss: 0.1359\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0323 - val_loss: 0.1345\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0394 - val_loss: 0.1344\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0240 - val_loss: 0.1337\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0232 - val_loss: 0.1330\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0332 - val_loss: 0.1320\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0285 - val_loss: 0.1316\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0232 - val_loss: 0.1308\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0270 - val_loss: 0.1295\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0222 - val_loss: 0.1281\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0213 - val_loss: 0.1279\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0276 - val_loss: 0.1268\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0310 - val_loss: 0.1259\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0248 - val_loss: 0.1245\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0194 - val_loss: 0.1233\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0216 - val_loss: 0.1230\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0164 - val_loss: 0.1223\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0272 - val_loss: 0.1212\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0267 - val_loss: 0.1201\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0220 - val_loss: 0.1190\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0277 - val_loss: 0.1169\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0257 - val_loss: 0.1171\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0223 - val_loss: 0.1164\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0198 - val_loss: 0.1153\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0142 - val_loss: 0.1130\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0174 - val_loss: 0.1123\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0170 - val_loss: 0.1108\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0209 - val_loss: 0.1107\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0200 - val_loss: 0.1101\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0237 - val_loss: 0.1086\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0151 - val_loss: 0.1075\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0209 - val_loss: 0.1055\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0307 - val_loss: 0.1059\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0201 - val_loss: 0.1043\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0148 - val_loss: 0.1036\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0118 - val_loss: 0.1030\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0129 - val_loss: 0.1020\n",
      "Epoch 47/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0169 - val_loss: 0.1009\n",
      "Epoch 48/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0140 - val_loss: 0.1002\n",
      "Epoch 49/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0168 - val_loss: 0.0979\n",
      "Epoch 50/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0117 - val_loss: 0.0969\n",
      "Epoch 51/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0137 - val_loss: 0.0959\n",
      "Epoch 52/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0168 - val_loss: 0.0977\n",
      "Epoch 53/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0116 - val_loss: 0.0964\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4005f6c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2219 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.1668 - val_loss: 0.0310\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0990 - val_loss: 0.0310\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0621 - val_loss: 0.0311\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4b1a27790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2220 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 242ms/step - loss: 0.1324 - val_loss: 0.0428\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0940 - val_loss: 0.0431\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0604 - val_loss: 0.0440\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb475497dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2221 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 1s 292ms/step - loss: 0.6840 - val_loss: 0.1255\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.3617 - val_loss: 0.1303\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2858 - val_loss: 0.1372\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb43ed79550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2222 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.8513WARNING:tensorflow:5 out of the last 63 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb3fbb29430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.7672 - val_loss: 0.1285\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4470 - val_loss: 0.1268\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2881 - val_loss: 0.1229\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2684 - val_loss: 0.1187\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2618 - val_loss: 0.1180\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2918 - val_loss: 0.1142\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1989 - val_loss: 0.1125\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2409 - val_loss: 0.1100\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1702 - val_loss: 0.1068\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1346 - val_loss: 0.1057\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1352 - val_loss: 0.1059\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1096 - val_loss: 0.1064\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3f996d700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2223 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.3045 - val_loss: 0.0335\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1199 - val_loss: 0.0334\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1036 - val_loss: 0.0334\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0860 - val_loss: 0.0334\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4176ae1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2224 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 1.1653 - val_loss: 0.0509\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.7576 - val_loss: 0.0499\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.4705 - val_loss: 0.0488\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.4114 - val_loss: 0.0485\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.5272 - val_loss: 0.0470\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.3648 - val_loss: 0.0481\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.2570 - val_loss: 0.0488\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb41437b0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2225 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 158ms/step - loss: 0.4888 - val_loss: 0.0337\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.2860 - val_loss: 0.0344\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2367 - val_loss: 0.0350\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb412e9e8b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2226 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 237ms/step - loss: 0.1712 - val_loss: 0.0408\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.1091 - val_loss: 0.0407\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0903 - val_loss: 0.0405\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0749 - val_loss: 0.0402\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0629 - val_loss: 0.0400\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0541 - val_loss: 0.0399\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0476 - val_loss: 0.0397\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0449 - val_loss: 0.0396\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0418 - val_loss: 0.0396\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0441 - val_loss: 0.0397\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0347 - val_loss: 0.0398\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb40dd4fb80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2227 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.8200 - val_loss: 0.0429\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.5083 - val_loss: 0.0442\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.3195 - val_loss: 0.0450\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb421929c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2228 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.4539 - val_loss: 0.0524\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.2443 - val_loss: 0.0499\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1213 - val_loss: 0.0478\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0944 - val_loss: 0.0468\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0708 - val_loss: 0.0466\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0904 - val_loss: 0.0463\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0498 - val_loss: 0.0457\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0435 - val_loss: 0.0450\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0466 - val_loss: 0.0444\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0677 - val_loss: 0.0437\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0575 - val_loss: 0.0436\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0473 - val_loss: 0.0436\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0475 - val_loss: 0.0438\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb45f364940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2229 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.2608 - val_loss: 0.7415\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1467 - val_loss: 0.7294\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1255 - val_loss: 0.7237\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0731 - val_loss: 0.7147\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0654 - val_loss: 0.7130\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1020 - val_loss: 0.7126\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0691 - val_loss: 0.7156\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0670 - val_loss: 0.7164\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb41a658940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2230 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 0.2100 - val_loss: 0.0565\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1259 - val_loss: 0.0569\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0991 - val_loss: 0.0572\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb432649d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2231 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.3180 - val_loss: 0.1318\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1646 - val_loss: 0.1243\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1268 - val_loss: 0.1168\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1002 - val_loss: 0.1109\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0854 - val_loss: 0.1052\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0746 - val_loss: 0.0992\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0668 - val_loss: 0.0939\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0690 - val_loss: 0.0893\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0626 - val_loss: 0.0851\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0545 - val_loss: 0.0801\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0458 - val_loss: 0.0754\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0463 - val_loss: 0.0713\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0441 - val_loss: 0.0680\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0419 - val_loss: 0.0646\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0343 - val_loss: 0.0610\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0397 - val_loss: 0.0579\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0380 - val_loss: 0.0550\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0363 - val_loss: 0.0523\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0328 - val_loss: 0.0495\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0322 - val_loss: 0.0475\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0292 - val_loss: 0.0459\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0340 - val_loss: 0.0445\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0343 - val_loss: 0.0436\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0245 - val_loss: 0.0431\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0369 - val_loss: 0.0429\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0239 - val_loss: 0.0429\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0296 - val_loss: 0.0425\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0273 - val_loss: 0.0428\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0300 - val_loss: 0.0429\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb427c7dee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2232 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 0.2867 - val_loss: 0.0423\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0926 - val_loss: 0.0436\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0705 - val_loss: 0.0447\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb44d0c7160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2233 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.3423 - val_loss: 0.0403\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1148 - val_loss: 0.0380\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0877 - val_loss: 0.0361\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0618 - val_loss: 0.0345\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0547 - val_loss: 0.0330\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0387 - val_loss: 0.0318\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0335 - val_loss: 0.0308\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0330 - val_loss: 0.0301\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0299 - val_loss: 0.0295\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0350 - val_loss: 0.0291\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0396 - val_loss: 0.0287\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0274 - val_loss: 0.0284\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0335 - val_loss: 0.0280\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0289 - val_loss: 0.0278\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0360 - val_loss: 0.0274\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0262 - val_loss: 0.0271\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0240 - val_loss: 0.0269\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0257 - val_loss: 0.0266\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0233 - val_loss: 0.0263\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0195 - val_loss: 0.0261\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0201 - val_loss: 0.0260\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0234 - val_loss: 0.0260\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0309 - val_loss: 0.0258\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0179 - val_loss: 0.0258\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0201 - val_loss: 0.0258\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb43b50f430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2234 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.3159 - val_loss: 0.0370\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.2060 - val_loss: 0.0348\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1396 - val_loss: 0.0336\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1019 - val_loss: 0.0326\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0867 - val_loss: 0.0319\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0661 - val_loss: 0.0315\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0578 - val_loss: 0.0311\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0565 - val_loss: 0.0313\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0535 - val_loss: 0.0310\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0412 - val_loss: 0.0309\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0408 - val_loss: 0.0307\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0397 - val_loss: 0.0310\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0319 - val_loss: 0.0313\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb444692160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2235 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 0.3373 - val_loss: 0.0359\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2068 - val_loss: 0.0368\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1700 - val_loss: 0.0370\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb43b388d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2236 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.2144 - val_loss: 0.0380\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0770 - val_loss: 0.0387\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0527 - val_loss: 0.0393\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb482c62820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2237 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 0.8087 - val_loss: 0.1334\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.3644 - val_loss: 0.1162\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.3001 - val_loss: 0.1026\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1982 - val_loss: 0.0894\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1547 - val_loss: 0.0779\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1349 - val_loss: 0.0688\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1315 - val_loss: 0.0605\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1305 - val_loss: 0.0532\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1241 - val_loss: 0.0470\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0970 - val_loss: 0.0418\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0949 - val_loss: 0.0375\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0949 - val_loss: 0.0340\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0819 - val_loss: 0.0308\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0779 - val_loss: 0.0284\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0779 - val_loss: 0.0262\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0742 - val_loss: 0.0247\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0603 - val_loss: 0.0239\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0643 - val_loss: 0.0243\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0583 - val_loss: 0.0248\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb45430c430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2238 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.3126 - val_loss: 0.0622\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1317 - val_loss: 0.0627\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0933 - val_loss: 0.0625\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb45f7ff0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2239 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.1951 - val_loss: 0.0531\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1352 - val_loss: 0.0491\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0904 - val_loss: 0.0453\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0829 - val_loss: 0.0429\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0731 - val_loss: 0.0400\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0542 - val_loss: 0.0374\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0508 - val_loss: 0.0352\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0491 - val_loss: 0.0337\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0623 - val_loss: 0.0323\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0432 - val_loss: 0.0313\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0394 - val_loss: 0.0309\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0381 - val_loss: 0.0312\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0334 - val_loss: 0.0314\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4860fbc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2240 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 196ms/step - loss: 0.1530 - val_loss: 0.0585\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0722 - val_loss: 0.0584\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0538 - val_loss: 0.0585\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0608 - val_loss: 0.0586\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb47b69a820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2241 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.6649 - val_loss: 0.0285\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.4026 - val_loss: 0.0291\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.2942 - val_loss: 0.0305\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb479524700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2242 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 0.4065 - val_loss: 0.0348\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.2745 - val_loss: 0.0346\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.2125 - val_loss: 0.0345\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.1751 - val_loss: 0.0349\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1642 - val_loss: 0.0353\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb466e4ab80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2243 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.2932 - val_loss: 0.0352\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1854 - val_loss: 0.0355\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1534 - val_loss: 0.0355\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb47e94adc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2244 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.5697 - val_loss: 0.0570\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.3336 - val_loss: 0.0569\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1429 - val_loss: 0.0542\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1118 - val_loss: 0.0527\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0810 - val_loss: 0.0511\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0796 - val_loss: 0.0494\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0763 - val_loss: 0.0484\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0678 - val_loss: 0.0470\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0618 - val_loss: 0.0465\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0657 - val_loss: 0.0454\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0511 - val_loss: 0.0447\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0537 - val_loss: 0.0446\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0429 - val_loss: 0.0449\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0388 - val_loss: 0.0460\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4395d4940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2245 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.1856 - val_loss: 0.0447\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0886 - val_loss: 0.0453\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0683 - val_loss: 0.0460\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb44291e9d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2246 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.3535 - val_loss: 0.0421\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1540 - val_loss: 0.0390\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1022 - val_loss: 0.0354\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0987 - val_loss: 0.0321\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1305 - val_loss: 0.0333\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0933 - val_loss: 0.0329\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb452d37d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2247 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.1952 - val_loss: 0.0244\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1442 - val_loss: 0.0245\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1189 - val_loss: 0.0241\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0943 - val_loss: 0.0237\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0894 - val_loss: 0.0240\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0619 - val_loss: 0.0238\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb43302bf70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2248 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 1.4801 - val_loss: 0.4334\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.6239 - val_loss: 0.4397\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.3000 - val_loss: 0.4274\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.2283 - val_loss: 0.4272\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.2021 - val_loss: 0.4241\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1650 - val_loss: 0.4252\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1455 - val_loss: 0.4201\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1415 - val_loss: 0.4170\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1440 - val_loss: 0.4198\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1257 - val_loss: 0.4243\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb49db53ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2249 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.2886 - val_loss: 0.0467\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1967 - val_loss: 0.0478\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1504 - val_loss: 0.0491\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb41bf06280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2250 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 0.1919 - val_loss: 0.0358\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1168 - val_loss: 0.0356\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0808 - val_loss: 0.0362\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0667 - val_loss: 0.0368\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb426942ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2251 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 2.2276 - val_loss: 2.1683\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.0397 - val_loss: 2.0498\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.6765 - val_loss: 1.9616\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.6166 - val_loss: 1.8615\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.6285 - val_loss: 1.7909\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3802 - val_loss: 1.7402\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.3775 - val_loss: 1.6772\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2389 - val_loss: 1.6258\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2459 - val_loss: 1.5702\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2029 - val_loss: 1.5265\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1410 - val_loss: 1.4865\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1853 - val_loss: 1.4366\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1335 - val_loss: 1.4026\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1334 - val_loss: 1.3496\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1193 - val_loss: 1.3172\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0943 - val_loss: 1.2870\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0971 - val_loss: 1.2508\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0882 - val_loss: 1.2376\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.076 - 0s 14ms/step - loss: 0.0853 - val_loss: 1.2341\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0668 - val_loss: 1.2239\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0655 - val_loss: 1.2197\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0727 - val_loss: 1.2186\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0651 - val_loss: 1.2078\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0602 - val_loss: 1.2092\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0745 - val_loss: 1.1994\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0453 - val_loss: 1.1994\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0396 - val_loss: 1.2021\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb42bd600d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2252 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.1825 - val_loss: 0.1374\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1138 - val_loss: 0.1350\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0807 - val_loss: 0.1306\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0782 - val_loss: 0.1245\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0637 - val_loss: 0.1212\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0516 - val_loss: 0.1174\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0406 - val_loss: 0.1134\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0463 - val_loss: 0.1072\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0377 - val_loss: 0.0997\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0373 - val_loss: 0.0904\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0369 - val_loss: 0.0813\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0283 - val_loss: 0.0784\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0283 - val_loss: 0.0753\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0276 - val_loss: 0.0715\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0303 - val_loss: 0.0681\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0229 - val_loss: 0.0656\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0236 - val_loss: 0.0622\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0310 - val_loss: 0.0594\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0214 - val_loss: 0.0566\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0163 - val_loss: 0.0539\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0193 - val_loss: 0.0514\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0178 - val_loss: 0.0498\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0275 - val_loss: 0.0486\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0346 - val_loss: 0.0468\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0251 - val_loss: 0.0462\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0288 - val_loss: 0.0463\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0205 - val_loss: 0.0465\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb40b6ab430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2253 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.8310 - val_loss: 0.1325\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.4828 - val_loss: 0.1292\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2331 - val_loss: 0.1229\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1693 - val_loss: 0.1197\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0803 - val_loss: 0.1163\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0669 - val_loss: 0.1131\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0743 - val_loss: 0.1096\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0668 - val_loss: 0.1076\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0541 - val_loss: 0.1049\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0625 - val_loss: 0.1035\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0681 - val_loss: 0.1002\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0475 - val_loss: 0.0980\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0591 - val_loss: 0.0952\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0398 - val_loss: 0.0919\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0404 - val_loss: 0.0888\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0416 - val_loss: 0.0864\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0360 - val_loss: 0.0839\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0548 - val_loss: 0.0813\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0329 - val_loss: 0.0796\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0667 - val_loss: 0.0762\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0733 - val_loss: 0.0744\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0469 - val_loss: 0.0725\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0387 - val_loss: 0.0711\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0395 - val_loss: 0.0697\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0553 - val_loss: 0.0692\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0299 - val_loss: 0.0688\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0395 - val_loss: 0.0688\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0409 - val_loss: 0.0683\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0328 - val_loss: 0.0681\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0448 - val_loss: 0.0675\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0265 - val_loss: 0.0673\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0227 - val_loss: 0.0675\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0275 - val_loss: 0.0672\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0364 - val_loss: 0.0669\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0261 - val_loss: 0.0663\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0295 - val_loss: 0.0660\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0250 - val_loss: 0.0646\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0258 - val_loss: 0.0639\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0305 - val_loss: 0.0625\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0244 - val_loss: 0.0608\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0252 - val_loss: 0.0600\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0210 - val_loss: 0.0591\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0236 - val_loss: 0.0583\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0199 - val_loss: 0.0581\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0301 - val_loss: 0.0577\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0208 - val_loss: 0.0574\n",
      "Epoch 47/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0265 - val_loss: 0.0572\n",
      "Epoch 48/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0237 - val_loss: 0.0566\n",
      "Epoch 49/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0211 - val_loss: 0.0579\n",
      "Epoch 50/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0245 - val_loss: 0.0583\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb44604db80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2254 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.1744 - val_loss: 0.0434\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1092 - val_loss: 0.0427\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0836 - val_loss: 0.0421\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0498 - val_loss: 0.0415\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0602 - val_loss: 0.0413\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0491 - val_loss: 0.0410\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0402 - val_loss: 0.0407\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0334 - val_loss: 0.0404\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0460 - val_loss: 0.0399\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0420 - val_loss: 0.0396\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0386 - val_loss: 0.0389\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0364 - val_loss: 0.0382\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0394 - val_loss: 0.0378\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0298 - val_loss: 0.0371\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0726 - val_loss: 0.0365\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0372 - val_loss: 0.0361\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0260 - val_loss: 0.0358\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0241 - val_loss: 0.0354\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0278 - val_loss: 0.0351\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0282 - val_loss: 0.0348\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0245 - val_loss: 0.0343\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0235 - val_loss: 0.0339\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0256 - val_loss: 0.0338\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0265 - val_loss: 0.0336\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0217 - val_loss: 0.0331\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0347 - val_loss: 0.0332\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0231 - val_loss: 0.0329\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0153 - val_loss: 0.0329\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0211 - val_loss: 0.0323\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0280 - val_loss: 0.0318\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0165 - val_loss: 0.0313\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0290 - val_loss: 0.0308\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0492 - val_loss: 0.0304\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0371 - val_loss: 0.0296\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0243 - val_loss: 0.0289\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0205 - val_loss: 0.0280\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0157 - val_loss: 0.0271\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0183 - val_loss: 0.0264\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0314 - val_loss: 0.0253\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0132 - val_loss: 0.0250\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0206 - val_loss: 0.0243\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0162 - val_loss: 0.0236\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0243 - val_loss: 0.0230\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0168 - val_loss: 0.0226\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0141 - val_loss: 0.0219\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0176 - val_loss: 0.0216\n",
      "Epoch 47/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0200 - val_loss: 0.0213\n",
      "Epoch 48/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0119 - val_loss: 0.0207\n",
      "Epoch 49/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0161 - val_loss: 0.0203\n",
      "Epoch 50/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0195 - val_loss: 0.0200\n",
      "Epoch 51/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0162 - val_loss: 0.0197\n",
      "Epoch 52/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0151 - val_loss: 0.0194\n",
      "Epoch 53/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0142 - val_loss: 0.0192\n",
      "Epoch 54/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0147 - val_loss: 0.0189\n",
      "Epoch 55/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0240 - val_loss: 0.0188\n",
      "Epoch 56/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0186 - val_loss: 0.0185\n",
      "Epoch 57/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0202 - val_loss: 0.0186\n",
      "Epoch 58/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0141 - val_loss: 0.0182\n",
      "Epoch 59/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0242 - val_loss: 0.0182\n",
      "Epoch 60/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0148 - val_loss: 0.0181\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb454371ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2255 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.1979 - val_loss: 0.0263\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1379 - val_loss: 0.0258\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1095 - val_loss: 0.0260\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0898 - val_loss: 0.0260\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb430e86ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2256 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.4011 - val_loss: 0.0535\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2259 - val_loss: 0.0567\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2419 - val_loss: 0.0627\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb41451e670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2257 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.2756 - val_loss: 0.0660\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1643 - val_loss: 0.0666\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1398 - val_loss: 0.0660\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1101 - val_loss: 0.0650\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1048 - val_loss: 0.0654\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0816 - val_loss: 0.0645\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0744 - val_loss: 0.0642\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0709 - val_loss: 0.0634\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0636 - val_loss: 0.0624\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0466 - val_loss: 0.0623\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0561 - val_loss: 0.0622\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0417 - val_loss: 0.0617\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0411 - val_loss: 0.0611\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0366 - val_loss: 0.0611\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0425 - val_loss: 0.0608\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0408 - val_loss: 0.0600\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0457 - val_loss: 0.0600\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0383 - val_loss: 0.0597\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0436 - val_loss: 0.0594\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0345 - val_loss: 0.0590\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0292 - val_loss: 0.0586\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0245 - val_loss: 0.0580\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0205 - val_loss: 0.0572\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0231 - val_loss: 0.0569\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0203 - val_loss: 0.0562\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0223 - val_loss: 0.0560\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0313 - val_loss: 0.0560\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0363 - val_loss: 0.0558\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0273 - val_loss: 0.0565\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0172 - val_loss: 0.0565\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb40c09de50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2258 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.4243 - val_loss: 0.1174\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1708 - val_loss: 0.1126\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1135 - val_loss: 0.1086\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0902 - val_loss: 0.1052\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0826 - val_loss: 0.1019\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0637 - val_loss: 0.0983\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0638 - val_loss: 0.0947\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0468 - val_loss: 0.0923\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0496 - val_loss: 0.0898\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0480 - val_loss: 0.0877\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0448 - val_loss: 0.0856\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0476 - val_loss: 0.0836\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0494 - val_loss: 0.0814\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0348 - val_loss: 0.0795\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0355 - val_loss: 0.0778\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0402 - val_loss: 0.0760\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0376 - val_loss: 0.0744\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0370 - val_loss: 0.0730\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0323 - val_loss: 0.0715\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0358 - val_loss: 0.0703\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0293 - val_loss: 0.0692\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.035 - 0s 20ms/step - loss: 0.0347 - val_loss: 0.0681\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0279 - val_loss: 0.0672\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0308 - val_loss: 0.0663\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0276 - val_loss: 0.0653\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0274 - val_loss: 0.0646\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0254 - val_loss: 0.0640\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0269 - val_loss: 0.0627\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0420 - val_loss: 0.0611\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0307 - val_loss: 0.0600\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0240 - val_loss: 0.0589\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0273 - val_loss: 0.0577\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0241 - val_loss: 0.0568\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0259 - val_loss: 0.0555\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0255 - val_loss: 0.0548\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0254 - val_loss: 0.0543\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0286 - val_loss: 0.0537\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0199 - val_loss: 0.0532\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0245 - val_loss: 0.0526\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0201 - val_loss: 0.0522\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0202 - val_loss: 0.0519\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0258 - val_loss: 0.0511\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0223 - val_loss: 0.0508\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0164 - val_loss: 0.0505\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.0180 - val_loss: 0.0502\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0169 - val_loss: 0.0500\n",
      "Epoch 47/60\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0172 - val_loss: 0.0498\n",
      "Epoch 48/60\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0159 - val_loss: 0.0498\n",
      "Epoch 49/60\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0155 - val_loss: 0.0499\n",
      "Epoch 50/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0181 - val_loss: 0.0497\n",
      "Epoch 51/60\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0176 - val_loss: 0.0499\n",
      "Epoch 52/60\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0157 - val_loss: 0.0502\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ffd4a9d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2259 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 0.4136 - val_loss: 0.0311\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.2290 - val_loss: 0.0282\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.2028 - val_loss: 0.0241\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1324 - val_loss: 0.0232\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0902 - val_loss: 0.0220\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0756 - val_loss: 0.0209\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0661 - val_loss: 0.0203\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0621 - val_loss: 0.0201\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0506 - val_loss: 0.0203\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0489 - val_loss: 0.0209\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4002bc1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2260 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.4842 - val_loss: 0.0450\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.2202 - val_loss: 0.0463\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1746 - val_loss: 0.0471\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb400925820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2261 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.4047 - val_loss: 0.0362\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2566 - val_loss: 0.0365\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.1967 - val_loss: 0.0362\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1738 - val_loss: 0.0357\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1719 - val_loss: 0.0356\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1305 - val_loss: 0.0356\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1063 - val_loss: 0.0359\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.1104 - val_loss: 0.0364\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb401a923a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2262 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 192ms/step - loss: 1.0346 - val_loss: 0.3459\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.5897 - val_loss: 0.3474\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.4165 - val_loss: 0.3509\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb404439ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2263 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.2635 - val_loss: 0.0271\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1853 - val_loss: 0.0264\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1572 - val_loss: 0.0261\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1530 - val_loss: 0.0262\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0925 - val_loss: 0.0263\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb447c03310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2264 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.1952 - val_loss: 0.0716\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1032 - val_loss: 0.0705\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1041 - val_loss: 0.0686\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1109 - val_loss: 0.0671\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0850 - val_loss: 0.0654\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0625 - val_loss: 0.0637\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0578 - val_loss: 0.0622\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0647 - val_loss: 0.0601\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0771 - val_loss: 0.0580\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0432 - val_loss: 0.0566\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0472 - val_loss: 0.0548\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0442 - val_loss: 0.0532\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0446 - val_loss: 0.0523\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0400 - val_loss: 0.0511\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0387 - val_loss: 0.0500\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0335 - val_loss: 0.0490\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0414 - val_loss: 0.0481\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0245 - val_loss: 0.0474\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0345 - val_loss: 0.0467\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0282 - val_loss: 0.0461\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0227 - val_loss: 0.0453\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0268 - val_loss: 0.0448\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0447 - val_loss: 0.0439\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0357 - val_loss: 0.0433\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0290 - val_loss: 0.0425\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0536 - val_loss: 0.0415\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0239 - val_loss: 0.0410\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0200 - val_loss: 0.0403\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0258 - val_loss: 0.0397\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0310 - val_loss: 0.0391\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0212 - val_loss: 0.0387\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0170 - val_loss: 0.0383\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0212 - val_loss: 0.0380\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0197 - val_loss: 0.0375\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0206 - val_loss: 0.0371\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0247 - val_loss: 0.0364\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0339 - val_loss: 0.0358\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0193 - val_loss: 0.0351\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0177 - val_loss: 0.0346\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0232 - val_loss: 0.0343\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0158 - val_loss: 0.0334\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0152 - val_loss: 0.0332\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0233 - val_loss: 0.0326\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0139 - val_loss: 0.0320\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0167 - val_loss: 0.0318\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0160 - val_loss: 0.0314\n",
      "Epoch 47/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0303 - val_loss: 0.0309\n",
      "Epoch 48/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0159 - val_loss: 0.0305\n",
      "Epoch 49/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0301 - val_loss: 0.0304\n",
      "Epoch 50/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0399 - val_loss: 0.0307\n",
      "Epoch 51/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0286 - val_loss: 0.0304\n",
      "Epoch 52/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0205 - val_loss: 0.0304\n",
      "Epoch 53/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0137 - val_loss: 0.0301\n",
      "Epoch 54/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0155 - val_loss: 0.0299\n",
      "Epoch 55/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0177 - val_loss: 0.0298\n",
      "Epoch 56/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0229 - val_loss: 0.0298\n",
      "Epoch 57/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0123 - val_loss: 0.0301\n",
      "Epoch 58/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0455 - val_loss: 0.0305\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb496faba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2265 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 0.4281 - val_loss: 0.0373\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3002 - val_loss: 0.0379\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.2494 - val_loss: 0.0387\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4afa401f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2266 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 1.3390 - val_loss: 0.0283\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.4699 - val_loss: 0.0281\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3526 - val_loss: 0.0283\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2373 - val_loss: 0.0284\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4e34cd820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2267 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.2366 - val_loss: 0.0438\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1629 - val_loss: 0.0438\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1160 - val_loss: 0.0436\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1229 - val_loss: 0.0429\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1077 - val_loss: 0.0422\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0871 - val_loss: 0.0414\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0852 - val_loss: 0.0405\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0741 - val_loss: 0.0397\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0633 - val_loss: 0.0387\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0587 - val_loss: 0.0378\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0575 - val_loss: 0.0369\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0558 - val_loss: 0.0361\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0462 - val_loss: 0.0352\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0505 - val_loss: 0.0344\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0374 - val_loss: 0.0336\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0665 - val_loss: 0.0330\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0571 - val_loss: 0.0323\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0397 - val_loss: 0.0315\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0440 - val_loss: 0.0307\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0293 - val_loss: 0.0299\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0294 - val_loss: 0.0290\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0319 - val_loss: 0.0281\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0307 - val_loss: 0.0271\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0241 - val_loss: 0.0263\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0284 - val_loss: 0.0253\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0340 - val_loss: 0.0243\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0216 - val_loss: 0.0234\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0324 - val_loss: 0.0224\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0244 - val_loss: 0.0215\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0248 - val_loss: 0.0207\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0387 - val_loss: 0.0199\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0301 - val_loss: 0.0193\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0225 - val_loss: 0.0186\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0227 - val_loss: 0.0182\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0238 - val_loss: 0.0177\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0241 - val_loss: 0.0174\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0197 - val_loss: 0.0171\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0188 - val_loss: 0.0170\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0235 - val_loss: 0.0168\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0343 - val_loss: 0.0168\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0172 - val_loss: 0.0170\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3fe518820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2268 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 2.4182 - val_loss: 1.0247\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.3095 - val_loss: 0.9984\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.0120 - val_loss: 0.9340\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.8216 - val_loss: 0.8590\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.7700 - val_loss: 0.8146\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6048 - val_loss: 0.7738\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.6260 - val_loss: 0.7360\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.4707 - val_loss: 0.6907\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3987 - val_loss: 0.6478\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.4625 - val_loss: 0.6094\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3844 - val_loss: 0.5695\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.3356 - val_loss: 0.5384\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.3218 - val_loss: 0.5035\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2741 - val_loss: 0.4741\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2547 - val_loss: 0.4451\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2264 - val_loss: 0.4246\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2064 - val_loss: 0.3999\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2089 - val_loss: 0.3827\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.2119 - val_loss: 0.3681\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1776 - val_loss: 0.3482\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1721 - val_loss: 0.3332\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1588 - val_loss: 0.3185\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1430 - val_loss: 0.3043\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1611 - val_loss: 0.2864\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1495 - val_loss: 0.2768\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1078 - val_loss: 0.2688\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.108 - 0s 11ms/step - loss: 0.1065 - val_loss: 0.2578\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1198 - val_loss: 0.2475\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1124 - val_loss: 0.2405\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0905 - val_loss: 0.2320\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0932 - val_loss: 0.2211\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0878 - val_loss: 0.2132\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0760 - val_loss: 0.2054\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1037 - val_loss: 0.1956\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0783 - val_loss: 0.1927\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0792 - val_loss: 0.1839\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0889 - val_loss: 0.1811\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0722 - val_loss: 0.1759\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0876 - val_loss: 0.1717\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1101 - val_loss: 0.1641\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0654 - val_loss: 0.1618\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0501 - val_loss: 0.1609\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0713 - val_loss: 0.1563\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0493 - val_loss: 0.1610\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0559 - val_loss: 0.1645\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb400224820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2269 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.5805 - val_loss: 0.1690\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2399 - val_loss: 0.1643\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1609 - val_loss: 0.1605\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1622 - val_loss: 0.1573\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1608 - val_loss: 0.1502\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1127 - val_loss: 0.1450\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0888 - val_loss: 0.1399\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1274 - val_loss: 0.1345\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0943 - val_loss: 0.1297\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0937 - val_loss: 0.1253\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0727 - val_loss: 0.1217\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0917 - val_loss: 0.1176\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0619 - val_loss: 0.1134\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0687 - val_loss: 0.1091\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0862 - val_loss: 0.1053\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0710 - val_loss: 0.1015\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0640 - val_loss: 0.0981\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0810 - val_loss: 0.0938\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0746 - val_loss: 0.0907\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0730 - val_loss: 0.0881\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0899 - val_loss: 0.0861\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0688 - val_loss: 0.0841\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0574 - val_loss: 0.0821\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0543 - val_loss: 0.0801\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0582 - val_loss: 0.0780\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0499 - val_loss: 0.0768\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0464 - val_loss: 0.0753\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0469 - val_loss: 0.0739\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0448 - val_loss: 0.0731\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0502 - val_loss: 0.0723\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0363 - val_loss: 0.0720\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0371 - val_loss: 0.0716\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0361 - val_loss: 0.0717\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0427 - val_loss: 0.0716\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3f9ece3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2270 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0947 - val_loss: 0.0264\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0580 - val_loss: 0.0265\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0590 - val_loss: 0.0265\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb412d3b310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2271 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.9651 - val_loss: 0.0363\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.5748 - val_loss: 0.0299\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4842 - val_loss: 0.0272\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.3837 - val_loss: 0.0242\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3438 - val_loss: 0.0213\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2794 - val_loss: 0.0183\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.2360 - val_loss: 0.0163\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2097 - val_loss: 0.0159\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1685 - val_loss: 0.0170\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1417 - val_loss: 0.0182\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4063aadc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2272 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.2272 - val_loss: 0.0366\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1951 - val_loss: 0.0342\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1566 - val_loss: 0.0325\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1379 - val_loss: 0.0300\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1206 - val_loss: 0.0283\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1078 - val_loss: 0.0270\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0982 - val_loss: 0.0264\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0914 - val_loss: 0.0262\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0890 - val_loss: 0.0230\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0785 - val_loss: 0.0227\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0685 - val_loss: 0.0216\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0634 - val_loss: 0.0207\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0596 - val_loss: 0.0196\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0553 - val_loss: 0.0201\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0622 - val_loss: 0.0190\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0475 - val_loss: 0.0178\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0501 - val_loss: 0.0173\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0528 - val_loss: 0.0169\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0403 - val_loss: 0.0163\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0424 - val_loss: 0.0161\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0421 - val_loss: 0.0155\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0431 - val_loss: 0.0153\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0338 - val_loss: 0.0153\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0468 - val_loss: 0.0153\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3f1e71d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2273 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.5067 - val_loss: 0.2857\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.3799 - val_loss: 0.2680\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3486 - val_loss: 0.2552\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2567 - val_loss: 0.2453\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2296 - val_loss: 0.2335\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2287 - val_loss: 0.2239\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1812 - val_loss: 0.2181\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1541 - val_loss: 0.2073\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1457 - val_loss: 0.1972\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1590 - val_loss: 0.1886\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1471 - val_loss: 0.1761\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1294 - val_loss: 0.1713\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1199 - val_loss: 0.1621\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1138 - val_loss: 0.1527\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1172 - val_loss: 0.1471\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0965 - val_loss: 0.1375\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1080 - val_loss: 0.1309\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0893 - val_loss: 0.1216\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0864 - val_loss: 0.1149\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1011 - val_loss: 0.1080\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0867 - val_loss: 0.1064\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0862 - val_loss: 0.0969\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0712 - val_loss: 0.0889\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0630 - val_loss: 0.0820\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0669 - val_loss: 0.0781\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0586 - val_loss: 0.0729\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0516 - val_loss: 0.0686\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0609 - val_loss: 0.0655\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0798 - val_loss: 0.0567\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0558 - val_loss: 0.0554\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0521 - val_loss: 0.0513\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0508 - val_loss: 0.0474\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0499 - val_loss: 0.0455\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0532 - val_loss: 0.0430\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0522 - val_loss: 0.0419\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0607 - val_loss: 0.0403\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0398 - val_loss: 0.0361\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0437 - val_loss: 0.0354\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0514 - val_loss: 0.0327\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0401 - val_loss: 0.0333\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0483 - val_loss: 0.0305\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0364 - val_loss: 0.0288\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0348 - val_loss: 0.0283\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0374 - val_loss: 0.0277\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0319 - val_loss: 0.0269\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0405 - val_loss: 0.0264\n",
      "Epoch 47/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0312 - val_loss: 0.0254\n",
      "Epoch 48/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0337 - val_loss: 0.0261\n",
      "Epoch 49/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0299 - val_loss: 0.0257\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3f40068b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2274 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.4506 - val_loss: 0.0686\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2412 - val_loss: 0.0679\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1922 - val_loss: 0.0656\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1974 - val_loss: 0.0629\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1538 - val_loss: 0.0606\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1216 - val_loss: 0.0587\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1273 - val_loss: 0.0568\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1171 - val_loss: 0.0548\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1046 - val_loss: 0.0524\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0865 - val_loss: 0.0501\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0775 - val_loss: 0.0478\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0866 - val_loss: 0.0453\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0824 - val_loss: 0.0431\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0735 - val_loss: 0.0409\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0658 - val_loss: 0.0391\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0749 - val_loss: 0.0373\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0581 - val_loss: 0.0365\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0705 - val_loss: 0.0361\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0801 - val_loss: 0.0353\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0524 - val_loss: 0.0345\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0579 - val_loss: 0.0336\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0477 - val_loss: 0.0328\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0376 - val_loss: 0.0317\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0620 - val_loss: 0.0309\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0478 - val_loss: 0.0300\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0678 - val_loss: 0.0286\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0538 - val_loss: 0.0291\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0383 - val_loss: 0.0280\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0413 - val_loss: 0.0277\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0330 - val_loss: 0.0273\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0631 - val_loss: 0.0264\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0414 - val_loss: 0.0254\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0354 - val_loss: 0.0245\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0276 - val_loss: 0.0245\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0269 - val_loss: 0.0241\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.039 - 0s 23ms/step - loss: 0.0395 - val_loss: 0.0241\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0415 - val_loss: 0.0240\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0224 - val_loss: 0.0235\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0402 - val_loss: 0.0229\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0479 - val_loss: 0.0236\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0595 - val_loss: 0.0240\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3eeb5da60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2275 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.5535 - val_loss: 0.0168\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3583 - val_loss: 0.0182\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2650 - val_loss: 0.0194\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d7ed05e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2276 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.1519 - val_loss: 0.0492\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1240 - val_loss: 0.0493\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0951 - val_loss: 0.0492\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0725 - val_loss: 0.0478\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0722 - val_loss: 0.0469\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0578 - val_loss: 0.0454\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0621 - val_loss: 0.0447\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0562 - val_loss: 0.0442\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0471 - val_loss: 0.0439\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0511 - val_loss: 0.0429\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0400 - val_loss: 0.0425\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0483 - val_loss: 0.0418\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0316 - val_loss: 0.0408\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0378 - val_loss: 0.0401\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0435 - val_loss: 0.0390\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0295 - val_loss: 0.0384\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0311 - val_loss: 0.0383\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0331 - val_loss: 0.0381\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0295 - val_loss: 0.0380\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0250 - val_loss: 0.0381\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0282 - val_loss: 0.0381\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d9335280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2277 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.6731 - val_loss: 0.0512\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3867 - val_loss: 0.0535\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3084 - val_loss: 0.0555\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d96929d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2278 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 1.0823 - val_loss: 0.1675\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.8103 - val_loss: 0.1541\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.7182 - val_loss: 0.1445\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.5415 - val_loss: 0.1363\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4698 - val_loss: 0.1282\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4305 - val_loss: 0.1210\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4119 - val_loss: 0.1157\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3409 - val_loss: 0.1127\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.3093 - val_loss: 0.1095\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2684 - val_loss: 0.1075\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2656 - val_loss: 0.1063\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2481 - val_loss: 0.1046\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2196 - val_loss: 0.1039\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2031 - val_loss: 0.1025\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1921 - val_loss: 0.1019\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1696 - val_loss: 0.1009\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1452 - val_loss: 0.1001\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1428 - val_loss: 0.0989\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1300 - val_loss: 0.0977\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1366 - val_loss: 0.0964\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1155 - val_loss: 0.0947\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1126 - val_loss: 0.0930\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1019 - val_loss: 0.0914\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1132 - val_loss: 0.0895\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0919 - val_loss: 0.0877\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1001 - val_loss: 0.0860\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1033 - val_loss: 0.0841\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0745 - val_loss: 0.0827\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0766 - val_loss: 0.0814\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0796 - val_loss: 0.0803\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0829 - val_loss: 0.0791\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0941 - val_loss: 0.0770\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0681 - val_loss: 0.0761\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0595 - val_loss: 0.0753\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0603 - val_loss: 0.0746\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0760 - val_loss: 0.0740\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0506 - val_loss: 0.0735\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0495 - val_loss: 0.0731\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0665 - val_loss: 0.0724\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0643 - val_loss: 0.0722\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0564 - val_loss: 0.0719\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0480 - val_loss: 0.0722\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0658 - val_loss: 0.0717\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0520 - val_loss: 0.0723\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0446 - val_loss: 0.0727\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3da9160d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2279 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.4559 - val_loss: 0.0366\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2355 - val_loss: 0.0358\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2350 - val_loss: 0.0352\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2007 - val_loss: 0.0345\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1492 - val_loss: 0.0338\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0860 - val_loss: 0.0312\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0776 - val_loss: 0.0294\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0680 - val_loss: 0.0262\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0832 - val_loss: 0.0243\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0893 - val_loss: 0.0226\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0594 - val_loss: 0.0209\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0506 - val_loss: 0.0198\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0582 - val_loss: 0.0189\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0554 - val_loss: 0.0183\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0565 - val_loss: 0.0176\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0594 - val_loss: 0.0173\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0511 - val_loss: 0.0169\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0414 - val_loss: 0.0168\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0438 - val_loss: 0.0166\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0487 - val_loss: 0.0165\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0427 - val_loss: 0.0163\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0353 - val_loss: 0.0161\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0546 - val_loss: 0.0161\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0301 - val_loss: 0.0160\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0314 - val_loss: 0.0158\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0330 - val_loss: 0.0159\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0289 - val_loss: 0.0160\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3dac374c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2280 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 1.4188 - val_loss: 0.0365\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.4753 - val_loss: 0.0299\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3132 - val_loss: 0.0203\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.2555 - val_loss: 0.0157\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1230 - val_loss: 0.0133\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1177 - val_loss: 0.0113\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0886 - val_loss: 0.0101\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1106 - val_loss: 0.0094\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1112 - val_loss: 0.0092\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0844 - val_loss: 0.0094\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0787 - val_loss: 0.0102\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3db849b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2281 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.7753 - val_loss: 0.0376\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.5841 - val_loss: 0.0353\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.4398 - val_loss: 0.0331\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3747 - val_loss: 0.0301\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.3189 - val_loss: 0.0294\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2504 - val_loss: 0.0276\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2470 - val_loss: 0.0265\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.2350 - val_loss: 0.0249\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1780 - val_loss: 0.0234\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1230 - val_loss: 0.0228\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1012 - val_loss: 0.0216\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0890 - val_loss: 0.0208\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1105 - val_loss: 0.0199\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0988 - val_loss: 0.0188\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0780 - val_loss: 0.0180\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0841 - val_loss: 0.0177\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0675 - val_loss: 0.0173\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0649 - val_loss: 0.0167\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0795 - val_loss: 0.0163\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0547 - val_loss: 0.0159\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0517 - val_loss: 0.0156\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0580 - val_loss: 0.0152\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0447 - val_loss: 0.0147\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0406 - val_loss: 0.0144\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0417 - val_loss: 0.0141\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0348 - val_loss: 0.0137\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0458 - val_loss: 0.0136\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0369 - val_loss: 0.0133\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0684 - val_loss: 0.0131\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0391 - val_loss: 0.0128\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0403 - val_loss: 0.0126\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0346 - val_loss: 0.0124\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0586 - val_loss: 0.0121\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0315 - val_loss: 0.0119\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0486 - val_loss: 0.0118\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0322 - val_loss: 0.0116\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0325 - val_loss: 0.0115\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0395 - val_loss: 0.0114\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0277 - val_loss: 0.0114\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0516 - val_loss: 0.0115\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0442 - val_loss: 0.0114\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3dbc474c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2282 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.5022 - val_loss: 0.1451\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.2688 - val_loss: 0.1514\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2314 - val_loss: 0.1541\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3dc8891f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2283 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.5237 - val_loss: 0.0232\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3052 - val_loss: 0.0227\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2174 - val_loss: 0.0211\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1849 - val_loss: 0.0197\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1575 - val_loss: 0.0188\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1460 - val_loss: 0.0182\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1483 - val_loss: 0.0166\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1475 - val_loss: 0.0160\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1298 - val_loss: 0.0153\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1111 - val_loss: 0.0150\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1079 - val_loss: 0.0147\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0948 - val_loss: 0.0147\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0771 - val_loss: 0.0147\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1110 - val_loss: 0.0151\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0983 - val_loss: 0.0149\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3dcbc6ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2284 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.7950 - val_loss: 0.1025\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.4508 - val_loss: 0.0882\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3192 - val_loss: 0.0825\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.3099 - val_loss: 0.0769\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2406 - val_loss: 0.0717\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1645 - val_loss: 0.0691\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1714 - val_loss: 0.0660\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1225 - val_loss: 0.0631\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1068 - val_loss: 0.0616\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1052 - val_loss: 0.0598\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1089 - val_loss: 0.0577\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0819 - val_loss: 0.0545\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1069 - val_loss: 0.0524\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0767 - val_loss: 0.0490\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0914 - val_loss: 0.0464\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1038 - val_loss: 0.0448\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0577 - val_loss: 0.0421\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0687 - val_loss: 0.0405\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1097 - val_loss: 0.0366\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0518 - val_loss: 0.0351\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0524 - val_loss: 0.0338\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0576 - val_loss: 0.0328\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.059 - 0s 12ms/step - loss: 0.0591 - val_loss: 0.0313\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0651 - val_loss: 0.0294\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0684 - val_loss: 0.0281\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0447 - val_loss: 0.0261\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0393 - val_loss: 0.0244\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0412 - val_loss: 0.0228\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0514 - val_loss: 0.0214\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0306 - val_loss: 0.0199\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0560 - val_loss: 0.0181\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0327 - val_loss: 0.0171\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0476 - val_loss: 0.0158\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0475 - val_loss: 0.0161\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0499 - val_loss: 0.0162\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3dfe03c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2285 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.3628 - val_loss: 0.0326\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2176 - val_loss: 0.0313\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1967 - val_loss: 0.0297\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1566 - val_loss: 0.0282\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1639 - val_loss: 0.0277\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1678 - val_loss: 0.0269\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1518 - val_loss: 0.0271\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1326 - val_loss: 0.0270\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e0bbb820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2286 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.2640 - val_loss: 0.3353\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1652 - val_loss: 0.3258\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1379 - val_loss: 0.3156\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1234 - val_loss: 0.3047\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1056 - val_loss: 0.2956\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0994 - val_loss: 0.2872\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1001 - val_loss: 0.2768\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1043 - val_loss: 0.2717\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0753 - val_loss: 0.2686\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0631 - val_loss: 0.2654\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0635 - val_loss: 0.2594\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0605 - val_loss: 0.2540\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0500 - val_loss: 0.2478\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0603 - val_loss: 0.2400\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0553 - val_loss: 0.2347\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0668 - val_loss: 0.2282\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0560 - val_loss: 0.2221\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0696 - val_loss: 0.2161\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0340 - val_loss: 0.2104\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0486 - val_loss: 0.2063\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0355 - val_loss: 0.2013\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0392 - val_loss: 0.1972\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0355 - val_loss: 0.1925\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0288 - val_loss: 0.1878\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0440 - val_loss: 0.1830\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0302 - val_loss: 0.1786\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0381 - val_loss: 0.1752\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0367 - val_loss: 0.1714\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0329 - val_loss: 0.1670\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0383 - val_loss: 0.1640\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0491 - val_loss: 0.1620\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0386 - val_loss: 0.1593\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0350 - val_loss: 0.1562\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0294 - val_loss: 0.1523\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0438 - val_loss: 0.1496\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0343 - val_loss: 0.1455\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0371 - val_loss: 0.1426\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0321 - val_loss: 0.1404\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0291 - val_loss: 0.1378\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0270 - val_loss: 0.1355\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0246 - val_loss: 0.1344\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0262 - val_loss: 0.1323\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0451 - val_loss: 0.1295\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0334 - val_loss: 0.1292\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0240 - val_loss: 0.1278\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0250 - val_loss: 0.1290\n",
      "Epoch 47/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0229 - val_loss: 0.1296\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e26740d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2287 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.3713 - val_loss: 0.0173\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1305 - val_loss: 0.0196\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1044 - val_loss: 0.0225\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e39a91f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2288 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.2366 - val_loss: 0.3675\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1349 - val_loss: 0.3604\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0896 - val_loss: 0.3565\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0963 - val_loss: 0.3533\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0746 - val_loss: 0.3485\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0644 - val_loss: 0.3462\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0630 - val_loss: 0.3436\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0603 - val_loss: 0.3407\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0549 - val_loss: 0.3382\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0487 - val_loss: 0.3377\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0682 - val_loss: 0.3363\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0698 - val_loss: 0.3341\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0434 - val_loss: 0.3331\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0432 - val_loss: 0.3325\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0519 - val_loss: 0.3318\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0392 - val_loss: 0.3312\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0426 - val_loss: 0.3314\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0437 - val_loss: 0.3301\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0432 - val_loss: 0.3288\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.032 - 0s 10ms/step - loss: 0.0413 - val_loss: 0.3266\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0446 - val_loss: 0.3254\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0388 - val_loss: 0.3226\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0460 - val_loss: 0.3202\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0375 - val_loss: 0.3153\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0331 - val_loss: 0.3111\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0408 - val_loss: 0.3065\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0341 - val_loss: 0.3023\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0409 - val_loss: 0.2981\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0274 - val_loss: 0.2928\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0293 - val_loss: 0.2877\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0254 - val_loss: 0.2825\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0345 - val_loss: 0.2786\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0301 - val_loss: 0.2738\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0312 - val_loss: 0.2693\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0297 - val_loss: 0.2639\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0449 - val_loss: 0.2587\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0279 - val_loss: 0.2552\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0276 - val_loss: 0.2501\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0220 - val_loss: 0.2464\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0205 - val_loss: 0.2428\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0232 - val_loss: 0.2384\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0210 - val_loss: 0.2347\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0189 - val_loss: 0.2312\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0261 - val_loss: 0.2258\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0421 - val_loss: 0.2226\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0303 - val_loss: 0.2186\n",
      "Epoch 47/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0229 - val_loss: 0.2146\n",
      "Epoch 48/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0222 - val_loss: 0.2095\n",
      "Epoch 49/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0251 - val_loss: 0.2055\n",
      "Epoch 50/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0182 - val_loss: 0.2006\n",
      "Epoch 51/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0227 - val_loss: 0.1970\n",
      "Epoch 52/60\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0152 - val_loss: 0.1921\n",
      "Epoch 53/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0225 - val_loss: 0.1873\n",
      "Epoch 54/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0291 - val_loss: 0.1825\n",
      "Epoch 55/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0181 - val_loss: 0.1786\n",
      "Epoch 56/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0209 - val_loss: 0.1749\n",
      "Epoch 57/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0151 - val_loss: 0.1696\n",
      "Epoch 58/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0246 - val_loss: 0.1657\n",
      "Epoch 59/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0239 - val_loss: 0.1615\n",
      "Epoch 60/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0240 - val_loss: 0.1581\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e3ec5550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2289 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 2.1628 - val_loss: 0.0141\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.4397 - val_loss: 0.0124\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.1386 - val_loss: 0.0125\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.7949 - val_loss: 0.0129\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e5647d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2290 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0764 - val_loss: 0.0314\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0600 - val_loss: 0.0289\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0527 - val_loss: 0.0268\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0416 - val_loss: 0.0251\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0640 - val_loss: 0.0236\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0434 - val_loss: 0.0226\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0320 - val_loss: 0.0217\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0371 - val_loss: 0.0210\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0339 - val_loss: 0.0201\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0331 - val_loss: 0.0193\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0342 - val_loss: 0.0185\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0293 - val_loss: 0.0176\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0274 - val_loss: 0.0167\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0315 - val_loss: 0.0160\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0280 - val_loss: 0.0155\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0330 - val_loss: 0.0150\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0290 - val_loss: 0.0149\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0279 - val_loss: 0.0149\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0243 - val_loss: 0.0150\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e76ecee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2291 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.3528 - val_loss: 0.1115\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2556 - val_loss: 0.1054\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1906 - val_loss: 0.1008\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1711 - val_loss: 0.0952\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1540 - val_loss: 0.0895\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1204 - val_loss: 0.0854\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1110 - val_loss: 0.0824\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1079 - val_loss: 0.0781\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1257 - val_loss: 0.0761\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1020 - val_loss: 0.0707\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0937 - val_loss: 0.0697\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0960 - val_loss: 0.0642\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0970 - val_loss: 0.0654\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0736 - val_loss: 0.0644\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb42f4ccca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2292 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.8106 - val_loss: 0.1496\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.6239 - val_loss: 0.1490\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.4828 - val_loss: 0.1492\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.4095 - val_loss: 0.1530\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4273731f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2293 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.2375 - val_loss: 0.0186\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1641 - val_loss: 0.0178\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1644 - val_loss: 0.0170\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1169 - val_loss: 0.0164\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1151 - val_loss: 0.0161\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0911 - val_loss: 0.0157\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0808 - val_loss: 0.0159\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0797 - val_loss: 0.0159\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4bdd33790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2294 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.2112 - val_loss: 0.0280\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1022 - val_loss: 0.0274\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0749 - val_loss: 0.0272\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0755 - val_loss: 0.0268\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0515 - val_loss: 0.0270\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0388 - val_loss: 0.0272\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb421929d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2295 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.0846 - val_loss: 0.0331\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0500 - val_loss: 0.0327\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0462 - val_loss: 0.0330\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0416 - val_loss: 0.0332\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3f18ab040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2296 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 0.3506 - val_loss: 0.0201\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1949 - val_loss: 0.0200\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1778 - val_loss: 0.0198\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1450 - val_loss: 0.0195\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1298 - val_loss: 0.0192\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1150 - val_loss: 0.0189\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1088 - val_loss: 0.0186\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0929 - val_loss: 0.0182\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1011 - val_loss: 0.0180\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0806 - val_loss: 0.0177\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0737 - val_loss: 0.0174\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0698 - val_loss: 0.0173\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0655 - val_loss: 0.0171\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0750 - val_loss: 0.0170\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0682 - val_loss: 0.0169\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0525 - val_loss: 0.0168\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0567 - val_loss: 0.0167\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0684 - val_loss: 0.0167\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0441 - val_loss: 0.0170\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.056 - 0s 11ms/step - loss: 0.0468 - val_loss: 0.0173\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ee92b550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2297 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.8358 - val_loss: 0.0479\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5776 - val_loss: 0.0420\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4826 - val_loss: 0.0405\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2923 - val_loss: 0.0390\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2398 - val_loss: 0.0372\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2131 - val_loss: 0.0342\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1791 - val_loss: 0.0311\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1457 - val_loss: 0.0288\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1247 - val_loss: 0.0270\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1098 - val_loss: 0.0255\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1114 - val_loss: 0.0243\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0985 - val_loss: 0.0233\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0895 - val_loss: 0.0225\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0974 - val_loss: 0.0219\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0872 - val_loss: 0.0213\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0922 - val_loss: 0.0209\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0714 - val_loss: 0.0209\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0881 - val_loss: 0.0211\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4d20c04c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2298 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 1.0073 - val_loss: 0.0521\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.7930 - val_loss: 0.0504\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.6078 - val_loss: 0.0492\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.4959 - val_loss: 0.0488\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.4046 - val_loss: 0.0477\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.2994 - val_loss: 0.0508\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2957 - val_loss: 0.0528\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e0bbbb80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2299 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.3814 - val_loss: 0.1007\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.2502 - val_loss: 0.1056\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1914 - val_loss: 0.1109\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3db976e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2300 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.4285 - val_loss: 0.3602\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2753 - val_loss: 0.3318\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2264 - val_loss: 0.3125\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1641 - val_loss: 0.2974\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1362 - val_loss: 0.2829\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1040 - val_loss: 0.2703\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0927 - val_loss: 0.2581\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0887 - val_loss: 0.2441\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0939 - val_loss: 0.2357\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0739 - val_loss: 0.2275\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0530 - val_loss: 0.2226\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0571 - val_loss: 0.2197\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0461 - val_loss: 0.2132\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0436 - val_loss: 0.2107\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0561 - val_loss: 0.2026\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0379 - val_loss: 0.2059\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0389 - val_loss: 0.2066\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d946f280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2301 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.3618 - val_loss: 0.0361\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1250 - val_loss: 0.0350\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0854 - val_loss: 0.0339\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0634 - val_loss: 0.0324\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0496 - val_loss: 0.0308\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0425 - val_loss: 0.0290\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0499 - val_loss: 0.0280\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0414 - val_loss: 0.0265\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0425 - val_loss: 0.0251\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0342 - val_loss: 0.0237\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0352 - val_loss: 0.0226\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0370 - val_loss: 0.0215\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0305 - val_loss: 0.0203\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0364 - val_loss: 0.0195\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0362 - val_loss: 0.0190\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0282 - val_loss: 0.0187\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0339 - val_loss: 0.0183\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0325 - val_loss: 0.0180\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0309 - val_loss: 0.0177\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0247 - val_loss: 0.0174\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0183 - val_loss: 0.0172\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0205 - val_loss: 0.0170\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0202 - val_loss: 0.0168\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0143 - val_loss: 0.0166\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0184 - val_loss: 0.0164\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0180 - val_loss: 0.0162\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0209 - val_loss: 0.0161\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0169 - val_loss: 0.0160\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0305 - val_loss: 0.0160\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0184 - val_loss: 0.0162\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0277 - val_loss: 0.0163\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3fbede0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2302 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 0.1974 - val_loss: 0.0354\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1820 - val_loss: 0.0352\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1692 - val_loss: 0.0342\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1349 - val_loss: 0.0349\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0854 - val_loss: 0.0353\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3f4006ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2303 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.5037 - val_loss: 0.0463\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3838 - val_loss: 0.0419\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2366 - val_loss: 0.0393\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1896 - val_loss: 0.0370\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1748 - val_loss: 0.0355\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1364 - val_loss: 0.0339\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1336 - val_loss: 0.0324\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1128 - val_loss: 0.0312\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1089 - val_loss: 0.0301\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0901 - val_loss: 0.0289\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0780 - val_loss: 0.0280\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0736 - val_loss: 0.0273\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0799 - val_loss: 0.0261\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0678 - val_loss: 0.0253\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0598 - val_loss: 0.0247\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0671 - val_loss: 0.0238\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0699 - val_loss: 0.0232\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0561 - val_loss: 0.0225\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0705 - val_loss: 0.0215\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0509 - val_loss: 0.0203\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0545 - val_loss: 0.0196\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0430 - val_loss: 0.0189\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0384 - val_loss: 0.0185\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.030 - 0s 11ms/step - loss: 0.0379 - val_loss: 0.0181\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0655 - val_loss: 0.0178\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0521 - val_loss: 0.0176\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0416 - val_loss: 0.0176\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0395 - val_loss: 0.0177\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0446 - val_loss: 0.0178\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3f7c1e280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2304 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.1711 - val_loss: 0.0217\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1228 - val_loss: 0.0203\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1006 - val_loss: 0.0193\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0852 - val_loss: 0.0183\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0740 - val_loss: 0.0176\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0770 - val_loss: 0.0168\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0614 - val_loss: 0.0163\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0712 - val_loss: 0.0158\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0574 - val_loss: 0.0154\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0644 - val_loss: 0.0151\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0479 - val_loss: 0.0149\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0484 - val_loss: 0.0147\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0516 - val_loss: 0.0147\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0537 - val_loss: 0.0148\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0510 - val_loss: 0.0149\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb412d3bf70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2305 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.3504 - val_loss: 0.0288\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1988 - val_loss: 0.0268\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1310 - val_loss: 0.0250\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1396 - val_loss: 0.0246\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1155 - val_loss: 0.0234\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0888 - val_loss: 0.0222\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0732 - val_loss: 0.0211\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0710 - val_loss: 0.0204\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0773 - val_loss: 0.0192\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0534 - val_loss: 0.0182\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0542 - val_loss: 0.0174\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0555 - val_loss: 0.0168\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0449 - val_loss: 0.0165\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0449 - val_loss: 0.0164\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0434 - val_loss: 0.0164\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0472 - val_loss: 0.0165\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0401 - val_loss: 0.0165\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3f1b94790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2306 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.2178 - val_loss: 0.1257\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1730 - val_loss: 0.1260\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1386 - val_loss: 0.1250\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1250 - val_loss: 0.1244\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1267 - val_loss: 0.1238\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0961 - val_loss: 0.1235\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1255 - val_loss: 0.1230\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1097 - val_loss: 0.1221\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1272 - val_loss: 0.1199\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1223 - val_loss: 0.1195\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0979 - val_loss: 0.1182\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1030 - val_loss: 0.1161\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0975 - val_loss: 0.1149\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1013 - val_loss: 0.1133\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0842 - val_loss: 0.1102\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0740 - val_loss: 0.1069\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0708 - val_loss: 0.1041\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0726 - val_loss: 0.1007\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0629 - val_loss: 0.0970\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1024 - val_loss: 0.0957\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0745 - val_loss: 0.0915\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0678 - val_loss: 0.0878\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0519 - val_loss: 0.0853\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0576 - val_loss: 0.0803\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0506 - val_loss: 0.0760\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0655 - val_loss: 0.0733\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0526 - val_loss: 0.0710\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0512 - val_loss: 0.0666\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0490 - val_loss: 0.0625\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0513 - val_loss: 0.0594\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0487 - val_loss: 0.0563\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0458 - val_loss: 0.0521\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0432 - val_loss: 0.0484\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0464 - val_loss: 0.0459\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0657 - val_loss: 0.0445\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0518 - val_loss: 0.0407\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0410 - val_loss: 0.0385\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0495 - val_loss: 0.0366\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0364 - val_loss: 0.0342\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0601 - val_loss: 0.0299\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0512 - val_loss: 0.0301\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.032 - 0s 11ms/step - loss: 0.0419 - val_loss: 0.0282\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0357 - val_loss: 0.0268\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0364 - val_loss: 0.0255\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0401 - val_loss: 0.0245\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0467 - val_loss: 0.0231\n",
      "Epoch 47/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0418 - val_loss: 0.0218\n",
      "Epoch 48/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0438 - val_loss: 0.0216\n",
      "Epoch 49/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0439 - val_loss: 0.0205\n",
      "Epoch 50/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0446 - val_loss: 0.0195\n",
      "Epoch 51/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0503 - val_loss: 0.0202\n",
      "Epoch 52/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0315 - val_loss: 0.0191\n",
      "Epoch 53/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0386 - val_loss: 0.0183\n",
      "Epoch 54/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0328 - val_loss: 0.0179\n",
      "Epoch 55/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0354 - val_loss: 0.0175\n",
      "Epoch 56/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0382 - val_loss: 0.0176\n",
      "Epoch 57/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0337 - val_loss: 0.0173\n",
      "Epoch 58/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0286 - val_loss: 0.0173\n",
      "Epoch 59/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0283 - val_loss: 0.0173\n",
      "Epoch 60/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0323 - val_loss: 0.0175\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4d27c31f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2307 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.3170 - val_loss: 0.0325\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1920 - val_loss: 0.0383\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1234 - val_loss: 0.0411\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4938bf160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2308 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 1.1397 - val_loss: 0.0153\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6554 - val_loss: 0.0149\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.4504 - val_loss: 0.0149\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.3425 - val_loss: 0.0151\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4dfce0af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2309 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.3703 - val_loss: 0.0889\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1961 - val_loss: 0.0886\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1759 - val_loss: 0.0865\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1048 - val_loss: 0.0841\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1126 - val_loss: 0.0804\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0934 - val_loss: 0.0785\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0814 - val_loss: 0.0747\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0953 - val_loss: 0.0723\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0750 - val_loss: 0.0683\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0850 - val_loss: 0.0657\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0645 - val_loss: 0.0630\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1024 - val_loss: 0.0598\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0807 - val_loss: 0.0575\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0593 - val_loss: 0.0543\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0631 - val_loss: 0.0515\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0651 - val_loss: 0.0480\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0696 - val_loss: 0.0457\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0569 - val_loss: 0.0434\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0560 - val_loss: 0.0411\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0592 - val_loss: 0.0385\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0474 - val_loss: 0.0366\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0452 - val_loss: 0.0346\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0563 - val_loss: 0.0319\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0452 - val_loss: 0.0302\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.055 - 0s 12ms/step - loss: 0.0621 - val_loss: 0.0277\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0495 - val_loss: 0.0262\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0476 - val_loss: 0.0256\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0621 - val_loss: 0.0243\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0521 - val_loss: 0.0239\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0644 - val_loss: 0.0220\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0438 - val_loss: 0.0219\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0467 - val_loss: 0.0216\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0523 - val_loss: 0.0210\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0426 - val_loss: 0.0211\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0516 - val_loss: 0.0206\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0533 - val_loss: 0.0209\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0457 - val_loss: 0.0209\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e88d6430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2310 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.4099 - val_loss: 0.4505\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2717 - val_loss: 0.4326\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2260 - val_loss: 0.4225\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1686 - val_loss: 0.4030\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.2565 - val_loss: 0.4141\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1780 - val_loss: 0.4005\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1204 - val_loss: 0.3855\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1346 - val_loss: 0.3786\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1112 - val_loss: 0.3645\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0986 - val_loss: 0.3526\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0866 - val_loss: 0.3379\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0792 - val_loss: 0.3206\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0774 - val_loss: 0.3130\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0832 - val_loss: 0.3014\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0742 - val_loss: 0.2867\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0764 - val_loss: 0.2753\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0724 - val_loss: 0.2647\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0815 - val_loss: 0.2569\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0658 - val_loss: 0.2474\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0770 - val_loss: 0.2376\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0698 - val_loss: 0.2276\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0568 - val_loss: 0.2211\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0610 - val_loss: 0.2121\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0463 - val_loss: 0.2073\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0543 - val_loss: 0.2032\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0432 - val_loss: 0.1972\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0668 - val_loss: 0.1921\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0497 - val_loss: 0.1878\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0531 - val_loss: 0.1875\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0407 - val_loss: 0.1824\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0432 - val_loss: 0.1785\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0496 - val_loss: 0.1772\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0571 - val_loss: 0.1743\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0419 - val_loss: 0.1719\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0354 - val_loss: 0.1687\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0312 - val_loss: 0.1674\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0575 - val_loss: 0.1649\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0530 - val_loss: 0.1666\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0426 - val_loss: 0.1637\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0454 - val_loss: 0.1621\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0319 - val_loss: 0.1613\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0342 - val_loss: 0.1604\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0313 - val_loss: 0.1606\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0303 - val_loss: 0.1601\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0289 - val_loss: 0.1607\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0267 - val_loss: 0.1599\n",
      "Epoch 47/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0303 - val_loss: 0.1598\n",
      "Epoch 48/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0281 - val_loss: 0.1599\n",
      "Epoch 49/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0242 - val_loss: 0.1592\n",
      "Epoch 50/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0330 - val_loss: 0.1586\n",
      "Epoch 51/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0262 - val_loss: 0.1599\n",
      "Epoch 52/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0244 - val_loss: 0.1632\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb403f29820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2311 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.9248 - val_loss: 0.0720\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.6951 - val_loss: 0.0687\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.6015 - val_loss: 0.0666\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.5577 - val_loss: 0.0649\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5220 - val_loss: 0.0625\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.5042 - val_loss: 0.0590\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.4088 - val_loss: 0.0563\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.3748 - val_loss: 0.0532\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3560 - val_loss: 0.0499\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.3036 - val_loss: 0.0470\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2931 - val_loss: 0.0440\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2923 - val_loss: 0.0419\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2718 - val_loss: 0.0394\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.2834 - val_loss: 0.0371\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.2228 - val_loss: 0.0352\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2218 - val_loss: 0.0336\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.2113 - val_loss: 0.0320\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2171 - val_loss: 0.0305\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1898 - val_loss: 0.0290\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1828 - val_loss: 0.0279\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1614 - val_loss: 0.0268\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1577 - val_loss: 0.0258\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1449 - val_loss: 0.0250\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1510 - val_loss: 0.0242\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1417 - val_loss: 0.0233\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1285 - val_loss: 0.0226\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1461 - val_loss: 0.0220\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1450 - val_loss: 0.0214\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1263 - val_loss: 0.0207\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1132 - val_loss: 0.0204\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1193 - val_loss: 0.0200\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1085 - val_loss: 0.0199\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1285 - val_loss: 0.0200\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1089 - val_loss: 0.0202\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb400950430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2312 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.2437 - val_loss: 0.0723\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2097 - val_loss: 0.0698\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1254 - val_loss: 0.0660\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0891 - val_loss: 0.0633\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0815 - val_loss: 0.0610\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0854 - val_loss: 0.0591\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0652 - val_loss: 0.0576\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0627 - val_loss: 0.0570\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0534 - val_loss: 0.0568\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0679 - val_loss: 0.0570\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0622 - val_loss: 0.0571\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4000b6dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2313 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.3129 - val_loss: 0.0374\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1978 - val_loss: 0.0366\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1434 - val_loss: 0.0339\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1223 - val_loss: 0.0307\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1084 - val_loss: 0.0298\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1051 - val_loss: 0.0283\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0884 - val_loss: 0.0274\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0832 - val_loss: 0.0267\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0649 - val_loss: 0.0256\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0610 - val_loss: 0.0258\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0699 - val_loss: 0.0252\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0626 - val_loss: 0.0249\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0673 - val_loss: 0.0254\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0649 - val_loss: 0.0258\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ff9a9550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2314 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.1349 - val_loss: 0.0703\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0923 - val_loss: 0.0658\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0733 - val_loss: 0.0626\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0715 - val_loss: 0.0588\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0611 - val_loss: 0.0563\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0496 - val_loss: 0.0536\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0483 - val_loss: 0.0512\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0414 - val_loss: 0.0483\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0400 - val_loss: 0.0459\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0360 - val_loss: 0.0439\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0410 - val_loss: 0.0422\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0295 - val_loss: 0.0408\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0297 - val_loss: 0.0394\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0292 - val_loss: 0.0382\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0246 - val_loss: 0.0369\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0244 - val_loss: 0.0355\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0290 - val_loss: 0.0342\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0195 - val_loss: 0.0328\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0164 - val_loss: 0.0317\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0301 - val_loss: 0.0306\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0210 - val_loss: 0.0294\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0216 - val_loss: 0.0286\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0187 - val_loss: 0.0280\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0172 - val_loss: 0.0275\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0282 - val_loss: 0.0273\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0189 - val_loss: 0.0269\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0267 - val_loss: 0.0265\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0209 - val_loss: 0.0265\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0181 - val_loss: 0.0264\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0177 - val_loss: 0.0261\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0189 - val_loss: 0.0261\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0175 - val_loss: 0.0258\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0208 - val_loss: 0.0254\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0184 - val_loss: 0.0252\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0154 - val_loss: 0.0251\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0396 - val_loss: 0.0253\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0180 - val_loss: 0.0256\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb427b6b040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2315 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.1801 - val_loss: 0.1564\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1532 - val_loss: 0.1584\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1227 - val_loss: 0.1593\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb40f1f49d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2316 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.2756 - val_loss: 0.0456\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1897 - val_loss: 0.0444\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1461 - val_loss: 0.0423\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1231 - val_loss: 0.0406\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1005 - val_loss: 0.0394\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0963 - val_loss: 0.0381\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0947 - val_loss: 0.0368\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0684 - val_loss: 0.0357\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0648 - val_loss: 0.0350\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1016 - val_loss: 0.0344\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0633 - val_loss: 0.0342\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0548 - val_loss: 0.0341\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0629 - val_loss: 0.0338\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0624 - val_loss: 0.0334\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0542 - val_loss: 0.0331\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0402 - val_loss: 0.0331\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0441 - val_loss: 0.0331\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0335 - val_loss: 0.0332\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4e8a05ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2317 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 2.4125 - val_loss: 0.1743\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.5698 - val_loss: 0.1647\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.0261 - val_loss: 0.1606\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.8638 - val_loss: 0.1535\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.8057 - val_loss: 0.1515\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.6021 - val_loss: 0.1477\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5721 - val_loss: 0.1403\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.4902 - val_loss: 0.1350\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4126 - val_loss: 0.1308\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4192 - val_loss: 0.1242\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3178 - val_loss: 0.1170\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.4706 - val_loss: 0.1110\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2895 - val_loss: 0.1047\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2504 - val_loss: 0.1002\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2519 - val_loss: 0.0953\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2273 - val_loss: 0.0929\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2787 - val_loss: 0.0897\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2178 - val_loss: 0.0879\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1714 - val_loss: 0.0862\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1713 - val_loss: 0.0854\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1729 - val_loss: 0.0847\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1506 - val_loss: 0.0853\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1431 - val_loss: 0.0855\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb40c0faee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2318 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.1785 - val_loss: 0.1413\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1328 - val_loss: 0.1406\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1144 - val_loss: 0.1400\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1055 - val_loss: 0.1394\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1028 - val_loss: 0.1389\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0937 - val_loss: 0.1383\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0823 - val_loss: 0.1370\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0824 - val_loss: 0.1362\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0752 - val_loss: 0.1347\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0733 - val_loss: 0.1327\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0627 - val_loss: 0.1314\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0703 - val_loss: 0.1301\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0546 - val_loss: 0.1284\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0582 - val_loss: 0.1264\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0609 - val_loss: 0.1247\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0445 - val_loss: 0.1223\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0516 - val_loss: 0.1201\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0525 - val_loss: 0.1184\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0455 - val_loss: 0.1157\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0466 - val_loss: 0.1136\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0443 - val_loss: 0.1103\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0414 - val_loss: 0.1083\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0399 - val_loss: 0.1069\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0400 - val_loss: 0.1050\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0404 - val_loss: 0.1035\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0392 - val_loss: 0.1016\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0425 - val_loss: 0.1000\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0355 - val_loss: 0.0976\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0414 - val_loss: 0.0959\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0325 - val_loss: 0.0941\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0343 - val_loss: 0.0926\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0349 - val_loss: 0.0912\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0410 - val_loss: 0.0898\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0304 - val_loss: 0.0877\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0316 - val_loss: 0.0855\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0356 - val_loss: 0.0839\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0321 - val_loss: 0.0822\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0326 - val_loss: 0.0808\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0299 - val_loss: 0.0799\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0316 - val_loss: 0.0783\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0313 - val_loss: 0.0765\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0467 - val_loss: 0.0753\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0323 - val_loss: 0.0740\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0348 - val_loss: 0.0729\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0286 - val_loss: 0.0715\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0273 - val_loss: 0.0701\n",
      "Epoch 47/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0261 - val_loss: 0.0681\n",
      "Epoch 48/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0299 - val_loss: 0.0662\n",
      "Epoch 49/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0258 - val_loss: 0.0650\n",
      "Epoch 50/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0281 - val_loss: 0.0634\n",
      "Epoch 51/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0235 - val_loss: 0.0617\n",
      "Epoch 52/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0295 - val_loss: 0.0600\n",
      "Epoch 53/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0246 - val_loss: 0.0584\n",
      "Epoch 54/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0235 - val_loss: 0.0572\n",
      "Epoch 55/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0219 - val_loss: 0.0558\n",
      "Epoch 56/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0209 - val_loss: 0.0545\n",
      "Epoch 57/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0268 - val_loss: 0.0529\n",
      "Epoch 58/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0260 - val_loss: 0.0512\n",
      "Epoch 59/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0233 - val_loss: 0.0497\n",
      "Epoch 60/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0232 - val_loss: 0.0478\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb40968aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2319 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.5385 - val_loss: 0.0894\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3145 - val_loss: 0.0883\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2657 - val_loss: 0.0879\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1829 - val_loss: 0.0882\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1555 - val_loss: 0.0877\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1585 - val_loss: 0.0880\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1130 - val_loss: 0.0875\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1050 - val_loss: 0.0870\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0970 - val_loss: 0.0861\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0762 - val_loss: 0.0854\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0959 - val_loss: 0.0840\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0802 - val_loss: 0.0827\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0736 - val_loss: 0.0816\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0778 - val_loss: 0.0802\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0515 - val_loss: 0.0784\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0669 - val_loss: 0.0765\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0636 - val_loss: 0.0746\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0789 - val_loss: 0.0734\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0587 - val_loss: 0.0723\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0452 - val_loss: 0.0710\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0390 - val_loss: 0.0700\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0518 - val_loss: 0.0689\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0474 - val_loss: 0.0675\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0390 - val_loss: 0.0665\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0384 - val_loss: 0.0657\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0614 - val_loss: 0.0645\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0416 - val_loss: 0.0637\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0383 - val_loss: 0.0632\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0375 - val_loss: 0.0630\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0370 - val_loss: 0.0627\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0304 - val_loss: 0.0624\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0491 - val_loss: 0.0622\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0349 - val_loss: 0.0620\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0357 - val_loss: 0.0621\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0397 - val_loss: 0.0622\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4269420d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2320 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 1.3089 - val_loss: 0.0347\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.5795 - val_loss: 0.0285\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3353 - val_loss: 0.0275\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2186 - val_loss: 0.0287\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2856 - val_loss: 0.0277\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4331515e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2321 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.9257 - val_loss: 0.1306\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4743 - val_loss: 0.1288\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.3774 - val_loss: 0.1252\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3244 - val_loss: 0.1235\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2644 - val_loss: 0.1229\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1925 - val_loss: 0.1203\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1668 - val_loss: 0.1189\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1429 - val_loss: 0.1178\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1510 - val_loss: 0.1174\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1002 - val_loss: 0.1176\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0929 - val_loss: 0.1181\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb435d65280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2322 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.5490 - val_loss: 0.0291\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4190 - val_loss: 0.0295\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3543 - val_loss: 0.0341\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb430bc79d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2323 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0737 - val_loss: 0.0599\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0616 - val_loss: 0.0594\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0461 - val_loss: 0.0593\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0393 - val_loss: 0.0593\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0347 - val_loss: 0.0595\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb46c6f09d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2324 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.4457 - val_loss: 0.0556\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3843 - val_loss: 0.0505\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2730 - val_loss: 0.0494\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1764 - val_loss: 0.0486\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1723 - val_loss: 0.0491\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1563 - val_loss: 0.0487\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb42e250af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2325 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.3222 - val_loss: 0.0385\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1727 - val_loss: 0.0366\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1071 - val_loss: 0.0350\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0772 - val_loss: 0.0329\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0998 - val_loss: 0.0313\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1294 - val_loss: 0.0299\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0726 - val_loss: 0.0283\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0824 - val_loss: 0.0260\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0717 - val_loss: 0.0247\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0533 - val_loss: 0.0234\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0544 - val_loss: 0.0222\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0517 - val_loss: 0.0211\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0582 - val_loss: 0.0204\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0659 - val_loss: 0.0197\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0566 - val_loss: 0.0192\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0650 - val_loss: 0.0189\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0572 - val_loss: 0.0189\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0545 - val_loss: 0.0188\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0478 - val_loss: 0.0188\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0498 - val_loss: 0.0187\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0443 - val_loss: 0.0186\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0433 - val_loss: 0.0185\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0522 - val_loss: 0.0184\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0419 - val_loss: 0.0184\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0479 - val_loss: 0.0182\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.048 - 0s 10ms/step - loss: 0.0461 - val_loss: 0.0184\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0389 - val_loss: 0.0184\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb44d495040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2326 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.3850 - val_loss: 0.0677\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.2023 - val_loss: 0.0678\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1658 - val_loss: 0.0678\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb464217ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2327 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.1239 - val_loss: 0.0839\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0832 - val_loss: 0.0810\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0765 - val_loss: 0.0783\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0630 - val_loss: 0.0753\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0472 - val_loss: 0.0729\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0421 - val_loss: 0.0697\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0303 - val_loss: 0.0677\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0382 - val_loss: 0.0657\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0359 - val_loss: 0.0643\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0298 - val_loss: 0.0632\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0239 - val_loss: 0.0622\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0292 - val_loss: 0.0608\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0335 - val_loss: 0.0598\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0229 - val_loss: 0.0586\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0258 - val_loss: 0.0575\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0337 - val_loss: 0.0566\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0257 - val_loss: 0.0564\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0240 - val_loss: 0.0556\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0157 - val_loss: 0.0550\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0274 - val_loss: 0.0552\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0173 - val_loss: 0.0546\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0186 - val_loss: 0.0539\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0222 - val_loss: 0.0534\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0199 - val_loss: 0.0524\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0255 - val_loss: 0.0517\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0181 - val_loss: 0.0516\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0177 - val_loss: 0.0507\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0188 - val_loss: 0.0503\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0209 - val_loss: 0.0495\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0225 - val_loss: 0.0483\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0172 - val_loss: 0.0481\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0140 - val_loss: 0.0475\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0167 - val_loss: 0.0468\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0164 - val_loss: 0.0462\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0222 - val_loss: 0.0459\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0183 - val_loss: 0.0450\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0146 - val_loss: 0.0440\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0179 - val_loss: 0.0426\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0287 - val_loss: 0.0420\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0192 - val_loss: 0.0408\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0251 - val_loss: 0.0401\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0156 - val_loss: 0.0394\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0171 - val_loss: 0.0385\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0162 - val_loss: 0.0373\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0197 - val_loss: 0.0364\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0130 - val_loss: 0.0356\n",
      "Epoch 47/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0186 - val_loss: 0.0346\n",
      "Epoch 48/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0189 - val_loss: 0.0341\n",
      "Epoch 49/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0173 - val_loss: 0.0328\n",
      "Epoch 50/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0125 - val_loss: 0.0321\n",
      "Epoch 51/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0169 - val_loss: 0.0306\n",
      "Epoch 52/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0124 - val_loss: 0.0298\n",
      "Epoch 53/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0128 - val_loss: 0.0292\n",
      "Epoch 54/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0137 - val_loss: 0.0281\n",
      "Epoch 55/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0123 - val_loss: 0.0271\n",
      "Epoch 56/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0147 - val_loss: 0.0266\n",
      "Epoch 57/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0134 - val_loss: 0.0256\n",
      "Epoch 58/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0136 - val_loss: 0.0251\n",
      "Epoch 59/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0104 - val_loss: 0.0244\n",
      "Epoch 60/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0128 - val_loss: 0.0238\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb471169550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2328 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 1.6330 - val_loss: 0.4511\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.9620 - val_loss: 0.4391\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.7217 - val_loss: 0.4176\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.4938 - val_loss: 0.3738\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.4061 - val_loss: 0.3400\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3426 - val_loss: 0.3222\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.2620 - val_loss: 0.3009\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2327 - val_loss: 0.2870\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1958 - val_loss: 0.2682\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1502 - val_loss: 0.2548\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1732 - val_loss: 0.2425\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1292 - val_loss: 0.2310\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1306 - val_loss: 0.2211\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1150 - val_loss: 0.2126\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0868 - val_loss: 0.2043\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0904 - val_loss: 0.1993\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0704 - val_loss: 0.1927\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0748 - val_loss: 0.1858\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0708 - val_loss: 0.1815\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0865 - val_loss: 0.1766\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0730 - val_loss: 0.1724\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0747 - val_loss: 0.1698\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0546 - val_loss: 0.1638\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0887 - val_loss: 0.1566\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0521 - val_loss: 0.1539\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0444 - val_loss: 0.1494\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0644 - val_loss: 0.1467\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0615 - val_loss: 0.1395\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0642 - val_loss: 0.1345\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0652 - val_loss: 0.1319\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0893 - val_loss: 0.1295\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0575 - val_loss: 0.1290\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0775 - val_loss: 0.1231\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0555 - val_loss: 0.1213\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0459 - val_loss: 0.1172\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0416 - val_loss: 0.1167\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0369 - val_loss: 0.1149\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0540 - val_loss: 0.1138\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0438 - val_loss: 0.1155\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0332 - val_loss: 0.1124\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0506 - val_loss: 0.1103\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0280 - val_loss: 0.1094\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0301 - val_loss: 0.1106\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0406 - val_loss: 0.1094\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0364 - val_loss: 0.1094\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0401 - val_loss: 0.1103\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4936b3d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2329 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.2184 - val_loss: 0.0176\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1714 - val_loss: 0.0184\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0722 - val_loss: 0.0189\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb45f7fff70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2330 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.7270 - val_loss: 0.0516\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.2758 - val_loss: 0.0515\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2104 - val_loss: 0.0510\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2013 - val_loss: 0.0505\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1349 - val_loss: 0.0500\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1423 - val_loss: 0.0500\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1254 - val_loss: 0.0495\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1209 - val_loss: 0.0485\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0770 - val_loss: 0.0479\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0679 - val_loss: 0.0471\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0781 - val_loss: 0.0469\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0675 - val_loss: 0.0472\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0560 - val_loss: 0.0482\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb45c59f040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2331 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.3087 - val_loss: 0.1434\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1966 - val_loss: 0.1433\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1335 - val_loss: 0.1413\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1301 - val_loss: 0.1381\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1049 - val_loss: 0.1345\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0914 - val_loss: 0.1317\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1346 - val_loss: 0.1294\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0917 - val_loss: 0.1256\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0719 - val_loss: 0.1233\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0621 - val_loss: 0.1194\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0613 - val_loss: 0.1145\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0539 - val_loss: 0.1117\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0704 - val_loss: 0.1064\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0596 - val_loss: 0.1019\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0519 - val_loss: 0.0979\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0426 - val_loss: 0.0930\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0768 - val_loss: 0.0888\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0498 - val_loss: 0.0825\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0439 - val_loss: 0.0762\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0344 - val_loss: 0.0706\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0424 - val_loss: 0.0660\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0410 - val_loss: 0.0598\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0472 - val_loss: 0.0580\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0440 - val_loss: 0.0514\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0255 - val_loss: 0.0476\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0276 - val_loss: 0.0440\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0302 - val_loss: 0.0450\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0313 - val_loss: 0.0398\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0379 - val_loss: 0.0388\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0319 - val_loss: 0.0365\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0256 - val_loss: 0.0331\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0196 - val_loss: 0.0308\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0251 - val_loss: 0.0307\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0219 - val_loss: 0.0303\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0171 - val_loss: 0.0275\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0385 - val_loss: 0.0270\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0175 - val_loss: 0.0268\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0249 - val_loss: 0.0258\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0257 - val_loss: 0.0253\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0220 - val_loss: 0.0249\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0238 - val_loss: 0.0252\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0252 - val_loss: 0.0247\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0809 - val_loss: 0.0239\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0217 - val_loss: 0.0243\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0223 - val_loss: 0.0235\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0348 - val_loss: 0.0240\n",
      "Epoch 47/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0273 - val_loss: 0.0248\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb45689e790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2332 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 0.1910 - val_loss: 0.0364\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1055 - val_loss: 0.0369\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0825 - val_loss: 0.0372\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb44dd70ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2333 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.2448 - val_loss: 0.1536\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1579 - val_loss: 0.1526\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1285 - val_loss: 0.1530\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0919 - val_loss: 0.1514\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0681 - val_loss: 0.1507\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0642 - val_loss: 0.1496\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0531 - val_loss: 0.1492\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0491 - val_loss: 0.1485\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0435 - val_loss: 0.1469\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0386 - val_loss: 0.1452\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0421 - val_loss: 0.1432\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0429 - val_loss: 0.1420\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0360 - val_loss: 0.1416\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0357 - val_loss: 0.1397\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0318 - val_loss: 0.1381\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0323 - val_loss: 0.1365\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0364 - val_loss: 0.1349\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0334 - val_loss: 0.1327\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0272 - val_loss: 0.1311\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0355 - val_loss: 0.1279\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0408 - val_loss: 0.1264\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0282 - val_loss: 0.1234\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0301 - val_loss: 0.1211\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0244 - val_loss: 0.1180\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0408 - val_loss: 0.1153\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0240 - val_loss: 0.1128\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0228 - val_loss: 0.1104\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0331 - val_loss: 0.1072\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0219 - val_loss: 0.1043\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0182 - val_loss: 0.1010\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0225 - val_loss: 0.0982\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0245 - val_loss: 0.0954\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0191 - val_loss: 0.0935\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0256 - val_loss: 0.0920\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0233 - val_loss: 0.0907\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0217 - val_loss: 0.0893\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0223 - val_loss: 0.0874\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0181 - val_loss: 0.0854\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0199 - val_loss: 0.0835\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0156 - val_loss: 0.0815\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0351 - val_loss: 0.0802\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0172 - val_loss: 0.0786\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0284 - val_loss: 0.0773\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0183 - val_loss: 0.0760\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.017 - 0s 11ms/step - loss: 0.0193 - val_loss: 0.0748\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0159 - val_loss: 0.0735\n",
      "Epoch 47/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0223 - val_loss: 0.0732\n",
      "Epoch 48/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0154 - val_loss: 0.0720\n",
      "Epoch 49/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0174 - val_loss: 0.0701\n",
      "Epoch 50/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0196 - val_loss: 0.0694\n",
      "Epoch 51/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0168 - val_loss: 0.0686\n",
      "Epoch 52/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0194 - val_loss: 0.0681\n",
      "Epoch 53/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0194 - val_loss: 0.0688\n",
      "Epoch 54/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0184 - val_loss: 0.0670\n",
      "Epoch 55/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0231 - val_loss: 0.0697\n",
      "Epoch 56/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0145 - val_loss: 0.0690\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4335d8f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2334 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.8059 - val_loss: 0.0293\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.6528 - val_loss: 0.0314\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.4313 - val_loss: 0.0333\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb42d773820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2335 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 0.1433 - val_loss: 0.1001\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0913 - val_loss: 0.1031\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0619 - val_loss: 0.1068\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb43256f670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2336 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 0.3423 - val_loss: 0.0346\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2138 - val_loss: 0.0366\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1865 - val_loss: 0.0391\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb41b819a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2337 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.2030WARNING:tensorflow:5 out of the last 66 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb47c0a9c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.2036 - val_loss: 0.0286\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1566 - val_loss: 0.0268\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1284 - val_loss: 0.0251\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1071 - val_loss: 0.0236\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0946 - val_loss: 0.0225\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0696 - val_loss: 0.0219\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0737 - val_loss: 0.0216\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0646 - val_loss: 0.0217\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0613 - val_loss: 0.0224\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb466cadf70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2338 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.3239 - val_loss: 0.0304\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2390 - val_loss: 0.0301\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1981 - val_loss: 0.0298\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1666 - val_loss: 0.0298\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1436 - val_loss: 0.0303\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb426c6c700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2339 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.3539 - val_loss: 0.0526\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2466 - val_loss: 0.0521\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1763 - val_loss: 0.0512\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1292 - val_loss: 0.0497\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1227 - val_loss: 0.0484\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0905 - val_loss: 0.0475\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0830 - val_loss: 0.0466\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0779 - val_loss: 0.0459\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0603 - val_loss: 0.0455\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0621 - val_loss: 0.0451\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0528 - val_loss: 0.0451\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0516 - val_loss: 0.0453\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4132fa820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2340 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.1368 - val_loss: 0.0781\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0857 - val_loss: 0.0757\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0676 - val_loss: 0.0738\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0636 - val_loss: 0.0716\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0577 - val_loss: 0.0690\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0474 - val_loss: 0.0660\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0444 - val_loss: 0.0631\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0510 - val_loss: 0.0599\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0409 - val_loss: 0.0563\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0471 - val_loss: 0.0528\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0327 - val_loss: 0.0500\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0387 - val_loss: 0.0477\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0374 - val_loss: 0.0448\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0572 - val_loss: 0.0424\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0358 - val_loss: 0.0397\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0337 - val_loss: 0.0378\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0334 - val_loss: 0.0363\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0294 - val_loss: 0.0350\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0321 - val_loss: 0.0337\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0322 - val_loss: 0.0327\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0309 - val_loss: 0.0322\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0351 - val_loss: 0.0319\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0281 - val_loss: 0.0314\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0353 - val_loss: 0.0314\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0261 - val_loss: 0.0310\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0404 - val_loss: 0.0308\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0271 - val_loss: 0.0302\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0255 - val_loss: 0.0298\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0285 - val_loss: 0.0288\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0234 - val_loss: 0.0283\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0209 - val_loss: 0.0280\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0204 - val_loss: 0.0275\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0212 - val_loss: 0.0272\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0268 - val_loss: 0.0266\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0245 - val_loss: 0.0265\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0357 - val_loss: 0.0267\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0299 - val_loss: 0.0265\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0359 - val_loss: 0.0269\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0222 - val_loss: 0.0268\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb41477b5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2341 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.4894 - val_loss: 0.0670\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.3054 - val_loss: 0.0707\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.2188 - val_loss: 0.0717\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb41bda49d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2342 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.4059 - val_loss: 0.0489\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2633 - val_loss: 0.0470\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1590 - val_loss: 0.0403\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1107 - val_loss: 0.0367\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0903 - val_loss: 0.0334\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1043 - val_loss: 0.0313\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0855 - val_loss: 0.0301\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0550 - val_loss: 0.0296\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0759 - val_loss: 0.0281\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0763 - val_loss: 0.0273\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0537 - val_loss: 0.0265\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0426 - val_loss: 0.0261\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0484 - val_loss: 0.0258\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0886 - val_loss: 0.0257\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0465 - val_loss: 0.0254\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0504 - val_loss: 0.0259\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0399 - val_loss: 0.0257\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4e9c23550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2343 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 215ms/step - loss: 0.1850 - val_loss: 0.2663\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0948 - val_loss: 0.2521\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1255 - val_loss: 0.2407\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0772 - val_loss: 0.2297\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0776 - val_loss: 0.2203\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0764 - val_loss: 0.2103\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0589 - val_loss: 0.2024\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0910 - val_loss: 0.1932\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0641 - val_loss: 0.1860\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0537 - val_loss: 0.1782\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0551 - val_loss: 0.1730\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0501 - val_loss: 0.1676\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0513 - val_loss: 0.1596\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.050 - 0s 10ms/step - loss: 0.0515 - val_loss: 0.1561\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0336 - val_loss: 0.1540\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0429 - val_loss: 0.1522\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0572 - val_loss: 0.1496\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0355 - val_loss: 0.1486\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0423 - val_loss: 0.1503\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0392 - val_loss: 0.1528\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb474d73310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2344 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 1.3811 - val_loss: 0.4185\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6096 - val_loss: 0.3958\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3947 - val_loss: 0.3862\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3461 - val_loss: 0.3732\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.3120 - val_loss: 0.3619\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2552 - val_loss: 0.3484\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2258 - val_loss: 0.3333\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2272 - val_loss: 0.3216\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1556 - val_loss: 0.3061\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1872 - val_loss: 0.2966\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1349 - val_loss: 0.2843\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1235 - val_loss: 0.2707\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1047 - val_loss: 0.2597\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0991 - val_loss: 0.2493\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0822 - val_loss: 0.2421\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0834 - val_loss: 0.2312\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1045 - val_loss: 0.2232\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0905 - val_loss: 0.2174\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0638 - val_loss: 0.2115\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0780 - val_loss: 0.2074\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0671 - val_loss: 0.2009\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0822 - val_loss: 0.1958\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0602 - val_loss: 0.1883\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0523 - val_loss: 0.1848\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0440 - val_loss: 0.1826\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0447 - val_loss: 0.1816\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0671 - val_loss: 0.1825\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0472 - val_loss: 0.1779\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0392 - val_loss: 0.1750\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0522 - val_loss: 0.1744\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0342 - val_loss: 0.1719\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0602 - val_loss: 0.1670\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0322 - val_loss: 0.1666\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0300 - val_loss: 0.1661\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0324 - val_loss: 0.1644\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0300 - val_loss: 0.1617\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0311 - val_loss: 0.1617\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0247 - val_loss: 0.1622\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0319 - val_loss: 0.1608\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0272 - val_loss: 0.1625\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0310 - val_loss: 0.1645\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4271db0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2345 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 0.2269 - val_loss: 0.0298\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1538 - val_loss: 0.0307\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1300 - val_loss: 0.0315\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb43159bdc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2346 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 0.2104 - val_loss: 0.0286\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1608 - val_loss: 0.0307\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1253 - val_loss: 0.0324\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb406697700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2347 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 0.2180 - val_loss: 0.0795\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1397 - val_loss: 0.0794\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1093 - val_loss: 0.0793\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0890 - val_loss: 0.0789\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0862 - val_loss: 0.0796\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0641 - val_loss: 0.0797\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ef0a2310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2348 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 209ms/step - loss: 0.5131 - val_loss: 0.0682\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.3150 - val_loss: 0.0699\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2127 - val_loss: 0.0748\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3f04a4a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2349 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 0.8393 - val_loss: 0.0269\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4106 - val_loss: 0.0266\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2735 - val_loss: 0.0267\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2818 - val_loss: 0.0267\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3f4d0e8b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2350 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.6838 - val_loss: 0.0555\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3715 - val_loss: 0.0509\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2393 - val_loss: 0.0500\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1947 - val_loss: 0.0486\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1485 - val_loss: 0.0502\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1377 - val_loss: 0.0505\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb400cf9160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2351 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.8346 - val_loss: 0.0536\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.5475 - val_loss: 0.0467\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3072 - val_loss: 0.0420\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.2676 - val_loss: 0.0401\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1801 - val_loss: 0.0377\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1606 - val_loss: 0.0358\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1322 - val_loss: 0.0347\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0771 - val_loss: 0.0338\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0761 - val_loss: 0.0332\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0683 - val_loss: 0.0325\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0546 - val_loss: 0.0319\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0571 - val_loss: 0.0314\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0461 - val_loss: 0.0309\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0448 - val_loss: 0.0305\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0457 - val_loss: 0.0300\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0375 - val_loss: 0.0296\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0361 - val_loss: 0.0291\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0434 - val_loss: 0.0285\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0352 - val_loss: 0.0280\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0383 - val_loss: 0.0274\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0333 - val_loss: 0.0269\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0294 - val_loss: 0.0262\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0296 - val_loss: 0.0257\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0573 - val_loss: 0.0253\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0379 - val_loss: 0.0248\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0556 - val_loss: 0.0243\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0311 - val_loss: 0.0241\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0319 - val_loss: 0.0239\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0256 - val_loss: 0.0238\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0250 - val_loss: 0.0239\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0274 - val_loss: 0.0240\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb417fc6430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2352 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.4278 - val_loss: 0.0303\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.2147 - val_loss: 0.0306\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2201 - val_loss: 0.0306\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb43b2f1310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2353 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 1.3063 - val_loss: 0.0261\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6095 - val_loss: 0.0265\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4238 - val_loss: 0.0274\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4b1af8790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2354 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.7860 - val_loss: 0.2401\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5422 - val_loss: 0.2210\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3544 - val_loss: 0.2099\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.2317 - val_loss: 0.1974\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1995 - val_loss: 0.1888\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1403 - val_loss: 0.1828\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1090 - val_loss: 0.1792\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1056 - val_loss: 0.1760\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1051 - val_loss: 0.1715\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0887 - val_loss: 0.1681\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0721 - val_loss: 0.1662\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0705 - val_loss: 0.1645\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0572 - val_loss: 0.1631\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0591 - val_loss: 0.1619\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0569 - val_loss: 0.1594\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0866 - val_loss: 0.1584\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0687 - val_loss: 0.1571\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0630 - val_loss: 0.1563\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0506 - val_loss: 0.1559\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0538 - val_loss: 0.1551\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0420 - val_loss: 0.1550\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0400 - val_loss: 0.1546\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0668 - val_loss: 0.1533\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0474 - val_loss: 0.1532\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0641 - val_loss: 0.1539\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0337 - val_loss: 0.1557\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3f7a430d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2355 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.4126 - val_loss: 0.0726\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1992 - val_loss: 0.0717\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1817 - val_loss: 0.0696\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0982 - val_loss: 0.0683\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1345 - val_loss: 0.0666\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1194 - val_loss: 0.0654\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0861 - val_loss: 0.0648\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0793 - val_loss: 0.0639\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1489 - val_loss: 0.0626\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0806 - val_loss: 0.0619\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0657 - val_loss: 0.0613\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0469 - val_loss: 0.0610\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0446 - val_loss: 0.0609\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0388 - val_loss: 0.0604\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0928 - val_loss: 0.0602\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0433 - val_loss: 0.0592\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0505 - val_loss: 0.0580\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0805 - val_loss: 0.0571\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0464 - val_loss: 0.0559\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0458 - val_loss: 0.0541\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0326 - val_loss: 0.0523\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0371 - val_loss: 0.0509\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0394 - val_loss: 0.0512\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0396 - val_loss: 0.0512\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e21343a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2356 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.3685 - val_loss: 0.0404\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.2228 - val_loss: 0.0402\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1931 - val_loss: 0.0393\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1855 - val_loss: 0.0386\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1863 - val_loss: 0.0371\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1272 - val_loss: 0.0366\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1311 - val_loss: 0.0357\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1169 - val_loss: 0.0356\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0934 - val_loss: 0.0347\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0891 - val_loss: 0.0341\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0782 - val_loss: 0.0333\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0751 - val_loss: 0.0332\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0672 - val_loss: 0.0327\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1096 - val_loss: 0.0323\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0734 - val_loss: 0.0325\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0630 - val_loss: 0.0328\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e612fca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2357 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.1572 - val_loss: 0.0292\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1294 - val_loss: 0.0294\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1049 - val_loss: 0.0293\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3fd02b310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2358 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.6456 - val_loss: 0.0698\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.2575 - val_loss: 0.0675\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1684 - val_loss: 0.0659\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1374 - val_loss: 0.0650\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1471 - val_loss: 0.0625\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1154 - val_loss: 0.0617\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1062 - val_loss: 0.0602\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0969 - val_loss: 0.0592\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0827 - val_loss: 0.0593\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0845 - val_loss: 0.0597\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e07e6c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2359 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.6702 - val_loss: 0.1414\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4224 - val_loss: 0.1451\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2427 - val_loss: 0.1439\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3dbb12940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2360 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.7969 - val_loss: 0.2479\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5604 - val_loss: 0.2382\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4006 - val_loss: 0.2348\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3150 - val_loss: 0.2295\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2918 - val_loss: 0.2257\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2528 - val_loss: 0.2206\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2001 - val_loss: 0.2163\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1928 - val_loss: 0.2083\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2179 - val_loss: 0.2003\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1889 - val_loss: 0.1938\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1446 - val_loss: 0.1861\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1405 - val_loss: 0.1751\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1243 - val_loss: 0.1629\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1318 - val_loss: 0.1499\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0963 - val_loss: 0.1389\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1105 - val_loss: 0.1289\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1014 - val_loss: 0.1202\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0972 - val_loss: 0.1111\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0803 - val_loss: 0.1023\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0770 - val_loss: 0.0951\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0833 - val_loss: 0.0896\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0865 - val_loss: 0.0831\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0744 - val_loss: 0.0782\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0679 - val_loss: 0.0736\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0603 - val_loss: 0.0688\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0702 - val_loss: 0.0642\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0525 - val_loss: 0.0606\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0706 - val_loss: 0.0570\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0692 - val_loss: 0.0535\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0526 - val_loss: 0.0506\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0401 - val_loss: 0.0475\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0512 - val_loss: 0.0445\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0407 - val_loss: 0.0425\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0590 - val_loss: 0.0402\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0483 - val_loss: 0.0380\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0428 - val_loss: 0.0366\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0367 - val_loss: 0.0351\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0414 - val_loss: 0.0338\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0325 - val_loss: 0.0325\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0352 - val_loss: 0.0318\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0389 - val_loss: 0.0313\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0311 - val_loss: 0.0309\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0311 - val_loss: 0.0306\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0403 - val_loss: 0.0305\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0302 - val_loss: 0.0304\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0402 - val_loss: 0.0306\n",
      "Epoch 47/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0323 - val_loss: 0.0305\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3fe2715e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2361 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.2752 - val_loss: 0.0578\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1277 - val_loss: 0.0614\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0865 - val_loss: 0.0655\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d95a6160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2362 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.6111 - val_loss: 0.0604\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3496 - val_loss: 0.0660\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2722 - val_loss: 0.0728\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cae010d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2363 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.3335 - val_loss: 0.0471\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1873 - val_loss: 0.0451\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1053 - val_loss: 0.0439\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0829 - val_loss: 0.0420\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0658 - val_loss: 0.0400\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0553 - val_loss: 0.0385\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0460 - val_loss: 0.0369\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0373 - val_loss: 0.0355\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0437 - val_loss: 0.0340\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0429 - val_loss: 0.0329\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0318 - val_loss: 0.0313\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0269 - val_loss: 0.0306\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0275 - val_loss: 0.0291\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0271 - val_loss: 0.0281\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0456 - val_loss: 0.0269\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0201 - val_loss: 0.0259\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0212 - val_loss: 0.0253\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0294 - val_loss: 0.0244\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0304 - val_loss: 0.0239\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0190 - val_loss: 0.0234\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0239 - val_loss: 0.0228\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0185 - val_loss: 0.0225\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0210 - val_loss: 0.0222\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0212 - val_loss: 0.0222\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0269 - val_loss: 0.0220\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0244 - val_loss: 0.0220\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0238 - val_loss: 0.0222\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0203 - val_loss: 0.0225\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cb1f80d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2364 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.2342 - val_loss: 0.2701\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1279 - val_loss: 0.2680\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0953 - val_loss: 0.2584\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0895 - val_loss: 0.2507\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0843 - val_loss: 0.2433\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0903 - val_loss: 0.2345\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0674 - val_loss: 0.2280\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0564 - val_loss: 0.2205\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0581 - val_loss: 0.2138\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0585 - val_loss: 0.2048\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0505 - val_loss: 0.2009\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0448 - val_loss: 0.1941\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0429 - val_loss: 0.1878\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0461 - val_loss: 0.1807\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0443 - val_loss: 0.1735\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0500 - val_loss: 0.1672\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0472 - val_loss: 0.1637\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0340 - val_loss: 0.1578\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0332 - val_loss: 0.1528\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0341 - val_loss: 0.1498\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0330 - val_loss: 0.1444\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0316 - val_loss: 0.1426\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0318 - val_loss: 0.1375\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0267 - val_loss: 0.1328\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0282 - val_loss: 0.1294\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0305 - val_loss: 0.1260\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0282 - val_loss: 0.1261\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0261 - val_loss: 0.1250\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0278 - val_loss: 0.1237\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0288 - val_loss: 0.1245\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0242 - val_loss: 0.1246\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cb538dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2365 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.1421 - val_loss: 0.0992\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0909 - val_loss: 0.1023\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0670 - val_loss: 0.1047\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cb934c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2366 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 1.1661 - val_loss: 0.9561\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6707 - val_loss: 0.8930\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.4444 - val_loss: 0.8553\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2991 - val_loss: 0.8149\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2621 - val_loss: 0.7836\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2797 - val_loss: 0.7570\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.2199 - val_loss: 0.6993\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2142 - val_loss: 0.6719\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1590 - val_loss: 0.6389\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1391 - val_loss: 0.6102\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1019 - val_loss: 0.5884\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1091 - val_loss: 0.5573\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0895 - val_loss: 0.5408\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0857 - val_loss: 0.5281\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0704 - val_loss: 0.5137\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0839 - val_loss: 0.5007\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0642 - val_loss: 0.4906\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0635 - val_loss: 0.4745\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0688 - val_loss: 0.4603\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0600 - val_loss: 0.4469\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0597 - val_loss: 0.4363\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0796 - val_loss: 0.4283\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0552 - val_loss: 0.4141\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0492 - val_loss: 0.4016\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0502 - val_loss: 0.3949\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0515 - val_loss: 0.3809\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0437 - val_loss: 0.3714\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0446 - val_loss: 0.3602\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0431 - val_loss: 0.3531\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0722 - val_loss: 0.3458\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0361 - val_loss: 0.3356\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0443 - val_loss: 0.3261\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0360 - val_loss: 0.3154\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0554 - val_loss: 0.3054\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0320 - val_loss: 0.2956\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0352 - val_loss: 0.2868\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0439 - val_loss: 0.2794\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0338 - val_loss: 0.2703\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0346 - val_loss: 0.2639\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0265 - val_loss: 0.2532\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0427 - val_loss: 0.2506\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0401 - val_loss: 0.2441\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0275 - val_loss: 0.2366\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0325 - val_loss: 0.2320\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0274 - val_loss: 0.2254\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0274 - val_loss: 0.2190\n",
      "Epoch 47/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0248 - val_loss: 0.2156\n",
      "Epoch 48/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0292 - val_loss: 0.2073\n",
      "Epoch 49/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0478 - val_loss: 0.2096\n",
      "Epoch 50/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0368 - val_loss: 0.2002\n",
      "Epoch 51/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0206 - val_loss: 0.1942\n",
      "Epoch 52/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0283 - val_loss: 0.1904\n",
      "Epoch 53/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0302 - val_loss: 0.1850\n",
      "Epoch 54/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0200 - val_loss: 0.1792\n",
      "Epoch 55/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0234 - val_loss: 0.1794\n",
      "Epoch 56/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0252 - val_loss: 0.1723\n",
      "Epoch 57/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0410 - val_loss: 0.1735\n",
      "Epoch 58/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0215 - val_loss: 0.1689\n",
      "Epoch 59/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0232 - val_loss: 0.1643\n",
      "Epoch 60/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0333 - val_loss: 0.1676\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cbd29790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2367 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.3279 - val_loss: 0.1250\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1822 - val_loss: 0.1191\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1443 - val_loss: 0.1142\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1481 - val_loss: 0.1099\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0976 - val_loss: 0.1057\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1035 - val_loss: 0.1017\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0828 - val_loss: 0.0978\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0760 - val_loss: 0.0943\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0731 - val_loss: 0.0907\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0645 - val_loss: 0.0878\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0687 - val_loss: 0.0852\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0533 - val_loss: 0.0826\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0625 - val_loss: 0.0804\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0579 - val_loss: 0.0784\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0434 - val_loss: 0.0764\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0487 - val_loss: 0.0748\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0474 - val_loss: 0.0737\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0459 - val_loss: 0.0731\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0332 - val_loss: 0.0728\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0382 - val_loss: 0.0728\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0355 - val_loss: 0.0731\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0361 - val_loss: 0.0736\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cc1275e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2368 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.3744 - val_loss: 0.0591\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2295 - val_loss: 0.0572\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1601 - val_loss: 0.0561\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1529 - val_loss: 0.0550\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1744 - val_loss: 0.0541\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1172 - val_loss: 0.0541\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1053 - val_loss: 0.0548\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0613 - val_loss: 0.0552\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cc54f310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2369 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.3155 - val_loss: 0.1937\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1750 - val_loss: 0.1785\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1339 - val_loss: 0.1668\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0953 - val_loss: 0.1579\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0834 - val_loss: 0.1478\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0618 - val_loss: 0.1408\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0412 - val_loss: 0.1330\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0404 - val_loss: 0.1255\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0644 - val_loss: 0.1181\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0353 - val_loss: 0.1146\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0348 - val_loss: 0.1102\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0374 - val_loss: 0.1057\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0324 - val_loss: 0.1038\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0345 - val_loss: 0.1001\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0378 - val_loss: 0.0969\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0303 - val_loss: 0.0940\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0272 - val_loss: 0.0914\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0264 - val_loss: 0.0896\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0250 - val_loss: 0.0864\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0232 - val_loss: 0.0838\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0289 - val_loss: 0.0829\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0197 - val_loss: 0.0810\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0184 - val_loss: 0.0790\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0233 - val_loss: 0.0759\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0209 - val_loss: 0.0741\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0211 - val_loss: 0.0715\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0224 - val_loss: 0.0687\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0260 - val_loss: 0.0667\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0212 - val_loss: 0.0642\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0228 - val_loss: 0.0620\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0179 - val_loss: 0.0600\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0161 - val_loss: 0.0574\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0253 - val_loss: 0.0549\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0181 - val_loss: 0.0527\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0244 - val_loss: 0.0504\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0225 - val_loss: 0.0489\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0204 - val_loss: 0.0469\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0174 - val_loss: 0.0452\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0145 - val_loss: 0.0433\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0204 - val_loss: 0.0417\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0162 - val_loss: 0.0404\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0182 - val_loss: 0.0395\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0143 - val_loss: 0.0391\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0162 - val_loss: 0.0388\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0211 - val_loss: 0.0387\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0160 - val_loss: 0.0377\n",
      "Epoch 47/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0187 - val_loss: 0.0364\n",
      "Epoch 48/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0154 - val_loss: 0.0358\n",
      "Epoch 49/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0143 - val_loss: 0.0351\n",
      "Epoch 50/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0326 - val_loss: 0.0348\n",
      "Epoch 51/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0132 - val_loss: 0.0344\n",
      "Epoch 52/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0114 - val_loss: 0.0345\n",
      "Epoch 53/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0131 - val_loss: 0.0343\n",
      "Epoch 54/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0146 - val_loss: 0.0345\n",
      "Epoch 55/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0118 - val_loss: 0.0346\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cc9ec160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2370 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 0.1683 - val_loss: 0.2720\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1301 - val_loss: 0.2773\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1211 - val_loss: 0.2830\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ccd24700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2371 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.6003 - val_loss: 0.1338\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.4024 - val_loss: 0.1272\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3653 - val_loss: 0.1228\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2900 - val_loss: 0.1153\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.2249 - val_loss: 0.1132\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1993 - val_loss: 0.1129\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1477 - val_loss: 0.1084\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1405 - val_loss: 0.1071\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1140 - val_loss: 0.1063\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0867 - val_loss: 0.1071\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1023 - val_loss: 0.1054\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0759 - val_loss: 0.1047\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0634 - val_loss: 0.1039\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0893 - val_loss: 0.1033\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0719 - val_loss: 0.1073\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0556 - val_loss: 0.1078\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cd1e1dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2372 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0889 - val_loss: 0.0960\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0691 - val_loss: 0.0941\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0574 - val_loss: 0.0916\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0601 - val_loss: 0.0893\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0374 - val_loss: 0.0872\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0430 - val_loss: 0.0850\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0362 - val_loss: 0.0835\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0267 - val_loss: 0.0828\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0311 - val_loss: 0.0835\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0392 - val_loss: 0.0845\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cfa07700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2373 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.3510 - val_loss: 0.0712\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2297 - val_loss: 0.0716\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1760 - val_loss: 0.0707\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1571 - val_loss: 0.0705\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1033 - val_loss: 0.0700\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0744 - val_loss: 0.0691\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0785 - val_loss: 0.0691\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0724 - val_loss: 0.0702\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0472 - val_loss: 0.0697\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d0797820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2374 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.1478 - val_loss: 0.0989\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1027 - val_loss: 0.0991\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0829 - val_loss: 0.0996\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d350f0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2375 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 0.1164 - val_loss: 0.1942\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1428 - val_loss: 0.1746\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1023 - val_loss: 0.1563\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0734 - val_loss: 0.1388\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0667 - val_loss: 0.1239\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0581 - val_loss: 0.1101\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0565 - val_loss: 0.0983\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0446 - val_loss: 0.0889\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0408 - val_loss: 0.0812\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0405 - val_loss: 0.0746\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0388 - val_loss: 0.0686\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0384 - val_loss: 0.0642\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0341 - val_loss: 0.0624\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0360 - val_loss: 0.0612\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0395 - val_loss: 0.0621\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0350 - val_loss: 0.0641\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d3f3e160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2376 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.6550 - val_loss: 0.0334\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3867 - val_loss: 0.0330\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2691 - val_loss: 0.0331\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2150 - val_loss: 0.0321\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1561 - val_loss: 0.0315\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1655 - val_loss: 0.0310\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1540 - val_loss: 0.0303\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0934 - val_loss: 0.0296\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1188 - val_loss: 0.0292\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0807 - val_loss: 0.0286\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0837 - val_loss: 0.0281\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0671 - val_loss: 0.0279\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0589 - val_loss: 0.0275\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1000 - val_loss: 0.0276\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0711 - val_loss: 0.0281\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d4cfc790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2377 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.5100 - val_loss: 0.0595\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2727 - val_loss: 0.0587\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.2492 - val_loss: 0.0557\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1783 - val_loss: 0.0541\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1394 - val_loss: 0.0519\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1161 - val_loss: 0.0505\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0844 - val_loss: 0.0511\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0876 - val_loss: 0.0515\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d7669430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2378 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 1.1756 - val_loss: 0.0279\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5770 - val_loss: 0.0267\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5219 - val_loss: 0.0255\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3676 - val_loss: 0.0249\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2780 - val_loss: 0.0243\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.2157 - val_loss: 0.0238\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1890 - val_loss: 0.0235\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1372 - val_loss: 0.0233\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1409 - val_loss: 0.0227\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1246 - val_loss: 0.0222\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1276 - val_loss: 0.0218\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1043 - val_loss: 0.0212\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1569 - val_loss: 0.0212\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1084 - val_loss: 0.0204\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1101 - val_loss: 0.0200\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0990 - val_loss: 0.0197\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0743 - val_loss: 0.0192\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0773 - val_loss: 0.0191\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0687 - val_loss: 0.0190\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0866 - val_loss: 0.0186\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0921 - val_loss: 0.0193\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0874 - val_loss: 0.0181\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0622 - val_loss: 0.0182\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0631 - val_loss: 0.0176\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0690 - val_loss: 0.0173\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0528 - val_loss: 0.0174\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0528 - val_loss: 0.0171\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0503 - val_loss: 0.0170\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0518 - val_loss: 0.0171\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0600 - val_loss: 0.0173\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb400cf9670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2379 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.1837 - val_loss: 0.0218\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1117 - val_loss: 0.0217\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1070 - val_loss: 0.0214\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0786 - val_loss: 0.0211\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0783 - val_loss: 0.0210\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0806 - val_loss: 0.0208\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0698 - val_loss: 0.0208\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0534 - val_loss: 0.0207\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0502 - val_loss: 0.0207\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0634 - val_loss: 0.0209\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb44d43fee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2380 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.8986 - val_loss: 0.1316\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.5693 - val_loss: 0.0916\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3651 - val_loss: 0.0940\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3470 - val_loss: 0.0906\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2646 - val_loss: 0.0891\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1772 - val_loss: 0.0905\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1424 - val_loss: 0.0936\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb42bdd0820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2381 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.4571 - val_loss: 0.0251\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1896 - val_loss: 0.0252\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0983 - val_loss: 0.0252\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4189fd3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2382 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 151ms/step - loss: 0.1741 - val_loss: 0.0182\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0858 - val_loss: 0.0182\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0767 - val_loss: 0.0182\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0632 - val_loss: 0.0183\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4e53ad3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2383 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 1.4148 - val_loss: 0.1050\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.9542 - val_loss: 0.0966\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.8273 - val_loss: 0.0887\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.7339 - val_loss: 0.0817\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.6900 - val_loss: 0.0742\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.5670 - val_loss: 0.0691\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.4938 - val_loss: 0.0651\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.4739 - val_loss: 0.0615\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.4793 - val_loss: 0.0590\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.4360 - val_loss: 0.0562\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3694 - val_loss: 0.0538\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.3413 - val_loss: 0.0529\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3355 - val_loss: 0.0507\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.3107 - val_loss: 0.0481\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.3267 - val_loss: 0.0465\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.2545 - val_loss: 0.0457\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2610 - val_loss: 0.0444\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2542 - val_loss: 0.0428\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2434 - val_loss: 0.0421\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2180 - val_loss: 0.0403\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1984 - val_loss: 0.0394\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1835 - val_loss: 0.0392\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1961 - val_loss: 0.0386\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2197 - val_loss: 0.0379\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1653 - val_loss: 0.0372\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1612 - val_loss: 0.0353\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1610 - val_loss: 0.0334\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1466 - val_loss: 0.0318\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1672 - val_loss: 0.0297\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1407 - val_loss: 0.0275\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1252 - val_loss: 0.0260\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1313 - val_loss: 0.0244\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1166 - val_loss: 0.0231\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1249 - val_loss: 0.0224\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1294 - val_loss: 0.0216\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1200 - val_loss: 0.0217\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1146 - val_loss: 0.0213\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0969 - val_loss: 0.0215\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1219 - val_loss: 0.0219\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d48d5940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2384 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.1916 - val_loss: 0.0444\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1176 - val_loss: 0.0461\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0746 - val_loss: 0.0467\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d22460d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2385 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.4319 - val_loss: 0.0929\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.3028 - val_loss: 0.0867\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.2133 - val_loss: 0.0804\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1796 - val_loss: 0.0755\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1731 - val_loss: 0.0729\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1493 - val_loss: 0.0678\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1260 - val_loss: 0.0640\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1323 - val_loss: 0.0583\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1134 - val_loss: 0.0539\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1009 - val_loss: 0.0509\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0828 - val_loss: 0.0486\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0898 - val_loss: 0.0469\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0880 - val_loss: 0.0450\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0775 - val_loss: 0.0429\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0827 - val_loss: 0.0417\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0850 - val_loss: 0.0400\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0585 - val_loss: 0.0381\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0698 - val_loss: 0.0365\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0563 - val_loss: 0.0353\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0642 - val_loss: 0.0337\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0505 - val_loss: 0.0318\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0510 - val_loss: 0.0302\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0460 - val_loss: 0.0293\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0353 - val_loss: 0.0282\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0338 - val_loss: 0.0278\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0446 - val_loss: 0.0266\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0328 - val_loss: 0.0256\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0324 - val_loss: 0.0244\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0279 - val_loss: 0.0240\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0461 - val_loss: 0.0223\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0264 - val_loss: 0.0221\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0259 - val_loss: 0.0219\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0201 - val_loss: 0.0220\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0521 - val_loss: 0.0216\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0217 - val_loss: 0.0218\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0165 - val_loss: 0.0219\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ccba31f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2386 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.2175 - val_loss: 0.1116\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1229 - val_loss: 0.1088\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1150 - val_loss: 0.1054\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1094 - val_loss: 0.0987\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0832 - val_loss: 0.0919\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0804 - val_loss: 0.0860\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0596 - val_loss: 0.0799\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0390 - val_loss: 0.0734\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0559 - val_loss: 0.0678\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0461 - val_loss: 0.0641\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0508 - val_loss: 0.0605\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0504 - val_loss: 0.0569\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0455 - val_loss: 0.0540\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0386 - val_loss: 0.0514\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0348 - val_loss: 0.0484\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0322 - val_loss: 0.0456\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0418 - val_loss: 0.0430\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0392 - val_loss: 0.0395\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0343 - val_loss: 0.0373\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0348 - val_loss: 0.0353\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0328 - val_loss: 0.0331\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0219 - val_loss: 0.0309\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0353 - val_loss: 0.0289\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0293 - val_loss: 0.0273\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0280 - val_loss: 0.0265\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0253 - val_loss: 0.0255\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0335 - val_loss: 0.0251\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0191 - val_loss: 0.0247\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0421 - val_loss: 0.0247\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0242 - val_loss: 0.0247\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0311 - val_loss: 0.0248\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0308 - val_loss: 0.0247\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cc4a23a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2387 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 0.5397 - val_loss: 0.1416\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1951 - val_loss: 0.1428\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1611 - val_loss: 0.1453\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cbd29f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2388 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.6943 - val_loss: 0.0400\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2605 - val_loss: 0.0392\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1870 - val_loss: 0.0407\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1865 - val_loss: 0.0408\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cb6793a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2389 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.2821 - val_loss: 0.0610\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2442 - val_loss: 0.0606\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1734 - val_loss: 0.0591\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1437 - val_loss: 0.0578\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1163 - val_loss: 0.0573\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1152 - val_loss: 0.0562\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0870 - val_loss: 0.0551\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0808 - val_loss: 0.0540\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0824 - val_loss: 0.0530\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0667 - val_loss: 0.0519\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0617 - val_loss: 0.0513\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0587 - val_loss: 0.0508\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0553 - val_loss: 0.0496\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0415 - val_loss: 0.0494\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0429 - val_loss: 0.0493\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0470 - val_loss: 0.0485\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0402 - val_loss: 0.0484\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0298 - val_loss: 0.0482\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0317 - val_loss: 0.0481\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0307 - val_loss: 0.0481\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0303 - val_loss: 0.0481\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0281 - val_loss: 0.0483\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0335 - val_loss: 0.0484\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cb1f8b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2390 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.6948 - val_loss: 0.3980\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2963 - val_loss: 0.3974\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1524 - val_loss: 0.4047\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1173 - val_loss: 0.4107\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e0b12700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2391 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.5512 - val_loss: 0.0232\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1525 - val_loss: 0.0254\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1113 - val_loss: 0.0280\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3dabb3160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2392 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.1982 - val_loss: 0.0323\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1256 - val_loss: 0.0305\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0688 - val_loss: 0.0300\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0546 - val_loss: 0.0296\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0450 - val_loss: 0.0285\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0430 - val_loss: 0.0275\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0538 - val_loss: 0.0270\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0495 - val_loss: 0.0265\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0416 - val_loss: 0.0255\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0335 - val_loss: 0.0248\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0273 - val_loss: 0.0240\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0339 - val_loss: 0.0235\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0194 - val_loss: 0.0233\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0242 - val_loss: 0.0234\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0225 - val_loss: 0.0239\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e2134dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2393 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.3905 - val_loss: 0.0563\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1795 - val_loss: 0.0575\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1394 - val_loss: 0.0597\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3fc461f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2394 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.4606 - val_loss: 0.0461\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3423 - val_loss: 0.0452\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.2972 - val_loss: 0.0478\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1811 - val_loss: 0.0500\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb44d349790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2395 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.0780 - val_loss: 0.0504\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0552 - val_loss: 0.0484\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0457 - val_loss: 0.0468\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0412 - val_loss: 0.0453\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0438 - val_loss: 0.0438\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0403 - val_loss: 0.0423\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0402 - val_loss: 0.0409\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0359 - val_loss: 0.0398\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0345 - val_loss: 0.0390\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0332 - val_loss: 0.0384\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0292 - val_loss: 0.0380\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0255 - val_loss: 0.0376\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.037 - 0s 9ms/step - loss: 0.0315 - val_loss: 0.0375\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0244 - val_loss: 0.0375\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0240 - val_loss: 0.0377\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb401c8c4c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2396 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.4006 - val_loss: 0.0358\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1901 - val_loss: 0.0399\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1491 - val_loss: 0.0445\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3f4d861f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2397 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 1.0867 - val_loss: 0.0471\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.7216 - val_loss: 0.0421\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.5435 - val_loss: 0.0395\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.4241 - val_loss: 0.0374\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3549 - val_loss: 0.0358\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3191 - val_loss: 0.0345\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3206 - val_loss: 0.0330\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1903 - val_loss: 0.0323\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2233 - val_loss: 0.0318\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1598 - val_loss: 0.0323\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1663 - val_loss: 0.0327\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3f04e5b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2398 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.7155 - val_loss: 0.1371\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3603 - val_loss: 0.1399\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2537 - val_loss: 0.1404\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3eed9daf0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2399 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 1.1933 - val_loss: 0.4236\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.7324 - val_loss: 0.4007\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.6829 - val_loss: 0.3834\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.5375 - val_loss: 0.3663\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.4665 - val_loss: 0.3502\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.4236 - val_loss: 0.3363\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.4221 - val_loss: 0.3245\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3199 - val_loss: 0.3155\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3075 - val_loss: 0.3036\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2776 - val_loss: 0.2909\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2448 - val_loss: 0.2808\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.2259 - val_loss: 0.2718\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2410 - val_loss: 0.2621\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1995 - val_loss: 0.2552\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2280 - val_loss: 0.2448\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1945 - val_loss: 0.2388\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1856 - val_loss: 0.2340\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1585 - val_loss: 0.2292\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1670 - val_loss: 0.2256\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1584 - val_loss: 0.2217\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1448 - val_loss: 0.2188\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1430 - val_loss: 0.2160\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1433 - val_loss: 0.2119\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1257 - val_loss: 0.2104\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1198 - val_loss: 0.2079\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1171 - val_loss: 0.2045\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1057 - val_loss: 0.2029\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0866 - val_loss: 0.2027\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1181 - val_loss: 0.2020\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0837 - val_loss: 0.2003\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0750 - val_loss: 0.1984\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0940 - val_loss: 0.1959\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0704 - val_loss: 0.1942\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0612 - val_loss: 0.1930\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0724 - val_loss: 0.1913\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0669 - val_loss: 0.1914\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0564 - val_loss: 0.1898\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0487 - val_loss: 0.1884\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0542 - val_loss: 0.1893\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0524 - val_loss: 0.1887\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ffba98b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2400 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.1752 - val_loss: 0.1232\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1301 - val_loss: 0.1203\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1233 - val_loss: 0.1170\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0898 - val_loss: 0.1144\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0911 - val_loss: 0.1134\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0742 - val_loss: 0.1122\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0647 - val_loss: 0.1105\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0679 - val_loss: 0.1095\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0952 - val_loss: 0.1080\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0547 - val_loss: 0.1063\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.0498 - val_loss: 0.1046\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0574 - val_loss: 0.1041\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0351 - val_loss: 0.1034\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0624 - val_loss: 0.1022\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0358 - val_loss: 0.1019\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0463 - val_loss: 0.1009\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0444 - val_loss: 0.1017\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0434 - val_loss: 0.1012\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4040d6430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2401 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.7378 - val_loss: 0.1528\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4872 - val_loss: 0.1490\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2971 - val_loss: 0.1458\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2173 - val_loss: 0.1435\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1479 - val_loss: 0.1428\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1277 - val_loss: 0.1412\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0993 - val_loss: 0.1416\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0963 - val_loss: 0.1417\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb436a5b4c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2402 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.2202 - val_loss: 0.0245\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0958 - val_loss: 0.0257\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0722 - val_loss: 0.0267\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb461fad5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2403 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.3936 - val_loss: 0.0367\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.2993 - val_loss: 0.0333\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1835 - val_loss: 0.0312\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1419 - val_loss: 0.0291\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1546 - val_loss: 0.0275\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1139 - val_loss: 0.0261\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1055 - val_loss: 0.0247\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0807 - val_loss: 0.0237\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0894 - val_loss: 0.0226\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0758 - val_loss: 0.0217\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0823 - val_loss: 0.0209\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0695 - val_loss: 0.0203\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0779 - val_loss: 0.0198\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0683 - val_loss: 0.0195\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0526 - val_loss: 0.0194\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0643 - val_loss: 0.0187\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0464 - val_loss: 0.0185\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0473 - val_loss: 0.0184\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0436 - val_loss: 0.0184\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0455 - val_loss: 0.0184\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0403 - val_loss: 0.0183\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0505 - val_loss: 0.0184\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0521 - val_loss: 0.0188\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4176aed30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2404 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.1979 - val_loss: 0.0626\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1379 - val_loss: 0.0626\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1093 - val_loss: 0.0630\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4142f5ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2405 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.1937 - val_loss: 0.0178\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1366 - val_loss: 0.0181\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1066 - val_loss: 0.0182\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb412cab5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2406 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.2592 - val_loss: 0.0348\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1217 - val_loss: 0.0344\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0770 - val_loss: 0.0339\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0641 - val_loss: 0.0326\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0518 - val_loss: 0.0316\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0521 - val_loss: 0.0311\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0423 - val_loss: 0.0303\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0694 - val_loss: 0.0297\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0376 - val_loss: 0.0288\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0433 - val_loss: 0.0281\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0410 - val_loss: 0.0277\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0616 - val_loss: 0.0273\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0353 - val_loss: 0.0267\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0484 - val_loss: 0.0266\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0382 - val_loss: 0.0257\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0443 - val_loss: 0.0255\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0292 - val_loss: 0.0253\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0520 - val_loss: 0.0257\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0516 - val_loss: 0.0254\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb43b0570d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2407 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.1933 - val_loss: 0.0503\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1206 - val_loss: 0.0484\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0952 - val_loss: 0.0468\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0795 - val_loss: 0.0452\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0828 - val_loss: 0.0441\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0622 - val_loss: 0.0432\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0734 - val_loss: 0.0420\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0545 - val_loss: 0.0413\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0502 - val_loss: 0.0407\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0414 - val_loss: 0.0402\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0443 - val_loss: 0.0404\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0424 - val_loss: 0.0408\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb41a501f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2408 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.2150 - val_loss: 0.1074\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1323 - val_loss: 0.1079\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0937 - val_loss: 0.1087\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4343f1820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2409 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.2249 - val_loss: 0.1513\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1142 - val_loss: 0.1444\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0700 - val_loss: 0.1402\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0682 - val_loss: 0.1355\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0701 - val_loss: 0.1311\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0606 - val_loss: 0.1260\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0498 - val_loss: 0.1215\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0430 - val_loss: 0.1162\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0475 - val_loss: 0.1116\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0415 - val_loss: 0.1066\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0568 - val_loss: 0.1019\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0457 - val_loss: 0.0978\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0351 - val_loss: 0.0927\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0372 - val_loss: 0.0896\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0262 - val_loss: 0.0872\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0344 - val_loss: 0.0867\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0333 - val_loss: 0.0864\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0429 - val_loss: 0.0864\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0419 - val_loss: 0.0893\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0332 - val_loss: 0.0941\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb451f38040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2410 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.3035 - val_loss: 0.0222\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.2184 - val_loss: 0.0202\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1537 - val_loss: 0.0200\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1437 - val_loss: 0.0199\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1229 - val_loss: 0.0198\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0967 - val_loss: 0.0197\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0822 - val_loss: 0.0198\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0714 - val_loss: 0.0201\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4298ada60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2411 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 2.6271 - val_loss: 0.0992\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.9170 - val_loss: 0.0944\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.4548 - val_loss: 0.0988\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 1.1508 - val_loss: 0.1034\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb441172dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2412 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.3174 - val_loss: 0.0200\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2064 - val_loss: 0.0208\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1628 - val_loss: 0.0217\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4a623c940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2413 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.3618 - val_loss: 0.0329\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2592 - val_loss: 0.0368\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1800 - val_loss: 0.0408\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4861df700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2414 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.9224 - val_loss: 0.0469\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.4132 - val_loss: 0.0465\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3008 - val_loss: 0.0435\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1981 - val_loss: 0.0431\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1818 - val_loss: 0.0428\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1910 - val_loss: 0.0445\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1724 - val_loss: 0.0453\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb47c0f2820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2415 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.3048 - val_loss: 0.0383\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1586 - val_loss: 0.0394\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1271 - val_loss: 0.0393\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb46c676a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2416 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.3442 - val_loss: 0.0785\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2233 - val_loss: 0.0769\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1561 - val_loss: 0.0741\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1277 - val_loss: 0.0717\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1086 - val_loss: 0.0703\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0950 - val_loss: 0.0697\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0719 - val_loss: 0.0696\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0650 - val_loss: 0.0682\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0476 - val_loss: 0.0682\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0482 - val_loss: 0.0675\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0446 - val_loss: 0.0671\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0389 - val_loss: 0.0664\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0335 - val_loss: 0.0667\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0324 - val_loss: 0.0665\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb440ba0310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2417 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.3816 - val_loss: 0.1130\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2656 - val_loss: 0.1095\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1422 - val_loss: 0.1077\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0918 - val_loss: 0.1069\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0818 - val_loss: 0.1086\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0797 - val_loss: 0.1081\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4463f2040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2418 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.8518 - val_loss: 0.3404\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6757 - val_loss: 0.3204\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5050 - val_loss: 0.3026\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.4387 - val_loss: 0.2889\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.3810 - val_loss: 0.2810\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3514 - val_loss: 0.2756\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.3310 - val_loss: 0.2680\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2886 - val_loss: 0.2601\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.2436 - val_loss: 0.2524\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2517 - val_loss: 0.2482\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2239 - val_loss: 0.2397\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2089 - val_loss: 0.2346\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2209 - val_loss: 0.2284\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2056 - val_loss: 0.2220\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1766 - val_loss: 0.2183\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1674 - val_loss: 0.2111\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1528 - val_loss: 0.2045\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1558 - val_loss: 0.1992\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1649 - val_loss: 0.1917\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1450 - val_loss: 0.1864\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1335 - val_loss: 0.1816\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1391 - val_loss: 0.1763\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1244 - val_loss: 0.1706\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1122 - val_loss: 0.1644\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1143 - val_loss: 0.1605\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1142 - val_loss: 0.1531\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1135 - val_loss: 0.1462\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1032 - val_loss: 0.1407\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1353 - val_loss: 0.1350\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1129 - val_loss: 0.1323\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0914 - val_loss: 0.1284\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0876 - val_loss: 0.1258\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0755 - val_loss: 0.1219\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0790 - val_loss: 0.1210\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0780 - val_loss: 0.1173\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0788 - val_loss: 0.1184\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0662 - val_loss: 0.1132\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0598 - val_loss: 0.1118\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0675 - val_loss: 0.1088\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0789 - val_loss: 0.1081\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0591 - val_loss: 0.1073\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0843 - val_loss: 0.1052\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0580 - val_loss: 0.1039\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0659 - val_loss: 0.1008\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0645 - val_loss: 0.1013\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0471 - val_loss: 0.1014\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb431281940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2419 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 1.0439 - val_loss: 0.0606\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.7135 - val_loss: 0.0595\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.5648 - val_loss: 0.0522\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3839 - val_loss: 0.0482\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3838 - val_loss: 0.0446\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.3057 - val_loss: 0.0396\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2848 - val_loss: 0.0360\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.2735 - val_loss: 0.0331\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2358 - val_loss: 0.0322\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2191 - val_loss: 0.0275\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1639 - val_loss: 0.0252\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1810 - val_loss: 0.0232\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2335 - val_loss: 0.0221\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1410 - val_loss: 0.0221\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1225 - val_loss: 0.0193\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1176 - val_loss: 0.0193\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1042 - val_loss: 0.0192\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0955 - val_loss: 0.0193\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1075 - val_loss: 0.0193\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb45e38fe50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2420 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.2589 - val_loss: 0.0286\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2129 - val_loss: 0.0270\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1886 - val_loss: 0.0257\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1530 - val_loss: 0.0247\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1436 - val_loss: 0.0239\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1222 - val_loss: 0.0231\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1211 - val_loss: 0.0224\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1153 - val_loss: 0.0217\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1003 - val_loss: 0.0212\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0942 - val_loss: 0.0207\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0912 - val_loss: 0.0202\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0838 - val_loss: 0.0198\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0704 - val_loss: 0.0192\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0647 - val_loss: 0.0187\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0708 - val_loss: 0.0182\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0663 - val_loss: 0.0178\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0520 - val_loss: 0.0175\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0477 - val_loss: 0.0174\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0463 - val_loss: 0.0171\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0431 - val_loss: 0.0169\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0465 - val_loss: 0.0167\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0396 - val_loss: 0.0166\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0432 - val_loss: 0.0164\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0375 - val_loss: 0.0163\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0316 - val_loss: 0.0162\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0380 - val_loss: 0.0160\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0437 - val_loss: 0.0159\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0336 - val_loss: 0.0158\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0349 - val_loss: 0.0158\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0315 - val_loss: 0.0157\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0313 - val_loss: 0.0156\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0280 - val_loss: 0.0156\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0248 - val_loss: 0.0156\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0270 - val_loss: 0.0156\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0299 - val_loss: 0.0156\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb41aa1b160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2421 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.8693 - val_loss: 0.0265\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2291 - val_loss: 0.0263\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1459 - val_loss: 0.0256\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1100 - val_loss: 0.0252\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1291 - val_loss: 0.0260\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0775 - val_loss: 0.0268\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb429912310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2422 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.1018 - val_loss: 0.0157\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0799 - val_loss: 0.0155\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0684 - val_loss: 0.0153\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0569 - val_loss: 0.0152\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0559 - val_loss: 0.0152\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0484 - val_loss: 0.0153\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0471 - val_loss: 0.0154\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb40b6ab9d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2423 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 2.1780 - val_loss: 0.5319\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.4292 - val_loss: 0.5047\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.0942 - val_loss: 0.4900\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.8949 - val_loss: 0.4791\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.8074 - val_loss: 0.4717\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.7407 - val_loss: 0.4665\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.6617 - val_loss: 0.4637\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.5872 - val_loss: 0.4632\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5484 - val_loss: 0.4636\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4832 - val_loss: 0.4636\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb44dc05ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2424 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 0.0841 - val_loss: 0.0308\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0726 - val_loss: 0.0321\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0714 - val_loss: 0.0333\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb40c2bdee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2425 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.3577 - val_loss: 0.0940\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1871 - val_loss: 0.0949\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1578 - val_loss: 0.0937\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1188 - val_loss: 0.0984\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1187 - val_loss: 0.1031\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb417618b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2426 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.1856 - val_loss: 0.0510\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0924 - val_loss: 0.0524\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0711 - val_loss: 0.0551\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3fffd9ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2427 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 1.8577 - val_loss: 0.5524\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.9415 - val_loss: 0.5312\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6676 - val_loss: 0.4908\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.5023 - val_loss: 0.4607\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.4578 - val_loss: 0.4179\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.3321 - val_loss: 0.3905\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2727 - val_loss: 0.3577\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2437 - val_loss: 0.3246\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2476 - val_loss: 0.2972\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2256 - val_loss: 0.2867\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2111 - val_loss: 0.2654\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1965 - val_loss: 0.2496\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1690 - val_loss: 0.2250\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1571 - val_loss: 0.2095\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1398 - val_loss: 0.1961\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1266 - val_loss: 0.1838\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1268 - val_loss: 0.1768\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1240 - val_loss: 0.1634\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0950 - val_loss: 0.1518\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0954 - val_loss: 0.1435\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0950 - val_loss: 0.1360\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0799 - val_loss: 0.1299\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0738 - val_loss: 0.1205\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0920 - val_loss: 0.1206\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0717 - val_loss: 0.1100\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0619 - val_loss: 0.1061\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0686 - val_loss: 0.1018\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0610 - val_loss: 0.1007\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0562 - val_loss: 0.1007\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0550 - val_loss: 0.1015\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb40052ed30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2428 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.5023 - val_loss: 0.0483\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2569 - val_loss: 0.0490\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1470 - val_loss: 0.0490\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb401a92c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2429 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.2232 - val_loss: 0.0150\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1638 - val_loss: 0.0151\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1128 - val_loss: 0.0154\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e88d6550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2430 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.6098 - val_loss: 0.0135\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2971 - val_loss: 0.0137\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2411 - val_loss: 0.0142\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ea6719d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2431 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.5342WARNING:tensorflow:5 out of the last 40 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb48b4deb80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 2.4825 - val_loss: 0.1047\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.7269 - val_loss: 0.0779\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.4968 - val_loss: 0.0710\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.4043 - val_loss: 0.0586\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3034 - val_loss: 0.0503\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2697 - val_loss: 0.0437\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.2346 - val_loss: 0.0383\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1900 - val_loss: 0.0349\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1653 - val_loss: 0.0321\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1509 - val_loss: 0.0289\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1253 - val_loss: 0.0272\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1537 - val_loss: 0.0253\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1056 - val_loss: 0.0228\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1008 - val_loss: 0.0204\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0968 - val_loss: 0.0190\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1142 - val_loss: 0.0176\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1633 - val_loss: 0.0168\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0981 - val_loss: 0.0166\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0977 - val_loss: 0.0168\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0807 - val_loss: 0.0175\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb47efa0280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2432 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.2519 - val_loss: 0.0143\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1286 - val_loss: 0.0143\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0892 - val_loss: 0.0141\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0839 - val_loss: 0.0139\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0629 - val_loss: 0.0140\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0766 - val_loss: 0.0140\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ef5e6c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2433 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.4931 - val_loss: 0.0574\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3512 - val_loss: 0.0517\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2923 - val_loss: 0.0472\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2539 - val_loss: 0.0429\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.2320 - val_loss: 0.0401\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2080 - val_loss: 0.0380\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2080 - val_loss: 0.0372\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1782 - val_loss: 0.0358\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1571 - val_loss: 0.0348\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1474 - val_loss: 0.0338\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1493 - val_loss: 0.0327\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1329 - val_loss: 0.0319\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1198 - val_loss: 0.0306\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1017 - val_loss: 0.0296\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1082 - val_loss: 0.0279\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0967 - val_loss: 0.0264\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0861 - val_loss: 0.0250\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0814 - val_loss: 0.0240\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0847 - val_loss: 0.0229\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0787 - val_loss: 0.0218\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0716 - val_loss: 0.0208\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0840 - val_loss: 0.0204\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0612 - val_loss: 0.0212\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0766 - val_loss: 0.0215\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3eab29ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2434 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.2721 - val_loss: 0.0427\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1730 - val_loss: 0.0435\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1796 - val_loss: 0.0445\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3f40064c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2435 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.4887 - val_loss: 0.0788\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.2362 - val_loss: 0.0775\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1772 - val_loss: 0.0788\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1480 - val_loss: 0.0761\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1255 - val_loss: 0.0732\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1067 - val_loss: 0.0720\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1201 - val_loss: 0.0709\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1108 - val_loss: 0.0724\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1209 - val_loss: 0.0742\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d7ed08b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2436 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 1.6455 - val_loss: 0.2792\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.3381 - val_loss: 0.2544\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1895 - val_loss: 0.2291\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1542 - val_loss: 0.2096\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1127 - val_loss: 0.1804\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1010 - val_loss: 0.1664\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0964 - val_loss: 0.1575\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0834 - val_loss: 0.1405\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1067 - val_loss: 0.1242\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0741 - val_loss: 0.1158\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0747 - val_loss: 0.1038\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0608 - val_loss: 0.0951\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0547 - val_loss: 0.0865\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0746 - val_loss: 0.0781\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0542 - val_loss: 0.0741\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0488 - val_loss: 0.0683\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0431 - val_loss: 0.0638\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0432 - val_loss: 0.0605\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0490 - val_loss: 0.0577\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0435 - val_loss: 0.0542\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0477 - val_loss: 0.0508\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0425 - val_loss: 0.0489\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0363 - val_loss: 0.0458\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0417 - val_loss: 0.0458\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0268 - val_loss: 0.0430\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0389 - val_loss: 0.0435\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0399 - val_loss: 0.0407\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0306 - val_loss: 0.0389\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0411 - val_loss: 0.0385\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0323 - val_loss: 0.0373\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0242 - val_loss: 0.0364\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0444 - val_loss: 0.0346\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0279 - val_loss: 0.0325\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0427 - val_loss: 0.0317\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0197 - val_loss: 0.0310\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0263 - val_loss: 0.0314\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0377 - val_loss: 0.0307\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0289 - val_loss: 0.0300\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0259 - val_loss: 0.0298\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0255 - val_loss: 0.0300\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0240 - val_loss: 0.0306\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d9e6df70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2437 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 1.8761 - val_loss: 0.3639\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 1.1938 - val_loss: 0.3091\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.9521 - val_loss: 0.2778\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.6980 - val_loss: 0.2545\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5816 - val_loss: 0.2224\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5625 - val_loss: 0.2082\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5395 - val_loss: 0.1856\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4824 - val_loss: 0.1662\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4498 - val_loss: 0.1511\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3952 - val_loss: 0.1365\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3785 - val_loss: 0.1235\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3437 - val_loss: 0.1119\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3394 - val_loss: 0.1032\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3149 - val_loss: 0.0920\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.2738 - val_loss: 0.0851\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2804 - val_loss: 0.0790\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2467 - val_loss: 0.0718\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2495 - val_loss: 0.0653\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2320 - val_loss: 0.0580\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2296 - val_loss: 0.0531\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2002 - val_loss: 0.0485\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2303 - val_loss: 0.0436\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1822 - val_loss: 0.0402\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1675 - val_loss: 0.0363\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1528 - val_loss: 0.0331\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1813 - val_loss: 0.0308\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1437 - val_loss: 0.0280\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1429 - val_loss: 0.0264\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1396 - val_loss: 0.0243\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1145 - val_loss: 0.0220\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1256 - val_loss: 0.0211\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1228 - val_loss: 0.0207\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1078 - val_loss: 0.0195\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0980 - val_loss: 0.0190\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1168 - val_loss: 0.0190\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0932 - val_loss: 0.0191\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3dccc5ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2438 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.3171 - val_loss: 0.0166\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3239 - val_loss: 0.0162\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2114 - val_loss: 0.0161\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1506 - val_loss: 0.0152\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1228 - val_loss: 0.0151\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0755 - val_loss: 0.0142\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0948 - val_loss: 0.0135\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0768 - val_loss: 0.0132\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0978 - val_loss: 0.0117\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0636 - val_loss: 0.0116\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0750 - val_loss: 0.0107\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0522 - val_loss: 0.0103\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1354 - val_loss: 0.0098\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0484 - val_loss: 0.0095\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0542 - val_loss: 0.0092\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0572 - val_loss: 0.0091\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0486 - val_loss: 0.0090\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0514 - val_loss: 0.0088\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0389 - val_loss: 0.0088\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0480 - val_loss: 0.0088\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0512 - val_loss: 0.0084\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0519 - val_loss: 0.0083\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0603 - val_loss: 0.0083\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0441 - val_loss: 0.0085\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e46489d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2439 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.3112 - val_loss: 0.0832\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2452 - val_loss: 0.0902\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1944 - val_loss: 0.0945\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb458fb2820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2440 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.3005 - val_loss: 0.0206\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1932 - val_loss: 0.0216\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1357 - val_loss: 0.0235\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb473752940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2441 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 178ms/step - loss: 0.2417 - val_loss: 0.0182\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1407 - val_loss: 0.0190\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1077 - val_loss: 0.0198\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb471592dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2442 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6780WARNING:tensorflow:5 out of the last 34 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb458e80040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.4675 - val_loss: 0.0109\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2725 - val_loss: 0.0109\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1839 - val_loss: 0.0106\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1442 - val_loss: 0.0099\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1201 - val_loss: 0.0093\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1265 - val_loss: 0.0093\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1244 - val_loss: 0.0089\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1042 - val_loss: 0.0085\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0859 - val_loss: 0.0083\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0767 - val_loss: 0.0081\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0915 - val_loss: 0.0078\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0735 - val_loss: 0.0078\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0640 - val_loss: 0.0078\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0654 - val_loss: 0.0077\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0722 - val_loss: 0.0077\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0572 - val_loss: 0.0077\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0489 - val_loss: 0.0077\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0517 - val_loss: 0.0077\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb430e86e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2443 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.1105 - val_loss: 0.5927\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 1.8158 - val_loss: 0.5779\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.2803 - val_loss: 0.5454\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.0277 - val_loss: 0.5289\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.9418 - val_loss: 0.5099\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.8209 - val_loss: 0.4913\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.7898 - val_loss: 0.4309\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6598 - val_loss: 0.4143\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5816 - val_loss: 0.3945\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.5829 - val_loss: 0.3836\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4873 - val_loss: 0.3664\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4913 - val_loss: 0.3560\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4095 - val_loss: 0.3422\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3732 - val_loss: 0.3349\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3820 - val_loss: 0.3225\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3810 - val_loss: 0.3086\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.3363 - val_loss: 0.2960\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3304 - val_loss: 0.2865\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3301 - val_loss: 0.2795\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3000 - val_loss: 0.2696\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2309 - val_loss: 0.2632\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3774 - val_loss: 0.2580\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2305 - val_loss: 0.2498\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2099 - val_loss: 0.2466\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1865 - val_loss: 0.2408\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1696 - val_loss: 0.2395\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1860 - val_loss: 0.2474\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1914 - val_loss: 0.2371\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1858 - val_loss: 0.2430\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2230 - val_loss: 0.2343\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1506 - val_loss: 0.2427\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1464 - val_loss: 0.2476\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d7cd8670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2444 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 0.3164 - val_loss: 0.1325\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1394 - val_loss: 0.1277\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1136 - val_loss: 0.1235\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0935 - val_loss: 0.1193\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0805 - val_loss: 0.1127\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0850 - val_loss: 0.1060\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0755 - val_loss: 0.0977\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0685 - val_loss: 0.0921\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0548 - val_loss: 0.0863\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0482 - val_loss: 0.0826\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0499 - val_loss: 0.0802\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0384 - val_loss: 0.0766\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0448 - val_loss: 0.0726\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0414 - val_loss: 0.0696\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0453 - val_loss: 0.0663\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0335 - val_loss: 0.0651\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0487 - val_loss: 0.0628\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0350 - val_loss: 0.0642\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0316 - val_loss: 0.0649\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d516c940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2445 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.3481 - val_loss: 0.0199\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2082 - val_loss: 0.0144\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1633 - val_loss: 0.0139\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1178 - val_loss: 0.0136\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0904 - val_loss: 0.0133\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1002 - val_loss: 0.0127\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0784 - val_loss: 0.0120\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0504 - val_loss: 0.0102\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0618 - val_loss: 0.0092\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0505 - val_loss: 0.0082\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0633 - val_loss: 0.0081\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0786 - val_loss: 0.0080\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0512 - val_loss: 0.0084\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0615 - val_loss: 0.0092\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cc7e61f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2446 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 0.3554 - val_loss: 0.0087\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2713 - val_loss: 0.0094\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2186 - val_loss: 0.0098\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e34539d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2447 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.4800 - val_loss: 0.0570\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3167 - val_loss: 0.0595\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2531 - val_loss: 0.0611\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cb01d3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2448 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.1575 - val_loss: 0.0079\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1049 - val_loss: 0.0080\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0536 - val_loss: 0.0081\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e34dc670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2449 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.8590WARNING:tensorflow:5 out of the last 24 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb3de036310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.7871 - val_loss: 0.1391\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.6074 - val_loss: 0.1383\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4795 - val_loss: 0.1336\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3368 - val_loss: 0.1379\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.2457 - val_loss: 0.1372\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ccc77160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2450 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.5524 - val_loss: 0.0379\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.3852 - val_loss: 0.0405\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2885 - val_loss: 0.0403\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cbc8d160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2451 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.1879 - val_loss: 0.0107\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1604 - val_loss: 0.0108\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1206 - val_loss: 0.0112\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cd046e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2452 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.4533 - val_loss: 0.0670\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3632 - val_loss: 0.0626\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2605 - val_loss: 0.0580\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1926 - val_loss: 0.0526\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1745 - val_loss: 0.0474\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1374 - val_loss: 0.0418\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1345 - val_loss: 0.0358\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1215 - val_loss: 0.0298\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1139 - val_loss: 0.0241\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1102 - val_loss: 0.0206\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0956 - val_loss: 0.0193\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0970 - val_loss: 0.0188\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0993 - val_loss: 0.0185\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0843 - val_loss: 0.0184\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0959 - val_loss: 0.0182\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0784 - val_loss: 0.0178\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0686 - val_loss: 0.0176\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0614 - val_loss: 0.0173\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0741 - val_loss: 0.0169\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0652 - val_loss: 0.0163\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0547 - val_loss: 0.0157\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0700 - val_loss: 0.0148\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0663 - val_loss: 0.0141\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0618 - val_loss: 0.0134\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0461 - val_loss: 0.0127\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0587 - val_loss: 0.0118\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0497 - val_loss: 0.0111\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0447 - val_loss: 0.0103\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0540 - val_loss: 0.0097\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0491 - val_loss: 0.0091\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0514 - val_loss: 0.0083\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0556 - val_loss: 0.0077\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0453 - val_loss: 0.0072\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0391 - val_loss: 0.0067\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0351 - val_loss: 0.0063\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0369 - val_loss: 0.0060\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0468 - val_loss: 0.0058\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0450 - val_loss: 0.0057\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0472 - val_loss: 0.0057\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0404 - val_loss: 0.0058\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0376 - val_loss: 0.0059\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ce951ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2453 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.2941 - val_loss: 0.1478\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1720 - val_loss: 0.1352\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1253 - val_loss: 0.1249\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1082 - val_loss: 0.1150\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1032 - val_loss: 0.1051\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0935 - val_loss: 0.0971\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0893 - val_loss: 0.0888\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0767 - val_loss: 0.0806\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0859 - val_loss: 0.0721\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0677 - val_loss: 0.0653\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0748 - val_loss: 0.0580\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0703 - val_loss: 0.0510\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0606 - val_loss: 0.0455\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0588 - val_loss: 0.0401\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0564 - val_loss: 0.0354\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0527 - val_loss: 0.0310\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0581 - val_loss: 0.0274\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0492 - val_loss: 0.0241\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0443 - val_loss: 0.0211\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0461 - val_loss: 0.0184\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0471 - val_loss: 0.0160\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0466 - val_loss: 0.0141\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0549 - val_loss: 0.0126\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0430 - val_loss: 0.0113\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0438 - val_loss: 0.0104\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0358 - val_loss: 0.0097\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0461 - val_loss: 0.0091\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0417 - val_loss: 0.0085\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0374 - val_loss: 0.0081\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0508 - val_loss: 0.0078\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0321 - val_loss: 0.0076\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0393 - val_loss: 0.0073\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0349 - val_loss: 0.0072\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0317 - val_loss: 0.0070\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0377 - val_loss: 0.0069\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0381 - val_loss: 0.0069\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0321 - val_loss: 0.0068\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0317 - val_loss: 0.0068\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0331 - val_loss: 0.0068\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0278 - val_loss: 0.0067\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0346 - val_loss: 0.0067\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0301 - val_loss: 0.0067\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0268 - val_loss: 0.0067\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0302 - val_loss: 0.0066\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0352 - val_loss: 0.0066\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0350 - val_loss: 0.0066\n",
      "Epoch 47/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0321 - val_loss: 0.0066\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d29d9310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2454 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.3143 - val_loss: 0.0296\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1995 - val_loss: 0.0281\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1618 - val_loss: 0.0267\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1065 - val_loss: 0.0257\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1223 - val_loss: 0.0244\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1247 - val_loss: 0.0232\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1041 - val_loss: 0.0222\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0913 - val_loss: 0.0213\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0842 - val_loss: 0.0208\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0770 - val_loss: 0.0200\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0740 - val_loss: 0.0197\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0562 - val_loss: 0.0187\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.040 - 0s 10ms/step - loss: 0.0687 - val_loss: 0.0174\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0700 - val_loss: 0.0161\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0602 - val_loss: 0.0151\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0667 - val_loss: 0.0137\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0431 - val_loss: 0.0126\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0401 - val_loss: 0.0112\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0431 - val_loss: 0.0095\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0495 - val_loss: 0.0085\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0375 - val_loss: 0.0073\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0295 - val_loss: 0.0065\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0378 - val_loss: 0.0060\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0346 - val_loss: 0.0057\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0307 - val_loss: 0.0054\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.023 - 0s 17ms/step - loss: 0.0269 - val_loss: 0.0051\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0287 - val_loss: 0.0049\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0352 - val_loss: 0.0048\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0346 - val_loss: 0.0048\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0265 - val_loss: 0.0048\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0410 - val_loss: 0.0049\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0291 - val_loss: 0.0049\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d51df3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2455 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.4251 - val_loss: 0.1254\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3326 - val_loss: 0.1192\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.2566 - val_loss: 0.1159\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1482 - val_loss: 0.1113\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1405 - val_loss: 0.1110\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1006 - val_loss: 0.1058\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1040 - val_loss: 0.1021\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0618 - val_loss: 0.0983\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0861 - val_loss: 0.0978\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0703 - val_loss: 0.0968\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0453 - val_loss: 0.0944\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0561 - val_loss: 0.0933\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0446 - val_loss: 0.0918\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0415 - val_loss: 0.0892\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0493 - val_loss: 0.0880\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0337 - val_loss: 0.0860\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0359 - val_loss: 0.0830\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0370 - val_loss: 0.0813\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0427 - val_loss: 0.0802\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0336 - val_loss: 0.0743\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0282 - val_loss: 0.0735\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0237 - val_loss: 0.0729\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0283 - val_loss: 0.0693\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0425 - val_loss: 0.0662\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0342 - val_loss: 0.0677\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0266 - val_loss: 0.0643\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0269 - val_loss: 0.0627\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0222 - val_loss: 0.0617\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0251 - val_loss: 0.0595\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0268 - val_loss: 0.0577\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0257 - val_loss: 0.0545\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0372 - val_loss: 0.0532\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0201 - val_loss: 0.0499\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0186 - val_loss: 0.0503\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0293 - val_loss: 0.0447\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0242 - val_loss: 0.0463\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0255 - val_loss: 0.0455\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3dc9db280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2456 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.2518 - val_loss: 0.0125\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.1885 - val_loss: 0.0121\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0917 - val_loss: 0.0119\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0665 - val_loss: 0.0121\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0714 - val_loss: 0.0119\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e072e160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2457 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.9150 - val_loss: 0.1146\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6909 - val_loss: 0.1110\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6329 - val_loss: 0.1093\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6275 - val_loss: 0.0993\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.5336 - val_loss: 0.0934\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.4541 - val_loss: 0.0859\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3985 - val_loss: 0.0808\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3921 - val_loss: 0.0789\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3358 - val_loss: 0.0759\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3151 - val_loss: 0.0740\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2934 - val_loss: 0.0740\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.3086 - val_loss: 0.0728\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2701 - val_loss: 0.0712\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2548 - val_loss: 0.0712\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2486 - val_loss: 0.0725\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.2169 - val_loss: 0.0727\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e23e4160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2458 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.2765 - val_loss: 0.0116\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0997 - val_loss: 0.0106\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0775 - val_loss: 0.0098\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0824 - val_loss: 0.0093\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1340 - val_loss: 0.0087\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1068 - val_loss: 0.0083\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0816 - val_loss: 0.0079\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0698 - val_loss: 0.0077\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0499 - val_loss: 0.0075\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0673 - val_loss: 0.0075\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0472 - val_loss: 0.0073\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0512 - val_loss: 0.0074\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0481 - val_loss: 0.0073\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0498 - val_loss: 0.0074\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0459 - val_loss: 0.0076\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3b7c70c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2459 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.5792 - val_loss: 0.1217\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4127 - val_loss: 0.1188\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2643 - val_loss: 0.1160\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2026 - val_loss: 0.1135\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1750 - val_loss: 0.1112\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1702 - val_loss: 0.1111\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1482 - val_loss: 0.1073\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1185 - val_loss: 0.1041\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1471 - val_loss: 0.1013\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1088 - val_loss: 0.0979\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0941 - val_loss: 0.0964\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0836 - val_loss: 0.0944\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0995 - val_loss: 0.0934\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0828 - val_loss: 0.0925\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0757 - val_loss: 0.0926\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0715 - val_loss: 0.0923\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0657 - val_loss: 0.0922\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0762 - val_loss: 0.0915\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0704 - val_loss: 0.0931\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0685 - val_loss: 0.0938\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3b98a89d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2460 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 1.7997 - val_loss: 0.3545\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.1614 - val_loss: 0.3324\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7705 - val_loss: 0.3194\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.7191 - val_loss: 0.2940\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.4976 - val_loss: 0.2826\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4559 - val_loss: 0.2618\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.4587 - val_loss: 0.2458\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3712 - val_loss: 0.2334\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3701 - val_loss: 0.2205\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.2950 - val_loss: 0.2095\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2732 - val_loss: 0.1986\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3023 - val_loss: 0.1818\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.2646 - val_loss: 0.1799\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2538 - val_loss: 0.1705\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2332 - val_loss: 0.1586\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2057 - val_loss: 0.1541\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1761 - val_loss: 0.1488\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1665 - val_loss: 0.1426\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1596 - val_loss: 0.1408\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1604 - val_loss: 0.1370\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1855 - val_loss: 0.1290\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1898 - val_loss: 0.1251\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1561 - val_loss: 0.1211\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1444 - val_loss: 0.1180\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1279 - val_loss: 0.1154\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1295 - val_loss: 0.1138\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0976 - val_loss: 0.1114\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1064 - val_loss: 0.1086\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1032 - val_loss: 0.1077\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1005 - val_loss: 0.1070\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0951 - val_loss: 0.1033\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0939 - val_loss: 0.1021\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0878 - val_loss: 0.1015\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0829 - val_loss: 0.0998\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0668 - val_loss: 0.0986\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0813 - val_loss: 0.0962\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0719 - val_loss: 0.0936\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0937 - val_loss: 0.0876\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0720 - val_loss: 0.0851\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0662 - val_loss: 0.0828\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0630 - val_loss: 0.0804\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0566 - val_loss: 0.0766\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0632 - val_loss: 0.0709\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0837 - val_loss: 0.0684\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0774 - val_loss: 0.0638\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0552 - val_loss: 0.0610\n",
      "Epoch 47/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0491 - val_loss: 0.0606\n",
      "Epoch 48/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0499 - val_loss: 0.0625\n",
      "Epoch 49/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0450 - val_loss: 0.0565\n",
      "Epoch 50/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0691 - val_loss: 0.0512\n",
      "Epoch 51/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0406 - val_loss: 0.0503\n",
      "Epoch 52/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0522 - val_loss: 0.0484\n",
      "Epoch 53/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0354 - val_loss: 0.0468\n",
      "Epoch 54/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0602 - val_loss: 0.0443\n",
      "Epoch 55/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0398 - val_loss: 0.0437\n",
      "Epoch 56/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0724 - val_loss: 0.0427\n",
      "Epoch 57/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0492 - val_loss: 0.0422\n",
      "Epoch 58/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0540 - val_loss: 0.0427\n",
      "Epoch 59/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0342 - val_loss: 0.0421\n",
      "Epoch 60/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0358 - val_loss: 0.0419\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3b9cac1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2461 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.1255 - val_loss: 0.0079\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0916 - val_loss: 0.0077\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0650 - val_loss: 0.0076\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0480 - val_loss: 0.0076\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0478 - val_loss: 0.0077\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0399 - val_loss: 0.0081\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3babcb3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2462 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.7021 - val_loss: 0.0093\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.4890 - val_loss: 0.0088\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2664 - val_loss: 0.0084\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2215 - val_loss: 0.0083\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1767 - val_loss: 0.0084\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1480 - val_loss: 0.0084\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3bbdc8310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2463 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.7940 - val_loss: 0.0309\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.4096 - val_loss: 0.0294\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.2888 - val_loss: 0.0280\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.2977 - val_loss: 0.0282\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.3009 - val_loss: 0.0271\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.2350 - val_loss: 0.0258\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2242 - val_loss: 0.0237\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.1887 - val_loss: 0.0225\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1656 - val_loss: 0.0210\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2126 - val_loss: 0.0202\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1757 - val_loss: 0.0177\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1536 - val_loss: 0.0162\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1543 - val_loss: 0.0155\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1464 - val_loss: 0.0135\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1249 - val_loss: 0.0126\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1192 - val_loss: 0.0111\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1160 - val_loss: 0.0105\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1260 - val_loss: 0.0092\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0989 - val_loss: 0.0088\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0992 - val_loss: 0.0086\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.1003 - val_loss: 0.0083\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0953 - val_loss: 0.0080\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0877 - val_loss: 0.0077\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0940 - val_loss: 0.0074\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0834 - val_loss: 0.0074\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0907 - val_loss: 0.0073\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0843 - val_loss: 0.0072\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0729 - val_loss: 0.0072\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0803 - val_loss: 0.0070\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0910 - val_loss: 0.0070\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0748 - val_loss: 0.0070\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0823 - val_loss: 0.0070\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0610 - val_loss: 0.0071\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3bcfd2940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2464 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.9730 - val_loss: 0.1692\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4426 - val_loss: 0.1748\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3221 - val_loss: 0.1753\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c042d310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2465 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 1.1262 - val_loss: 0.2147\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.7248 - val_loss: 0.1801\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.6068 - val_loss: 0.1614\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4388 - val_loss: 0.1398\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3051 - val_loss: 0.1240\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.2666 - val_loss: 0.1155\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1912 - val_loss: 0.1102\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1637 - val_loss: 0.1095\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1428 - val_loss: 0.1054\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1753 - val_loss: 0.0975\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1162 - val_loss: 0.0954\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1033 - val_loss: 0.0931\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1295 - val_loss: 0.0912\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1015 - val_loss: 0.0842\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1002 - val_loss: 0.0881\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0944 - val_loss: 0.0835\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0720 - val_loss: 0.0806\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0866 - val_loss: 0.0819\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0970 - val_loss: 0.0766\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0919 - val_loss: 0.0787\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0586 - val_loss: 0.0745\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0637 - val_loss: 0.0735\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0632 - val_loss: 0.0744\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0590 - val_loss: 0.0793\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c0c8a790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2466 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.3937 - val_loss: 0.0413\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2871 - val_loss: 0.0376\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2378 - val_loss: 0.0349\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1806 - val_loss: 0.0328\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1779 - val_loss: 0.0316\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1638 - val_loss: 0.0301\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1554 - val_loss: 0.0286\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1769 - val_loss: 0.0272\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1367 - val_loss: 0.0260\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1333 - val_loss: 0.0251\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1081 - val_loss: 0.0245\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1136 - val_loss: 0.0240\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1076 - val_loss: 0.0238\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1104 - val_loss: 0.0241\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1014 - val_loss: 0.0241\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c1f0fe50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2467 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 0.2894 - val_loss: 0.0966\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.2312 - val_loss: 0.1007\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1345 - val_loss: 0.1010\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb471592280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2468 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 0.5781 - val_loss: 0.0341\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3217 - val_loss: 0.0343\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2246 - val_loss: 0.0334\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1757 - val_loss: 0.0332\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1273 - val_loss: 0.0334\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1077 - val_loss: 0.0340\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb401a92dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2469 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.2259 - val_loss: 0.0119\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1380 - val_loss: 0.0124\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0826 - val_loss: 0.0125\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb478ecda60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2470 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 0.3962 - val_loss: 0.0112\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2218 - val_loss: 0.0112\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1603 - val_loss: 0.0108\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1416 - val_loss: 0.0107\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1052 - val_loss: 0.0112\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0970 - val_loss: 0.0116\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb44dd70820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2471 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.4223 - val_loss: 0.0329\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2706 - val_loss: 0.0301\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.1733 - val_loss: 0.0281\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1444 - val_loss: 0.0260\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0915 - val_loss: 0.0246\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0784 - val_loss: 0.0231\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0715 - val_loss: 0.0219\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0626 - val_loss: 0.0210\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0467 - val_loss: 0.0199\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0458 - val_loss: 0.0190\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0535 - val_loss: 0.0180\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0458 - val_loss: 0.0163\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0410 - val_loss: 0.0143\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0652 - val_loss: 0.0126\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0315 - val_loss: 0.0115\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0408 - val_loss: 0.0105\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0366 - val_loss: 0.0097\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0443 - val_loss: 0.0093\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0363 - val_loss: 0.0092\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0278 - val_loss: 0.0092\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0224 - val_loss: 0.0094\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb401c8c5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2472 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.2934 - val_loss: 0.0389\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2252 - val_loss: 0.0402\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1840 - val_loss: 0.0403\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb435d65280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2473 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 2.2949 - val_loss: 0.1697\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.1228 - val_loss: 0.1443\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.6078 - val_loss: 0.1252\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.5032 - val_loss: 0.1244\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.4466 - val_loss: 0.1151\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3302 - val_loss: 0.1098\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3060 - val_loss: 0.1045\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3413 - val_loss: 0.1014\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2218 - val_loss: 0.0980\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2079 - val_loss: 0.0940\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1994 - val_loss: 0.0892\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1547 - val_loss: 0.0843\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1362 - val_loss: 0.0828\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1374 - val_loss: 0.0801\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1407 - val_loss: 0.0744\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1128 - val_loss: 0.0703\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1141 - val_loss: 0.0676\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1119 - val_loss: 0.0657\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1102 - val_loss: 0.0626\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0988 - val_loss: 0.0599\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1072 - val_loss: 0.0574\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0942 - val_loss: 0.0525\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0757 - val_loss: 0.0490\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0898 - val_loss: 0.0460\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0800 - val_loss: 0.0452\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0792 - val_loss: 0.0407\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.057 - 0s 11ms/step - loss: 0.0717 - val_loss: 0.0388\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0623 - val_loss: 0.0366\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0665 - val_loss: 0.0352\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0722 - val_loss: 0.0313\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0743 - val_loss: 0.0295\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0586 - val_loss: 0.0274\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0723 - val_loss: 0.0257\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0665 - val_loss: 0.0230\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0564 - val_loss: 0.0228\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0812 - val_loss: 0.0199\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0804 - val_loss: 0.0184\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0588 - val_loss: 0.0171\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0662 - val_loss: 0.0165\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0432 - val_loss: 0.0153\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0440 - val_loss: 0.0140\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0466 - val_loss: 0.0132\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0445 - val_loss: 0.0131\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0468 - val_loss: 0.0120\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0411 - val_loss: 0.0119\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0379 - val_loss: 0.0110\n",
      "Epoch 47/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0464 - val_loss: 0.0113\n",
      "Epoch 48/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0489 - val_loss: 0.0106\n",
      "Epoch 49/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0394 - val_loss: 0.0105\n",
      "Epoch 50/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0456 - val_loss: 0.0106\n",
      "Epoch 51/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0410 - val_loss: 0.0106\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c04b2c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2474 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.4753 - val_loss: 0.1661\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3826 - val_loss: 0.1604\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.2901 - val_loss: 0.1547\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2364 - val_loss: 0.1510\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2188 - val_loss: 0.1467\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1818 - val_loss: 0.1425\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1594 - val_loss: 0.1392\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1410 - val_loss: 0.1361\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1387 - val_loss: 0.1325\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1271 - val_loss: 0.1297\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1177 - val_loss: 0.1274\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1194 - val_loss: 0.1245\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1010 - val_loss: 0.1222\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1031 - val_loss: 0.1193\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0975 - val_loss: 0.1174\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0906 - val_loss: 0.1156\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0744 - val_loss: 0.1148\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0876 - val_loss: 0.1137\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0784 - val_loss: 0.1123\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0731 - val_loss: 0.1113\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0711 - val_loss: 0.1109\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0654 - val_loss: 0.1111\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0664 - val_loss: 0.1109\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3be6361f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2475 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 1.1100 - val_loss: 0.0063\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.6528 - val_loss: 0.0058\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.4868 - val_loss: 0.0056\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.3830 - val_loss: 0.0058\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2942 - val_loss: 0.0059\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3bb6f6c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2476 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.3756 - val_loss: 0.0373\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2616 - val_loss: 0.0395\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2216 - val_loss: 0.0418\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3b996d0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2477 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.2566 - val_loss: 0.0565\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1673 - val_loss: 0.0541\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1557 - val_loss: 0.0546\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0957 - val_loss: 0.0518\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0858 - val_loss: 0.0538\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0719 - val_loss: 0.0534\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e35a6160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2478 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.2503 - val_loss: 0.0078\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1690 - val_loss: 0.0080\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1364 - val_loss: 0.0084\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e09d5e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2479 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.4768 - val_loss: 0.0175\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3113 - val_loss: 0.0156\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2092 - val_loss: 0.0146\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1454 - val_loss: 0.0137\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1354 - val_loss: 0.0132\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1174 - val_loss: 0.0126\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0995 - val_loss: 0.0123\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0945 - val_loss: 0.0120\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1035 - val_loss: 0.0117\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0889 - val_loss: 0.0116\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0850 - val_loss: 0.0118\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0760 - val_loss: 0.0119\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d9ed1ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2480 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.4317 - val_loss: 0.0091\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2540 - val_loss: 0.0083\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1509 - val_loss: 0.0081\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1365 - val_loss: 0.0080\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.1172 - val_loss: 0.0076\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1144 - val_loss: 0.0074\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0865 - val_loss: 0.0073\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0853 - val_loss: 0.0073\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0663 - val_loss: 0.0073\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0591 - val_loss: 0.0072\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0684 - val_loss: 0.0072\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0511 - val_loss: 0.0072\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0586 - val_loss: 0.0072\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0570 - val_loss: 0.0072\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0475 - val_loss: 0.0072\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ce9511f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2481 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 0.4605 - val_loss: 0.0878\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2533 - val_loss: 0.0741\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.2061 - val_loss: 0.0642\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1501 - val_loss: 0.0583\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1766 - val_loss: 0.0517\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1539 - val_loss: 0.0456\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1262 - val_loss: 0.0397\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1019 - val_loss: 0.0346\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0770 - val_loss: 0.0304\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0587 - val_loss: 0.0275\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0578 - val_loss: 0.0238\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0612 - val_loss: 0.0211\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0584 - val_loss: 0.0188\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0591 - val_loss: 0.0168\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0534 - val_loss: 0.0145\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0466 - val_loss: 0.0130\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0548 - val_loss: 0.0117\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0480 - val_loss: 0.0104\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0493 - val_loss: 0.0097\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0470 - val_loss: 0.0089\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0433 - val_loss: 0.0084\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0376 - val_loss: 0.0080\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0375 - val_loss: 0.0078\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0562 - val_loss: 0.0079\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0471 - val_loss: 0.0079\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e34dc430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2482 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.5646 - val_loss: 0.0082\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2159 - val_loss: 0.0094\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1832 - val_loss: 0.0099\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e3e27b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2483 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.5408 - val_loss: 0.0713\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3844 - val_loss: 0.0712\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2800 - val_loss: 0.0695\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2645 - val_loss: 0.0675\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2330 - val_loss: 0.0651\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2020 - val_loss: 0.0627\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1650 - val_loss: 0.0606\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1589 - val_loss: 0.0582\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1427 - val_loss: 0.0560\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1404 - val_loss: 0.0535\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1279 - val_loss: 0.0546\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1177 - val_loss: 0.0557\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c7c2cb80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2484 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 2.7047 - val_loss: 1.7053\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 1.5249 - val_loss: 1.5601\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.0844 - val_loss: 1.4160\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.8901 - val_loss: 1.3194\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.7159 - val_loss: 1.2198\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5816 - val_loss: 1.1732\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.5004 - val_loss: 1.0639\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.4188 - val_loss: 1.0074\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3787 - val_loss: 0.9174\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3516 - val_loss: 0.8764\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2718 - val_loss: 0.8334\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2525 - val_loss: 0.7930\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2300 - val_loss: 0.7465\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2017 - val_loss: 0.7218\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1926 - val_loss: 0.6993\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.2259 - val_loss: 0.6590\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1778 - val_loss: 0.6409\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1559 - val_loss: 0.6149\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1258 - val_loss: 0.5945\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1250 - val_loss: 0.5727\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1157 - val_loss: 0.5563\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0972 - val_loss: 0.5389\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1026 - val_loss: 0.5255\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0986 - val_loss: 0.5027\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0801 - val_loss: 0.4911\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0909 - val_loss: 0.4765\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0912 - val_loss: 0.4578\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1145 - val_loss: 0.4423\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0956 - val_loss: 0.4311\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1037 - val_loss: 0.4200\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0752 - val_loss: 0.4117\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0681 - val_loss: 0.4038\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0717 - val_loss: 0.3947\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0709 - val_loss: 0.3869\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0802 - val_loss: 0.3799\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1329 - val_loss: 0.3730\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0641 - val_loss: 0.3659\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0622 - val_loss: 0.3578\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0926 - val_loss: 0.3486\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0561 - val_loss: 0.3408\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0625 - val_loss: 0.3324\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0717 - val_loss: 0.3268\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0867 - val_loss: 0.3205\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0538 - val_loss: 0.3139\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0695 - val_loss: 0.3088\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0433 - val_loss: 0.3053\n",
      "Epoch 47/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0493 - val_loss: 0.3014\n",
      "Epoch 48/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0471 - val_loss: 0.2979\n",
      "Epoch 49/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0412 - val_loss: 0.2934\n",
      "Epoch 50/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0365 - val_loss: 0.2874\n",
      "Epoch 51/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0418 - val_loss: 0.2812\n",
      "Epoch 52/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0413 - val_loss: 0.2749\n",
      "Epoch 53/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0525 - val_loss: 0.2686\n",
      "Epoch 54/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0362 - val_loss: 0.2639\n",
      "Epoch 55/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0425 - val_loss: 0.2566\n",
      "Epoch 56/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0427 - val_loss: 0.2504\n",
      "Epoch 57/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0393 - val_loss: 0.2448\n",
      "Epoch 58/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0329 - val_loss: 0.2381\n",
      "Epoch 59/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0403 - val_loss: 0.2305\n",
      "Epoch 60/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0283 - val_loss: 0.2213\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb45f75c040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2485 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 0.3503 - val_loss: 0.0103\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2176 - val_loss: 0.0102\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1864 - val_loss: 0.0102\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1517 - val_loss: 0.0102\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1384 - val_loss: 0.0102\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1175 - val_loss: 0.0102\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1024 - val_loss: 0.0102\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0922 - val_loss: 0.0101\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0845 - val_loss: 0.0102\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0683 - val_loss: 0.0102\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4fee27ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2486 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.3008 - val_loss: 0.2142\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1824 - val_loss: 0.2075\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1701 - val_loss: 0.2015\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1489 - val_loss: 0.1950\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1346 - val_loss: 0.1870\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1201 - val_loss: 0.1815\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1182 - val_loss: 0.1766\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1158 - val_loss: 0.1715\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1076 - val_loss: 0.1666\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1206 - val_loss: 0.1623\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1019 - val_loss: 0.1581\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0906 - val_loss: 0.1557\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0877 - val_loss: 0.1538\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0827 - val_loss: 0.1540\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1002 - val_loss: 0.1557\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e4648ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2487 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.2317 - val_loss: 0.0080\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1663 - val_loss: 0.0081\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1268 - val_loss: 0.0081\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3dccc5ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2488 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.3783 - val_loss: 0.0119\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.2679 - val_loss: 0.0121\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2140 - val_loss: 0.0122\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d97418b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2489 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.6322 - val_loss: 0.0179\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.4662 - val_loss: 0.0225\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3579 - val_loss: 0.0240\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3fbedea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2490 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.0459WARNING:tensorflow:5 out of the last 25 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb3f4006d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 1.0551 - val_loss: 0.2274\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.8968 - val_loss: 0.2121\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6915 - val_loss: 0.2018\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6119 - val_loss: 0.1916\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.5600 - val_loss: 0.1882\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6015 - val_loss: 0.1768\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4983 - val_loss: 0.1721\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4414 - val_loss: 0.1713\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3665 - val_loss: 0.1735\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4004 - val_loss: 0.1737\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3eb953e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2491 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.5483 - val_loss: 0.0290\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3933 - val_loss: 0.0255\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2332 - val_loss: 0.0231\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2040 - val_loss: 0.0212\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1886 - val_loss: 0.0199\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1534 - val_loss: 0.0187\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1275 - val_loss: 0.0175\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1184 - val_loss: 0.0164\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1194 - val_loss: 0.0153\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1330 - val_loss: 0.0148\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0992 - val_loss: 0.0145\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0943 - val_loss: 0.0141\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1224 - val_loss: 0.0137\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0802 - val_loss: 0.0137\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0778 - val_loss: 0.0135\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0791 - val_loss: 0.0138\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0705 - val_loss: 0.0137\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ffab4280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2492 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.4895 - val_loss: 0.0132\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1825 - val_loss: 0.0127\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1440 - val_loss: 0.0128\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1418 - val_loss: 0.0126\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1173 - val_loss: 0.0126\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1284 - val_loss: 0.0129\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3f1b94c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2493 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.1175 - val_loss: 0.0940\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0970 - val_loss: 0.0894\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0777 - val_loss: 0.0867\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0757 - val_loss: 0.0848\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0661 - val_loss: 0.0825\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0624 - val_loss: 0.0795\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0560 - val_loss: 0.0768\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0550 - val_loss: 0.0744\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0519 - val_loss: 0.0719\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0448 - val_loss: 0.0699\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0412 - val_loss: 0.0686\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0445 - val_loss: 0.0673\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0386 - val_loss: 0.0661\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0367 - val_loss: 0.0647\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0359 - val_loss: 0.0633\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0352 - val_loss: 0.0620\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0360 - val_loss: 0.0609\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0427 - val_loss: 0.0600\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0299 - val_loss: 0.0588\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0293 - val_loss: 0.0578\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0313 - val_loss: 0.0564\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0287 - val_loss: 0.0551\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0315 - val_loss: 0.0534\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0257 - val_loss: 0.0520\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0294 - val_loss: 0.0507\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0453 - val_loss: 0.0495\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0291 - val_loss: 0.0479\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0246 - val_loss: 0.0466\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0331 - val_loss: 0.0456\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0276 - val_loss: 0.0446\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0293 - val_loss: 0.0436\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0272 - val_loss: 0.0433\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0276 - val_loss: 0.0429\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0241 - val_loss: 0.0425\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0228 - val_loss: 0.0421\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0240 - val_loss: 0.0418\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0245 - val_loss: 0.0416\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0303 - val_loss: 0.0415\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0236 - val_loss: 0.0413\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0198 - val_loss: 0.0410\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0216 - val_loss: 0.0406\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0263 - val_loss: 0.0400\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0206 - val_loss: 0.0400\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0225 - val_loss: 0.0392\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0261 - val_loss: 0.0392\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0243 - val_loss: 0.0391\n",
      "Epoch 47/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0254 - val_loss: 0.0382\n",
      "Epoch 48/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0178 - val_loss: 0.0382\n",
      "Epoch 49/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0181 - val_loss: 0.0377\n",
      "Epoch 50/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0170 - val_loss: 0.0374\n",
      "Epoch 51/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0284 - val_loss: 0.0377\n",
      "Epoch 52/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0212 - val_loss: 0.0370\n",
      "Epoch 53/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0186 - val_loss: 0.0370\n",
      "Epoch 54/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0256 - val_loss: 0.0367\n",
      "Epoch 55/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0201 - val_loss: 0.0360\n",
      "Epoch 56/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0211 - val_loss: 0.0358\n",
      "Epoch 57/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0190 - val_loss: 0.0353\n",
      "Epoch 58/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0165 - val_loss: 0.0346\n",
      "Epoch 59/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0167 - val_loss: 0.0341\n",
      "Epoch 60/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0181 - val_loss: 0.0341\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4e34cd820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2494 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.1401 - val_loss: 0.0561\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1541 - val_loss: 0.0548\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1000 - val_loss: 0.0541\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0947 - val_loss: 0.0515\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0913 - val_loss: 0.0522\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0880 - val_loss: 0.0488\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0631 - val_loss: 0.0484\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0634 - val_loss: 0.0471\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0619 - val_loss: 0.0458\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0618 - val_loss: 0.0439\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0611 - val_loss: 0.0434\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0585 - val_loss: 0.0425\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0531 - val_loss: 0.0409\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0546 - val_loss: 0.0398\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0647 - val_loss: 0.0393\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0590 - val_loss: 0.0378\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0434 - val_loss: 0.0365\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0413 - val_loss: 0.0357\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0394 - val_loss: 0.0347\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0509 - val_loss: 0.0345\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0406 - val_loss: 0.0320\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0339 - val_loss: 0.0313\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0291 - val_loss: 0.0307\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0449 - val_loss: 0.0301\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0295 - val_loss: 0.0301\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0313 - val_loss: 0.0291\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0336 - val_loss: 0.0282\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0227 - val_loss: 0.0279\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0220 - val_loss: 0.0273\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0241 - val_loss: 0.0265\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0321 - val_loss: 0.0263\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0308 - val_loss: 0.0260\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0275 - val_loss: 0.0258\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0254 - val_loss: 0.0257\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0232 - val_loss: 0.0252\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0228 - val_loss: 0.0243\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0255 - val_loss: 0.0241\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0204 - val_loss: 0.0238\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0213 - val_loss: 0.0231\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0263 - val_loss: 0.0229\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0221 - val_loss: 0.0228\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0210 - val_loss: 0.0232\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0278 - val_loss: 0.0226\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0179 - val_loss: 0.0221\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0199 - val_loss: 0.0218\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0183 - val_loss: 0.0214\n",
      "Epoch 47/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0196 - val_loss: 0.0212\n",
      "Epoch 48/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0217 - val_loss: 0.0209\n",
      "Epoch 49/60\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0334 - val_loss: 0.0205\n",
      "Epoch 50/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0218 - val_loss: 0.0204\n",
      "Epoch 51/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0231 - val_loss: 0.0199\n",
      "Epoch 52/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0196 - val_loss: 0.0190\n",
      "Epoch 53/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0162 - val_loss: 0.0186\n",
      "Epoch 54/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0289 - val_loss: 0.0180\n",
      "Epoch 55/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0221 - val_loss: 0.0175\n",
      "Epoch 56/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0161 - val_loss: 0.0172\n",
      "Epoch 57/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0173 - val_loss: 0.0167\n",
      "Epoch 58/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0181 - val_loss: 0.0166\n",
      "Epoch 59/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0202 - val_loss: 0.0166\n",
      "Epoch 60/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0189 - val_loss: 0.0169\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4920c4ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2495 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 175ms/step - loss: 0.9753 - val_loss: 0.1721\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.8008 - val_loss: 0.1594\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5958 - val_loss: 0.1508\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.5165 - val_loss: 0.1468\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.4382 - val_loss: 0.1431\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.4422 - val_loss: 0.1384\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.4158 - val_loss: 0.1363\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2879 - val_loss: 0.1366\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.2561 - val_loss: 0.1368\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e88d6f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2496 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 0.4707 - val_loss: 0.0863\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.4122 - val_loss: 0.0818\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3669 - val_loss: 0.0754\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.3457 - val_loss: 0.0705\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.2932 - val_loss: 0.0655\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2933 - val_loss: 0.0617\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.2626 - val_loss: 0.0567\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2383 - val_loss: 0.0529\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.2319 - val_loss: 0.0491\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.2124 - val_loss: 0.0453\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2057 - val_loss: 0.0424\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1900 - val_loss: 0.0400\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1740 - val_loss: 0.0380\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.1680 - val_loss: 0.0363\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1646 - val_loss: 0.0347\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1440 - val_loss: 0.0335\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1337 - val_loss: 0.0327\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1410 - val_loss: 0.0312\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1265 - val_loss: 0.0296\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1360 - val_loss: 0.0285\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1130 - val_loss: 0.0278\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1088 - val_loss: 0.0268\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1105 - val_loss: 0.0266\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0981 - val_loss: 0.0266\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0966 - val_loss: 0.0271\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4002bcee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2497 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 166ms/step - loss: 0.7775 - val_loss: 0.0758\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5711 - val_loss: 0.0725\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.4705 - val_loss: 0.0693\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3630 - val_loss: 0.0667\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2872 - val_loss: 0.0646\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2396 - val_loss: 0.0625\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1933 - val_loss: 0.0601\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1814 - val_loss: 0.0588\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1792 - val_loss: 0.0577\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1807 - val_loss: 0.0564\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1343 - val_loss: 0.0556\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1156 - val_loss: 0.0553\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1142 - val_loss: 0.0541\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1108 - val_loss: 0.0539\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1122 - val_loss: 0.0539\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1017 - val_loss: 0.0552\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb417618a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2498 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.9117 - val_loss: 0.1246\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.5642 - val_loss: 0.1176\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.5108 - val_loss: 0.1027\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.4528 - val_loss: 0.0943\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.3656 - val_loss: 0.0853\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3025 - val_loss: 0.0786\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2598 - val_loss: 0.0739\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2556 - val_loss: 0.0714\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2204 - val_loss: 0.0685\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.2307 - val_loss: 0.0655\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2234 - val_loss: 0.0615\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1798 - val_loss: 0.0576\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1654 - val_loss: 0.0546\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1625 - val_loss: 0.0534\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1506 - val_loss: 0.0496\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1403 - val_loss: 0.0487\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1263 - val_loss: 0.0482\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1396 - val_loss: 0.0473\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1174 - val_loss: 0.0457\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1256 - val_loss: 0.0454\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1124 - val_loss: 0.0441\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0947 - val_loss: 0.0442\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1025 - val_loss: 0.0435\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1017 - val_loss: 0.0436\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0901 - val_loss: 0.0442\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb40f1f4af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2499 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 1.4573 - val_loss: 0.1331\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.7263 - val_loss: 0.1320\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4771 - val_loss: 0.1276\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.4034 - val_loss: 0.1233\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.3037 - val_loss: 0.1178\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.2574 - val_loss: 0.1113\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2130 - val_loss: 0.1040\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2029 - val_loss: 0.0974\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2155 - val_loss: 0.0919\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1643 - val_loss: 0.0866\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1453 - val_loss: 0.0819\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1655 - val_loss: 0.0772\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1492 - val_loss: 0.0728\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1442 - val_loss: 0.0693\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1491 - val_loss: 0.0669\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1248 - val_loss: 0.0647\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1143 - val_loss: 0.0631\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1032 - val_loss: 0.0621\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0978 - val_loss: 0.0612\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0873 - val_loss: 0.0606\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1043 - val_loss: 0.0612\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0885 - val_loss: 0.0625\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb453f99430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2500 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 1.2234 - val_loss: 0.1544\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6757 - val_loss: 0.1365\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3715 - val_loss: 0.1331\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5232 - val_loss: 0.1227\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.4652 - val_loss: 0.1155\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2976 - val_loss: 0.1116\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.2849 - val_loss: 0.1047\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2988 - val_loss: 0.1031\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1892 - val_loss: 0.0995\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1758 - val_loss: 0.0941\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1742 - val_loss: 0.0921\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1488 - val_loss: 0.0892\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1605 - val_loss: 0.0838\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1604 - val_loss: 0.0797\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1260 - val_loss: 0.0763\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1315 - val_loss: 0.0754\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1026 - val_loss: 0.0736\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1227 - val_loss: 0.0680\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1386 - val_loss: 0.0644\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0955 - val_loss: 0.0641\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0928 - val_loss: 0.0616\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0898 - val_loss: 0.0569\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0844 - val_loss: 0.0554\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0850 - val_loss: 0.0515\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0936 - val_loss: 0.0496\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1051 - val_loss: 0.0446\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0744 - val_loss: 0.0427\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0610 - val_loss: 0.0399\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0601 - val_loss: 0.0380\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1106 - val_loss: 0.0369\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.0725 - val_loss: 0.0352\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0643 - val_loss: 0.0346\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0823 - val_loss: 0.0332\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0587 - val_loss: 0.0317\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0762 - val_loss: 0.0308\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0745 - val_loss: 0.0288\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0696 - val_loss: 0.0287\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0676 - val_loss: 0.0274\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0662 - val_loss: 0.0272\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0837 - val_loss: 0.0265\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0611 - val_loss: 0.0259\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0578 - val_loss: 0.0252\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0489 - val_loss: 0.0250\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0982 - val_loss: 0.0248\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0528 - val_loss: 0.0259\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0435 - val_loss: 0.0272\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb40a465c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2501 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 1.4881 - val_loss: 0.2371\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.8667 - val_loss: 0.2151\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.7163 - val_loss: 0.1995\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5803 - val_loss: 0.2171\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4472 - val_loss: 0.2159\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb41aa1baf0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2502 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.4625 - val_loss: 0.1417\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3301 - val_loss: 0.1347\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2548 - val_loss: 0.1290\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2420 - val_loss: 0.1213\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2022 - val_loss: 0.1196\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1608 - val_loss: 0.1140\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1454 - val_loss: 0.1108\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1381 - val_loss: 0.1042\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1619 - val_loss: 0.0988\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1427 - val_loss: 0.0990\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1158 - val_loss: 0.0973\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1049 - val_loss: 0.0958\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0920 - val_loss: 0.0919\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0886 - val_loss: 0.0897\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0801 - val_loss: 0.0907\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0774 - val_loss: 0.0882\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0765 - val_loss: 0.0879\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0804 - val_loss: 0.0844\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0603 - val_loss: 0.0829\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.0610 - val_loss: 0.0806\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0561 - val_loss: 0.0783\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0453 - val_loss: 0.0763\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0526 - val_loss: 0.0752\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0492 - val_loss: 0.0736\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0454 - val_loss: 0.0711\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0504 - val_loss: 0.0691\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0607 - val_loss: 0.0682\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0517 - val_loss: 0.0666\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0781 - val_loss: 0.0650\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0546 - val_loss: 0.0638\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0437 - val_loss: 0.0638\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0389 - val_loss: 0.0635\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0430 - val_loss: 0.0618\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0458 - val_loss: 0.0610\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0431 - val_loss: 0.0600\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0432 - val_loss: 0.0586\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0427 - val_loss: 0.0575\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0433 - val_loss: 0.0563\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0329 - val_loss: 0.0563\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0369 - val_loss: 0.0548\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0357 - val_loss: 0.0536\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0304 - val_loss: 0.0537\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0363 - val_loss: 0.0542\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4903c7670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2503 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.3461 - val_loss: 0.0266\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2413 - val_loss: 0.0261\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2022 - val_loss: 0.0251\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1690 - val_loss: 0.0250\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1509 - val_loss: 0.0246\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1520 - val_loss: 0.0240\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1417 - val_loss: 0.0239\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1247 - val_loss: 0.0237\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1123 - val_loss: 0.0240\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1102 - val_loss: 0.0236\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.1125 - val_loss: 0.0233\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1213 - val_loss: 0.0225\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1261 - val_loss: 0.0225\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0933 - val_loss: 0.0225\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0711 - val_loss: 0.0218\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0869 - val_loss: 0.0199\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0785 - val_loss: 0.0194\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0734 - val_loss: 0.0188\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0924 - val_loss: 0.0186\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0756 - val_loss: 0.0182\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0742 - val_loss: 0.0178\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0638 - val_loss: 0.0182\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0650 - val_loss: 0.0184\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4516e1550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2504 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 1.1212 - val_loss: 0.0162\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.7998 - val_loss: 0.0148\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4739 - val_loss: 0.0138\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.3509 - val_loss: 0.0134\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3256 - val_loss: 0.0128\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2520 - val_loss: 0.0124\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2329 - val_loss: 0.0122\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.2072 - val_loss: 0.0121\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1832 - val_loss: 0.0121\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1733 - val_loss: 0.0122\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb43b1809d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2505 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.3464 - val_loss: 0.1400\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2351 - val_loss: 0.1295\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2043 - val_loss: 0.1211\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1695 - val_loss: 0.1124\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1560 - val_loss: 0.1045\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1770 - val_loss: 0.0959\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1107 - val_loss: 0.0884\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1078 - val_loss: 0.0798\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0980 - val_loss: 0.0694\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0871 - val_loss: 0.0595\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0959 - val_loss: 0.0500\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0890 - val_loss: 0.0417\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0847 - val_loss: 0.0348\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0655 - val_loss: 0.0284\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0561 - val_loss: 0.0229\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0543 - val_loss: 0.0186\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0485 - val_loss: 0.0148\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0555 - val_loss: 0.0121\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0469 - val_loss: 0.0104\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0512 - val_loss: 0.0098\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0557 - val_loss: 0.0104\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0460 - val_loss: 0.0121\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb47d5c1430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2506 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.4348 - val_loss: 0.0673\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3175 - val_loss: 0.0686\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2391 - val_loss: 0.0681\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb491078f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2507 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.6530 - val_loss: 0.3905\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3383 - val_loss: 0.3581\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1807 - val_loss: 0.3395\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1828 - val_loss: 0.3198\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1205 - val_loss: 0.3051\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1039 - val_loss: 0.2925\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0897 - val_loss: 0.2783\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0990 - val_loss: 0.2670\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1031 - val_loss: 0.2499\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1060 - val_loss: 0.2367\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0737 - val_loss: 0.2250\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0961 - val_loss: 0.2139\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0665 - val_loss: 0.2033\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0720 - val_loss: 0.1929\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0577 - val_loss: 0.1830\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0640 - val_loss: 0.1743\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0581 - val_loss: 0.1659\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0558 - val_loss: 0.1575\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0576 - val_loss: 0.1489\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0449 - val_loss: 0.1407\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0577 - val_loss: 0.1325\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0633 - val_loss: 0.1257\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0422 - val_loss: 0.1178\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0393 - val_loss: 0.1109\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0377 - val_loss: 0.1052\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0566 - val_loss: 0.0999\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0421 - val_loss: 0.0953\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0434 - val_loss: 0.0913\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0364 - val_loss: 0.0875\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0565 - val_loss: 0.0836\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0346 - val_loss: 0.0805\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0501 - val_loss: 0.0775\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0292 - val_loss: 0.0743\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0316 - val_loss: 0.0713\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0485 - val_loss: 0.0678\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0542 - val_loss: 0.0655\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0426 - val_loss: 0.0635\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0317 - val_loss: 0.0619\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0259 - val_loss: 0.0601\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0522 - val_loss: 0.0573\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0358 - val_loss: 0.0552\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0290 - val_loss: 0.0539\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0327 - val_loss: 0.0527\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0309 - val_loss: 0.0517\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.032 - 0s 11ms/step - loss: 0.0276 - val_loss: 0.0501\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0300 - val_loss: 0.0491\n",
      "Epoch 47/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0313 - val_loss: 0.0479\n",
      "Epoch 48/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0362 - val_loss: 0.0459\n",
      "Epoch 49/60\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0266 - val_loss: 0.0442\n",
      "Epoch 50/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0483 - val_loss: 0.0412\n",
      "Epoch 51/60\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 0.0325 - val_loss: 0.0412\n",
      "Epoch 52/60\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0249 - val_loss: 0.0395\n",
      "Epoch 53/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0274 - val_loss: 0.0386\n",
      "Epoch 54/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0235 - val_loss: 0.0373\n",
      "Epoch 55/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0278 - val_loss: 0.0363\n",
      "Epoch 56/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0310 - val_loss: 0.0356\n",
      "Epoch 57/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0321 - val_loss: 0.0349\n",
      "Epoch 58/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0256 - val_loss: 0.0345\n",
      "Epoch 59/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0323 - val_loss: 0.0335\n",
      "Epoch 60/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0280 - val_loss: 0.0331\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb44e6ec940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2508 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.5138 - val_loss: 0.0352\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3041 - val_loss: 0.0424\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2241 - val_loss: 0.0470\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb464200d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2509 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.2695 - val_loss: 0.0418\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1980 - val_loss: 0.0360\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1736 - val_loss: 0.0343\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1152 - val_loss: 0.0334\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0990 - val_loss: 0.0320\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0892 - val_loss: 0.0269\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0907 - val_loss: 0.0244\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0737 - val_loss: 0.0226\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0697 - val_loss: 0.0212\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0553 - val_loss: 0.0208\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0489 - val_loss: 0.0206\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0516 - val_loss: 0.0195\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0566 - val_loss: 0.0189\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 0.3996 - val_loss: 0.0451\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2778 - val_loss: 0.0496\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1591 - val_loss: 0.0527\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb43c5dc940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2700 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.4799 - val_loss: 0.0915\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1517 - val_loss: 0.0957\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1049 - val_loss: 0.1001\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb47c0a9e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2701 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.2785 - val_loss: 0.0287\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1661 - val_loss: 0.0256\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0967 - val_loss: 0.0222\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0995 - val_loss: 0.0205\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0872 - val_loss: 0.0183\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0709 - val_loss: 0.0162\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0803 - val_loss: 0.0136\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0581 - val_loss: 0.0124\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0499 - val_loss: 0.0109\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0753 - val_loss: 0.0101\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0652 - val_loss: 0.0095\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0553 - val_loss: 0.0086\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0567 - val_loss: 0.0077\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0800 - val_loss: 0.0073\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0498 - val_loss: 0.0069\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0453 - val_loss: 0.0068\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0528 - val_loss: 0.0066\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0645 - val_loss: 0.0063\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0520 - val_loss: 0.0064\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0528 - val_loss: 0.0062\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0485 - val_loss: 0.0063\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0447 - val_loss: 0.0065\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb41454a3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2702 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.5956 - val_loss: 0.0112\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3883 - val_loss: 0.0140\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.2880 - val_loss: 0.0162\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3f9cfb310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2703 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 0.2927 - val_loss: 0.0218\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1439 - val_loss: 0.0232\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1113 - val_loss: 0.0247\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4040d6430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2704 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 0.5876 - val_loss: 0.0113\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.2449 - val_loss: 0.0113\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1245 - val_loss: 0.0118\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ee982670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2705 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.4183WARNING:tensorflow:5 out of the last 32 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb3f1ee0790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.4658 - val_loss: 0.2531\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3334 - val_loss: 0.2457\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.3599 - val_loss: 0.2323\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2545 - val_loss: 0.2228\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.2477 - val_loss: 0.2120\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.2102 - val_loss: 0.2035\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1829 - val_loss: 0.1934\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1659 - val_loss: 0.1853\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1384 - val_loss: 0.1767\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1519 - val_loss: 0.1669\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1368 - val_loss: 0.1591\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1550 - val_loss: 0.1515\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1452 - val_loss: 0.1426\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1474 - val_loss: 0.1323\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1061 - val_loss: 0.1249\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1016 - val_loss: 0.1178\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1043 - val_loss: 0.1108\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0828 - val_loss: 0.1064\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0999 - val_loss: 0.1025\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0833 - val_loss: 0.0985\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0906 - val_loss: 0.0940\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0784 - val_loss: 0.0929\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0730 - val_loss: 0.0928\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0841 - val_loss: 0.0917\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0766 - val_loss: 0.0921\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0754 - val_loss: 0.0930\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ef0a2f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2706 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 0.4447 - val_loss: 0.0059\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2291 - val_loss: 0.0063\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1911 - val_loss: 0.0069\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3f67068b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2707 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 0.1313 - val_loss: 0.2780\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0907 - val_loss: 0.2813\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0747 - val_loss: 0.2863\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3fdda0670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2708 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 2.8055 - val_loss: 0.2482\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.5159 - val_loss: 0.2300\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.3123 - val_loss: 0.2055\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.9634 - val_loss: 0.1821\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.7564 - val_loss: 0.1645\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6622 - val_loss: 0.1452\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.5610 - val_loss: 0.1288\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5341 - val_loss: 0.1148\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5448 - val_loss: 0.1033\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4715 - val_loss: 0.0948\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.4006 - val_loss: 0.0864\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.4758 - val_loss: 0.0803\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.3471 - val_loss: 0.0720\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.3598 - val_loss: 0.0658\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2925 - val_loss: 0.0592\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2909 - val_loss: 0.0541\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3063 - val_loss: 0.0471\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2552 - val_loss: 0.0423\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2577 - val_loss: 0.0380\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2463 - val_loss: 0.0340\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2076 - val_loss: 0.0309\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2391 - val_loss: 0.0284\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2374 - val_loss: 0.0260\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1757 - val_loss: 0.0235\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1619 - val_loss: 0.0216\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1540 - val_loss: 0.0200\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1622 - val_loss: 0.0184\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1363 - val_loss: 0.0173\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1712 - val_loss: 0.0165\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1716 - val_loss: 0.0159\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1486 - val_loss: 0.0149\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1372 - val_loss: 0.0142\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1370 - val_loss: 0.0139\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1081 - val_loss: 0.0137\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1043 - val_loss: 0.0136\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1103 - val_loss: 0.0135\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1233 - val_loss: 0.0136\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1253 - val_loss: 0.0131\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1199 - val_loss: 0.0134\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1232 - val_loss: 0.0139\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e56aed30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2709 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.8969 - val_loss: 0.0152\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5822 - val_loss: 0.0146\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.4798 - val_loss: 0.0142\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.4081 - val_loss: 0.0136\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3492 - val_loss: 0.0138\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3231 - val_loss: 0.0133\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.3179 - val_loss: 0.0131\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3332 - val_loss: 0.0122\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2472 - val_loss: 0.0118\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2261 - val_loss: 0.0120\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.2018 - val_loss: 0.0126\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cb538ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2710 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 1.6706 - val_loss: 0.0589\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.0447 - val_loss: 0.0508\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.7167 - val_loss: 0.0448\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5600 - val_loss: 0.0405\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5438 - val_loss: 0.0359\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3956 - val_loss: 0.0317\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3315 - val_loss: 0.0287\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2875 - val_loss: 0.0272\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2698 - val_loss: 0.0256\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2584 - val_loss: 0.0254\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2106 - val_loss: 0.0253\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2304 - val_loss: 0.0264\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1816 - val_loss: 0.0281\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cb5f2af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2711 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.2203 - val_loss: 0.0543\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1177 - val_loss: 0.0514\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0677 - val_loss: 0.0491\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0708 - val_loss: 0.0468\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0772 - val_loss: 0.0441\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0858 - val_loss: 0.0426\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0606 - val_loss: 0.0407\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0580 - val_loss: 0.0392\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0782 - val_loss: 0.0373\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0751 - val_loss: 0.0363\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0670 - val_loss: 0.0349\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0379 - val_loss: 0.0342\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0434 - val_loss: 0.0330\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0429 - val_loss: 0.0324\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0574 - val_loss: 0.0315\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0478 - val_loss: 0.0311\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0366 - val_loss: 0.0313\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0455 - val_loss: 0.0314\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cc4a2b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2712 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.5371 - val_loss: 0.0961\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3030 - val_loss: 0.0907\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2204 - val_loss: 0.0876\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1717 - val_loss: 0.0821\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1521 - val_loss: 0.0785\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1815 - val_loss: 0.0751\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1519 - val_loss: 0.0728\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 0.1207 - val_loss: 0.0710\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1202 - val_loss: 0.0703\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0894 - val_loss: 0.0682\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0865 - val_loss: 0.0652\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0864 - val_loss: 0.0626\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0739 - val_loss: 0.0610\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0733 - val_loss: 0.0583\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0779 - val_loss: 0.0540\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0705 - val_loss: 0.0506\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0704 - val_loss: 0.0484\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0659 - val_loss: 0.0462\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0576 - val_loss: 0.0440\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0793 - val_loss: 0.0412\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0526 - val_loss: 0.0378\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0583 - val_loss: 0.0363\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0496 - val_loss: 0.0353\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0527 - val_loss: 0.0336\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0482 - val_loss: 0.0316\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0448 - val_loss: 0.0306\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0510 - val_loss: 0.0299\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0422 - val_loss: 0.0284\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0446 - val_loss: 0.0274\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0362 - val_loss: 0.0275\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0345 - val_loss: 0.0269\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0307 - val_loss: 0.0264\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0394 - val_loss: 0.0262\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0350 - val_loss: 0.0242\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0349 - val_loss: 0.0232\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0355 - val_loss: 0.0222\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0331 - val_loss: 0.0220\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0292 - val_loss: 0.0216\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0401 - val_loss: 0.0214\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0304 - val_loss: 0.0213\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0407 - val_loss: 0.0210\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0295 - val_loss: 0.0204\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0268 - val_loss: 0.0200\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0349 - val_loss: 0.0195\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0227 - val_loss: 0.0193\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0326 - val_loss: 0.0189\n",
      "Epoch 47/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0391 - val_loss: 0.0186\n",
      "Epoch 48/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0364 - val_loss: 0.0186\n",
      "Epoch 49/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0251 - val_loss: 0.0187\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cd7be310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2713 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.4649 - val_loss: 0.0253\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2502 - val_loss: 0.0269\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1886 - val_loss: 0.0283\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d35dd9d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2714 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.4681 - val_loss: 0.0119\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2696 - val_loss: 0.0102\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1988 - val_loss: 0.0089\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1274 - val_loss: 0.0077\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1131 - val_loss: 0.0069\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0975 - val_loss: 0.0064\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0939 - val_loss: 0.0061\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0685 - val_loss: 0.0060\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0685 - val_loss: 0.0060\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0615 - val_loss: 0.0060\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3dbcd33a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2715 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.1662 - val_loss: 0.1936\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1108 - val_loss: 0.1915\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1120 - val_loss: 0.1883\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0931 - val_loss: 0.1881\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0749 - val_loss: 0.1873\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0804 - val_loss: 0.1865\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0788 - val_loss: 0.1861\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0666 - val_loss: 0.1867\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0534 - val_loss: 0.1886\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d6337310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2716 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.1782 - val_loss: 0.0189\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1695 - val_loss: 0.0187\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1128 - val_loss: 0.0193\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0898 - val_loss: 0.0199\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3eab0d280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2717 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0803 - val_loss: 0.0149\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0619 - val_loss: 0.0135\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0574 - val_loss: 0.0123\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0413 - val_loss: 0.0111\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0485 - val_loss: 0.0104\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0379 - val_loss: 0.0101\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0480 - val_loss: 0.0099\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0370 - val_loss: 0.0097\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0278 - val_loss: 0.0094\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0277 - val_loss: 0.0092\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0249 - val_loss: 0.0092\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0210 - val_loss: 0.0094\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ba8edaf0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2718 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 1.2971 - val_loss: 0.0059\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.7888 - val_loss: 0.0094\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5358 - val_loss: 0.0132\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c547d310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2719 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.4438 - val_loss: 0.0113\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1912 - val_loss: 0.0146\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1253 - val_loss: 0.0154\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c5b4caf0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2720 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.5693 - val_loss: 0.0160\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3353 - val_loss: 0.0145\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2814 - val_loss: 0.0140\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2364 - val_loss: 0.0144\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1862 - val_loss: 0.0152\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c61c98b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2721 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.6954 - val_loss: 0.0274\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3429 - val_loss: 0.0238\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2276 - val_loss: 0.0214\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1391 - val_loss: 0.0196\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0730 - val_loss: 0.0183\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0899 - val_loss: 0.0172\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0773 - val_loss: 0.0161\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0531 - val_loss: 0.0147\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0584 - val_loss: 0.0134\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0643 - val_loss: 0.0125\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0490 - val_loss: 0.0118\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0703 - val_loss: 0.0112\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0554 - val_loss: 0.0106\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0574 - val_loss: 0.0103\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0390 - val_loss: 0.0100\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0424 - val_loss: 0.0096\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0351 - val_loss: 0.0092\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0452 - val_loss: 0.0090\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0848 - val_loss: 0.0083\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0632 - val_loss: 0.0078\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0328 - val_loss: 0.0075\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0482 - val_loss: 0.0073\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0413 - val_loss: 0.0069\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0247 - val_loss: 0.0069\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0376 - val_loss: 0.0070\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0382 - val_loss: 0.0072\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a88dc550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2722 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.2482 - val_loss: 0.0452\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1562 - val_loss: 0.0470\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1276 - val_loss: 0.0489\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a9864ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2723 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.5666 - val_loss: 0.0118\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2079 - val_loss: 0.0118\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0985 - val_loss: 0.0118\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ac545d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2724 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.2777 - val_loss: 0.1294\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1786 - val_loss: 0.1242\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1589 - val_loss: 0.1186\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1396 - val_loss: 0.1163\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1092 - val_loss: 0.1131\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1107 - val_loss: 0.1100\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0914 - val_loss: 0.1070\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0787 - val_loss: 0.1043\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0713 - val_loss: 0.1023\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0767 - val_loss: 0.0998\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0792 - val_loss: 0.0985\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0650 - val_loss: 0.0973\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0559 - val_loss: 0.0968\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0695 - val_loss: 0.0958\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0485 - val_loss: 0.0940\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0496 - val_loss: 0.0924\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0492 - val_loss: 0.0926\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0481 - val_loss: 0.0925\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3b2060670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2725 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.2953 - val_loss: 0.0568\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.2158 - val_loss: 0.0622\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1664 - val_loss: 0.0681\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4979aaf70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2726 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 1.3131 - val_loss: 0.2725\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5684 - val_loss: 0.2462\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.3931 - val_loss: 0.2312\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.3383 - val_loss: 0.2271\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.2291 - val_loss: 0.2215\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1909 - val_loss: 0.2199\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1360 - val_loss: 0.2255\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1418 - val_loss: 0.2156\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1329 - val_loss: 0.2151\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1003 - val_loss: 0.2131\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0984 - val_loss: 0.2068\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0760 - val_loss: 0.1994\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0817 - val_loss: 0.1920\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0910 - val_loss: 0.1831\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0597 - val_loss: 0.1772\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0604 - val_loss: 0.1710\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0729 - val_loss: 0.1644\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0617 - val_loss: 0.1591\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0492 - val_loss: 0.1526\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0608 - val_loss: 0.1465\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0560 - val_loss: 0.1387\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0449 - val_loss: 0.1336\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0585 - val_loss: 0.1274\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0636 - val_loss: 0.1212\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0447 - val_loss: 0.1159\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0631 - val_loss: 0.1093\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0547 - val_loss: 0.1062\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0492 - val_loss: 0.1027\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0521 - val_loss: 0.0977\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0483 - val_loss: 0.0937\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0566 - val_loss: 0.0895\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0375 - val_loss: 0.0862\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0453 - val_loss: 0.0811\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0508 - val_loss: 0.0769\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0444 - val_loss: 0.0732\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0391 - val_loss: 0.0690\n",
      "Epoch 37/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0453 - val_loss: 0.0676\n",
      "Epoch 38/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0439 - val_loss: 0.0647\n",
      "Epoch 39/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0615 - val_loss: 0.0618\n",
      "Epoch 40/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0533 - val_loss: 0.0561\n",
      "Epoch 41/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0298 - val_loss: 0.0542\n",
      "Epoch 42/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0271 - val_loss: 0.0522\n",
      "Epoch 43/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0266 - val_loss: 0.0504\n",
      "Epoch 44/60\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0291 - val_loss: 0.0464\n",
      "Epoch 45/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0350 - val_loss: 0.0438\n",
      "Epoch 46/60\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.023 - 0s 10ms/step - loss: 0.0281 - val_loss: 0.0428\n",
      "Epoch 47/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0294 - val_loss: 0.0426\n",
      "Epoch 48/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0342 - val_loss: 0.0410\n",
      "Epoch 49/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0187 - val_loss: 0.0399\n",
      "Epoch 50/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0259 - val_loss: 0.0379\n",
      "Epoch 51/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0294 - val_loss: 0.0368\n",
      "Epoch 52/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0543 - val_loss: 0.0343\n",
      "Epoch 53/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0221 - val_loss: 0.0337\n",
      "Epoch 54/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0228 - val_loss: 0.0333\n",
      "Epoch 55/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0210 - val_loss: 0.0329\n",
      "Epoch 56/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0222 - val_loss: 0.0328\n",
      "Epoch 57/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0323 - val_loss: 0.0312\n",
      "Epoch 58/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0224 - val_loss: 0.0293\n",
      "Epoch 59/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0220 - val_loss: 0.0281\n",
      "Epoch 60/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0264 - val_loss: 0.0267\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb48b4de280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2727 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.2246 - val_loss: 0.0177\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1755 - val_loss: 0.0173\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1479 - val_loss: 0.0169\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1328 - val_loss: 0.0164\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1268 - val_loss: 0.0160\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1145 - val_loss: 0.0154\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.1182 - val_loss: 0.0149\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1333 - val_loss: 0.0142\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0936 - val_loss: 0.0138\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0965 - val_loss: 0.0132\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1012 - val_loss: 0.0126\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0810 - val_loss: 0.0120\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0819 - val_loss: 0.0115\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0759 - val_loss: 0.0111\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0936 - val_loss: 0.0107\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0629 - val_loss: 0.0103\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0715 - val_loss: 0.0099\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0594 - val_loss: 0.0095\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0625 - val_loss: 0.0093\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0558 - val_loss: 0.0091\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0515 - val_loss: 0.0089\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0591 - val_loss: 0.0087\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0579 - val_loss: 0.0086\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0439 - val_loss: 0.0085\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0414 - val_loss: 0.0084\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0444 - val_loss: 0.0084\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0409 - val_loss: 0.0083\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0429 - val_loss: 0.0084\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0383 - val_loss: 0.0084\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb47d5c1430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2728 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.7030 - val_loss: 0.1011\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2000 - val_loss: 0.0871\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1468 - val_loss: 0.0757\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1049 - val_loss: 0.0671\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1027 - val_loss: 0.0566\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1081 - val_loss: 0.0508\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0846 - val_loss: 0.0426\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0776 - val_loss: 0.0355\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0480 - val_loss: 0.0320\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0619 - val_loss: 0.0262\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0493 - val_loss: 0.0233\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0465 - val_loss: 0.0201\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0402 - val_loss: 0.0179\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0389 - val_loss: 0.0154\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0538 - val_loss: 0.0120\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0283 - val_loss: 0.0107\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0379 - val_loss: 0.0089\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 0.0330 - val_loss: 0.0073\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0243 - val_loss: 0.0063\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0301 - val_loss: 0.0056\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0400 - val_loss: 0.0045\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0209 - val_loss: 0.0041\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0240 - val_loss: 0.0035\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0230 - val_loss: 0.0033\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0237 - val_loss: 0.0033\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0516 - val_loss: 0.0032\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0440 - val_loss: 0.0031\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0250 - val_loss: 0.0031\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0273 - val_loss: 0.0030\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0193 - val_loss: 0.0029\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0315 - val_loss: 0.0029\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.0203 - val_loss: 0.0029\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0192 - val_loss: 0.0029\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0205 - val_loss: 0.0029\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0153 - val_loss: 0.0029\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb41652f1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2729 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.2966 - val_loss: 0.0185\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1682 - val_loss: 0.0151\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1177 - val_loss: 0.0125\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0992 - val_loss: 0.0105\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0820 - val_loss: 0.0088\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0827 - val_loss: 0.0074\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0625 - val_loss: 0.0065\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0497 - val_loss: 0.0057\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0473 - val_loss: 0.0050\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0439 - val_loss: 0.0046\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0386 - val_loss: 0.0044\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0353 - val_loss: 0.0043\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0347 - val_loss: 0.0043\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0314 - val_loss: 0.0044\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c72e23a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2730 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.1430 - val_loss: 0.0274\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1116 - val_loss: 0.0258\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0660 - val_loss: 0.0238\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0566 - val_loss: 0.0228\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0520 - val_loss: 0.0221\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0560 - val_loss: 0.0218\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0322 - val_loss: 0.0206\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0513 - val_loss: 0.0194\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0382 - val_loss: 0.0185\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0465 - val_loss: 0.0173\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0634 - val_loss: 0.0171\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0421 - val_loss: 0.0168\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0283 - val_loss: 0.0170\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0285 - val_loss: 0.0173\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d90eb040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2731 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.3919 - val_loss: 0.0415\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2130 - val_loss: 0.0369\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1687 - val_loss: 0.0336\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1735 - val_loss: 0.0296\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1485 - val_loss: 0.0268\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1448 - val_loss: 0.0242\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1199 - val_loss: 0.0220\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1011 - val_loss: 0.0205\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.0919 - val_loss: 0.0190\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0923 - val_loss: 0.0175\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0927 - val_loss: 0.0166\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0773 - val_loss: 0.0155\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0736 - val_loss: 0.0149\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0650 - val_loss: 0.0144\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0644 - val_loss: 0.0140\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0628 - val_loss: 0.0134\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0543 - val_loss: 0.0133\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0633 - val_loss: 0.0128\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0371 - val_loss: 0.0123\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0462 - val_loss: 0.0119\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0477 - val_loss: 0.0118\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0418 - val_loss: 0.0117\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0335 - val_loss: 0.0118\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0443 - val_loss: 0.0118\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb399038dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2732 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 1.2676 - val_loss: 0.0110\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.7411 - val_loss: 0.0083\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.3870 - val_loss: 0.0073\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3101 - val_loss: 0.0067\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1904 - val_loss: 0.0064\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1698 - val_loss: 0.0063\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1472 - val_loss: 0.0063\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1280 - val_loss: 0.0065\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c47fea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2733 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.9025 - val_loss: 0.6220\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.5849 - val_loss: 0.5981\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4245 - val_loss: 0.5736\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2864 - val_loss: 0.5494\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2180 - val_loss: 0.5299\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.2198 - val_loss: 0.5138\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1650 - val_loss: 0.4994\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1476 - val_loss: 0.4853\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1377 - val_loss: 0.4706\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1468 - val_loss: 0.4611\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1027 - val_loss: 0.4487\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1057 - val_loss: 0.4386\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0992 - val_loss: 0.4324\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0990 - val_loss: 0.4274\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0839 - val_loss: 0.4241\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0775 - val_loss: 0.4172\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0886 - val_loss: 0.4145\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0786 - val_loss: 0.4081\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0815 - val_loss: 0.4046\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0798 - val_loss: 0.4024\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0728 - val_loss: 0.4045\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0618 - val_loss: 0.4015\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0604 - val_loss: 0.4036\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0624 - val_loss: 0.4066\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c35769d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2734 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0799 - val_loss: 0.0118\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0601 - val_loss: 0.0129\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0551 - val_loss: 0.0146\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb397ed7670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2735 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.6429 - val_loss: 0.0274\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2172 - val_loss: 0.0261\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1705 - val_loss: 0.0246\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1339 - val_loss: 0.0228\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0979 - val_loss: 0.0213\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1027 - val_loss: 0.0202\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0853 - val_loss: 0.0186\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0935 - val_loss: 0.0166\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.0873 - val_loss: 0.0155\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0882 - val_loss: 0.0140\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0786 - val_loss: 0.0129\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0594 - val_loss: 0.0113\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0647 - val_loss: 0.0105\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0525 - val_loss: 0.0094\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0509 - val_loss: 0.0086\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0485 - val_loss: 0.0072\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0489 - val_loss: 0.0066\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0449 - val_loss: 0.0057\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0402 - val_loss: 0.0050\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0426 - val_loss: 0.0043\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0510 - val_loss: 0.0040\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0451 - val_loss: 0.0036\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0531 - val_loss: 0.0034\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0411 - val_loss: 0.0033\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0374 - val_loss: 0.0034\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0272 - val_loss: 0.0033\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0310 - val_loss: 0.0032\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0298 - val_loss: 0.0032\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0289 - val_loss: 0.0032\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0414 - val_loss: 0.0031\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0247 - val_loss: 0.0030\n",
      "Epoch 32/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0269 - val_loss: 0.0030\n",
      "Epoch 33/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0271 - val_loss: 0.0029\n",
      "Epoch 34/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0183 - val_loss: 0.0029\n",
      "Epoch 35/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0257 - val_loss: 0.0030\n",
      "Epoch 36/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0244 - val_loss: 0.0031\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c4e355e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2736 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.3143 - val_loss: 0.1125\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1938 - val_loss: 0.1172\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1655 - val_loss: 0.1279\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a4f11160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2737 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.4004 - val_loss: 0.0057\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2526 - val_loss: 0.0059\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1851 - val_loss: 0.0057\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1635 - val_loss: 0.0056\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1488 - val_loss: 0.0059\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1204 - val_loss: 0.0067\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a5336040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2738 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.2688 - val_loss: 0.0190\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1382 - val_loss: 0.0194\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0999 - val_loss: 0.0189\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0744 - val_loss: 0.0178\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0788 - val_loss: 0.0163\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0687 - val_loss: 0.0161\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0375 - val_loss: 0.0156\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0372 - val_loss: 0.0150\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0289 - val_loss: 0.0140\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0305 - val_loss: 0.0138\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0461 - val_loss: 0.0141\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0259 - val_loss: 0.0143\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a567ab80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2739 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.2494 - val_loss: 0.0268\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1107 - val_loss: 0.0244\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0630 - val_loss: 0.0226\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0516 - val_loss: 0.0205\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0427 - val_loss: 0.0180\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0368 - val_loss: 0.0154\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0323 - val_loss: 0.0136\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0339 - val_loss: 0.0122\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0248 - val_loss: 0.0106\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0266 - val_loss: 0.0096\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0244 - val_loss: 0.0086\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0225 - val_loss: 0.0080\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0226 - val_loss: 0.0078\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0175 - val_loss: 0.0076\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0155 - val_loss: 0.0075\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0207 - val_loss: 0.0074\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0224 - val_loss: 0.0073\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0197 - val_loss: 0.0072\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0175 - val_loss: 0.0071\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0176 - val_loss: 0.0071\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0175 - val_loss: 0.0070\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0210 - val_loss: 0.0070\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0158 - val_loss: 0.0070\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0139 - val_loss: 0.0070\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0204 - val_loss: 0.0071\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a5a72af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2740 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.3127 - val_loss: 0.0312\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1960 - val_loss: 0.0319\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0882 - val_loss: 0.0323\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a5e6a550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2741 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 1.3712 - val_loss: 0.0054\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.8839 - val_loss: 0.0051\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6676 - val_loss: 0.0051\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4378 - val_loss: 0.0052\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a62394c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2742 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.2713 - val_loss: 0.0046\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1774 - val_loss: 0.0046\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1020 - val_loss: 0.0048\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a6656ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2743 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 0.5483 - val_loss: 0.0651\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2851 - val_loss: 0.0525\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2026 - val_loss: 0.0454\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1642 - val_loss: 0.0415\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1402 - val_loss: 0.0395\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1098 - val_loss: 0.0382\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0821 - val_loss: 0.0356\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0628 - val_loss: 0.0354\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0513 - val_loss: 0.0352\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0488 - val_loss: 0.0351\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0949 - val_loss: 0.0331\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0550 - val_loss: 0.0323\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0358 - val_loss: 0.0318\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0378 - val_loss: 0.0317\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0374 - val_loss: 0.0322\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0453 - val_loss: 0.0319\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3890384c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2744 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.1386 - val_loss: 0.0244\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.1034 - val_loss: 0.0222\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0822 - val_loss: 0.0203\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0786 - val_loss: 0.0187\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.0632 - val_loss: 0.0178\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0589 - val_loss: 0.0177\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0520 - val_loss: 0.0177\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0512 - val_loss: 0.0177\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3893795e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2745 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.7944 - val_loss: 0.3243\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5458 - val_loss: 0.2951\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.4584 - val_loss: 0.2634\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2773 - val_loss: 0.2092\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 0.2167 - val_loss: 0.1943\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1578 - val_loss: 0.1697\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 0.1406 - val_loss: 0.1446\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1737 - val_loss: 0.1412\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1394 - val_loss: 0.1222\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1157 - val_loss: 0.1100\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0918 - val_loss: 0.1059\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0859 - val_loss: 0.0921\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0610 - val_loss: 0.0822\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0780 - val_loss: 0.0720\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0692 - val_loss: 0.0679\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0589 - val_loss: 0.0609\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0950 - val_loss: 0.0537\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0935 - val_loss: 0.0415\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0841 - val_loss: 0.0351\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0732 - val_loss: 0.0367\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0618 - val_loss: 0.0373\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb389f3a040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2746 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 1.3582 - val_loss: 0.1827\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.9313 - val_loss: 0.1552\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.7459 - val_loss: 0.1286\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.6248 - val_loss: 0.1121\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.4811 - val_loss: 0.1059\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.4313 - val_loss: 0.0964\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3745 - val_loss: 0.0888\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.3564 - val_loss: 0.0816\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2920 - val_loss: 0.0736\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2919 - val_loss: 0.0742\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2721 - val_loss: 0.0711\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2532 - val_loss: 0.0719\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.2598 - val_loss: 0.0703\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.2324 - val_loss: 0.0658\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2304 - val_loss: 0.0643\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.2177 - val_loss: 0.0673\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 0.2350 - val_loss: 0.0654\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb38a62d5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2747 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.6284 - val_loss: 0.1639\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2378 - val_loss: 0.1525\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1246 - val_loss: 0.1436\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1082 - val_loss: 0.1379\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0970 - val_loss: 0.1328\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0802 - val_loss: 0.1248\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0800 - val_loss: 0.1180\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0669 - val_loss: 0.1103\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0502 - val_loss: 0.1043\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0468 - val_loss: 0.0988\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0418 - val_loss: 0.0943\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0356 - val_loss: 0.0880\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0323 - val_loss: 0.0847\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0541 - val_loss: 0.0798\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0378 - val_loss: 0.0766\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0381 - val_loss: 0.0739\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0255 - val_loss: 0.0691\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0310 - val_loss: 0.0660\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0458 - val_loss: 0.0626\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.0282 - val_loss: 0.0618\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0332 - val_loss: 0.0592\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0219 - val_loss: 0.0570\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0269 - val_loss: 0.0558\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0204 - val_loss: 0.0555\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0229 - val_loss: 0.0538\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0192 - val_loss: 0.0522\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0161 - val_loss: 0.0530\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0286 - val_loss: 0.0517\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0231 - val_loss: 0.0535\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0230 - val_loss: 0.0544\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb38bdf7ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2748 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.2742 - val_loss: 0.1396\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1635 - val_loss: 0.1269\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1207 - val_loss: 0.1168\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 0.1048 - val_loss: 0.1090\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1082 - val_loss: 0.1025\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1144 - val_loss: 0.0968\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0797 - val_loss: 0.0918\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0928 - val_loss: 0.0883\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0786 - val_loss: 0.0835\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0725 - val_loss: 0.0776\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0634 - val_loss: 0.0710\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0613 - val_loss: 0.0643\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0592 - val_loss: 0.0577\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0647 - val_loss: 0.0511\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0849 - val_loss: 0.0443\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0622 - val_loss: 0.0383\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0460 - val_loss: 0.0319\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0551 - val_loss: 0.0281\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.0552 - val_loss: 0.0237\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.029 - 0s 10ms/step - loss: 0.0425 - val_loss: 0.0200\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0374 - val_loss: 0.0165\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0398 - val_loss: 0.0132\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0464 - val_loss: 0.0112\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0598 - val_loss: 0.0089\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0320 - val_loss: 0.0071\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0406 - val_loss: 0.0062\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0301 - val_loss: 0.0051\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0276 - val_loss: 0.0048\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0256 - val_loss: 0.0047\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0335 - val_loss: 0.0048\n",
      "Epoch 31/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0322 - val_loss: 0.0052\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb38d8f2820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2749 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 0.6631 - val_loss: 0.0137\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.4543 - val_loss: 0.0125\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.3558 - val_loss: 0.0116\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.3075 - val_loss: 0.0105\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2138 - val_loss: 0.0095\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1703 - val_loss: 0.0088\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1970 - val_loss: 0.0081\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1670 - val_loss: 0.0075\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1270 - val_loss: 0.0072\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1184 - val_loss: 0.0069\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0946 - val_loss: 0.0066\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.1015 - val_loss: 0.0063\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0833 - val_loss: 0.0060\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0922 - val_loss: 0.0058\n",
      "Epoch 15/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0706 - val_loss: 0.0056\n",
      "Epoch 16/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0749 - val_loss: 0.0054\n",
      "Epoch 17/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1043 - val_loss: 0.0052\n",
      "Epoch 18/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0655 - val_loss: 0.0051\n",
      "Epoch 19/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0894 - val_loss: 0.0050\n",
      "Epoch 20/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0761 - val_loss: 0.0048\n",
      "Epoch 21/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0514 - val_loss: 0.0048\n",
      "Epoch 22/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0550 - val_loss: 0.0048\n",
      "Epoch 23/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0447 - val_loss: 0.0047\n",
      "Epoch 24/60\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.0535 - val_loss: 0.0047\n",
      "Epoch 25/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0446 - val_loss: 0.0047\n",
      "Epoch 26/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0352 - val_loss: 0.0047\n",
      "Epoch 27/60\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0347 - val_loss: 0.0046\n",
      "Epoch 28/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0397 - val_loss: 0.0046\n",
      "Epoch 29/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.0375 - val_loss: 0.0047\n",
      "Epoch 30/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0314 - val_loss: 0.0047\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb38fd2c790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/60\n",
      "WARNING:tensorflow:Layer layer_normalization_2750 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.3012 - val_loss: 0.0255\n",
      "Epoch 2/60\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.2457 - val_loss: 0.0223\n",
      "Epoch 3/60\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.1700 - val_loss: 0.0204\n",
      "Epoch 4/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1840 - val_loss: 0.0189\n",
      "Epoch 5/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1273 - val_loss: 0.0168\n",
      "Epoch 6/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1540 - val_loss: 0.0148\n",
      "Epoch 7/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.1147 - val_loss: 0.0138\n",
      "Epoch 8/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0819 - val_loss: 0.0124\n",
      "Epoch 9/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0894 - val_loss: 0.0112\n",
      "Epoch 10/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0870 - val_loss: 0.0103\n",
      "Epoch 11/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0734 - val_loss: 0.0097\n",
      "Epoch 12/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0593 - val_loss: 0.0088\n",
      "Epoch 13/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0591 - val_loss: 0.0075\n",
      "Epoch 14/60\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.0891 - val_loss: 0.0083\n",
      "Epoch 15/60\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0706"
     ]
    }
   ],
   "source": [
    "loss = []\n",
    "preds = []\n",
    "seed(1)\n",
    "for retrain_idx in range(552):\n",
    "    X = Xl.iloc[t_train_start[retrain_idx]:t_train_end[retrain_idx],:]\n",
    "    X_idx = X.apply(pd.Series.nunique) != 1\n",
    "    X = X.loc[:,X_idx]\n",
    "    X_val = Xl.loc[t_val_start[retrain_idx]:t_val_end[retrain_idx],X_idx]\n",
    "    X_test = Xl.loc[t_test_start[retrain_idx]:t_test_end[retrain_idx],X_idx]\n",
    "    y = y_agg.iloc[t_train_start[retrain_idx]:t_train_end[retrain_idx],:]\n",
    "    y_val = y_agg.loc[t_val_start[retrain_idx]:t_val_end[retrain_idx],:]\n",
    "    y_test = y_agg.loc[t_test_start[retrain_idx]:t_test_end[retrain_idx],:]\n",
    "    model = Sequential()\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(64,activation='tanh'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(32,activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(32,activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(16,activation='relu'))\n",
    "    model.add(Dense(16,activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    optimizer = SGD(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer,loss='mse')\n",
    "    early_stop = EarlyStopping(monitor='val_loss', verbose=0, patience=2)\n",
    "    history = model.fit(x=X,y=y,validation_data=(X_val,y_val), batch_size=64, epochs=60,verbose=1,callbacks=[early_stop])\n",
    "    preds.append(model.predict(X_test))\n",
    "    loss.append(min(history.history['val_loss']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_org = y_agg.loc[t_test_start[0]:t_test_end[551],:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   -1.89024\n",
       "dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-np.sum((np.squeeze(y_org)-np.squeeze(preds))**2)/np.sum((y_org-np.mean(y_org))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A good R^2 is expected to be between 0 to 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Finance.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
