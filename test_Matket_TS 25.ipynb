{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "NcDhtxLCV8Tw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, LayerNormalization, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from numpy.random import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "vI12F-E9V8T1",
    "outputId": "869138a1-ef3d-4c37-86c8-e408129de90a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "Xl = pd.read_csv('Xl.csv',header=None)\n",
    "Xs = pd.read_csv('Xs.csv',header=None)\n",
    "R = pd.read_csv('y.csv',header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_agg=R.iloc[::-1].rolling(window=12).sum().iloc[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "Psf8guRAV8T1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_train_start = list(range(46*12))\n",
    "t_train_end =[x+120 for x in t_train_start]\n",
    "t_val_start= [x for x in t_train_end]\n",
    "t_val_end = [x+60 for x in t_val_start]\n",
    "t_test_start = [x for x in t_val_end]\n",
    "t_test_end = [x for x in t_test_start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(552,)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(t_test_end) #model needed to be retrained every month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "731"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_test_end[551]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2764 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1248 - val_loss: 0.0474\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0441 - val_loss: 0.0315\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0399 - val_loss: 0.0328\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0381 - val_loss: 0.0333\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a6335700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2765 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.0576 - val_loss: 0.0339\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0270 - val_loss: 0.0332\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0199 - val_loss: 0.0319\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0191 - val_loss: 0.0319\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0127 - val_loss: 0.0333\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0107 - val_loss: 0.0341\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a5e6a700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2766 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.2212 - val_loss: 0.0497\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0417 - val_loss: 0.0392\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0196 - val_loss: 0.0318\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0133 - val_loss: 0.0288\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0121 - val_loss: 0.0278\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0111 - val_loss: 0.0281\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0108 - val_loss: 0.0290\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a57aef70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2767 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 0.2584 - val_loss: 0.0315\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0691 - val_loss: 0.0329\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0403 - val_loss: 0.0367\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a521b550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2768 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.0292 - val_loss: 0.0341\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0190 - val_loss: 0.0337\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0172 - val_loss: 0.0337\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0110 - val_loss: 0.0331\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0109 - val_loss: 0.0307\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0090 - val_loss: 0.0281\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0091 - val_loss: 0.0296\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0087 - val_loss: 0.0310\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c4e35280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2769 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 0.0808 - val_loss: 0.0562\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0368 - val_loss: 0.0606\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0293 - val_loss: 0.0603\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb39d720f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2770 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.3407 - val_loss: 0.0859\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0991 - val_loss: 0.0885\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0342 - val_loss: 0.0869\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c737c550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2771 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.2182 - val_loss: 0.1399\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0780 - val_loss: 0.1139\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0478 - val_loss: 0.0863\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0280 - val_loss: 0.0725\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0248 - val_loss: 0.0649\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0203 - val_loss: 0.0590\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0168 - val_loss: 0.0532\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0123 - val_loss: 0.0480\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0107 - val_loss: 0.0441\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0096 - val_loss: 0.0410\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0086 - val_loss: 0.0385\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0085 - val_loss: 0.0365\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0094 - val_loss: 0.0350\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0081 - val_loss: 0.0336\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0077 - val_loss: 0.0326\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0072 - val_loss: 0.0319\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0066 - val_loss: 0.0314\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0064 - val_loss: 0.0312\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0059 - val_loss: 0.0312\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0054 - val_loss: 0.0310\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0048 - val_loss: 0.0311\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0048 - val_loss: 0.0315\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c651e5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2772 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.0481 - val_loss: 0.0413\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0288 - val_loss: 0.0388\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0283 - val_loss: 0.0335\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0167 - val_loss: 0.0300\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0138 - val_loss: 0.0305\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0137 - val_loss: 0.0313\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ad12c8b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2773 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.0972 - val_loss: 0.0415\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0422 - val_loss: 0.0333\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0327 - val_loss: 0.0309\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0243 - val_loss: 0.0321\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0223 - val_loss: 0.0338\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb43b2f1790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2774 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.3251 - val_loss: 0.0628\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0619 - val_loss: 0.0405\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0215 - val_loss: 0.0337\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0168 - val_loss: 0.0330\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0184 - val_loss: 0.0330\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0200 - val_loss: 0.0333\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0184 - val_loss: 0.0335\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb42bc94790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2775 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.1159 - val_loss: 0.0460\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0363 - val_loss: 0.0431\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0231 - val_loss: 0.0398\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0171 - val_loss: 0.0374\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0167 - val_loss: 0.0364\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0150 - val_loss: 0.0357\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0140 - val_loss: 0.0353\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0141 - val_loss: 0.0355\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0133 - val_loss: 0.0352\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0122 - val_loss: 0.0346\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0114 - val_loss: 0.0338\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0101 - val_loss: 0.0333\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0095 - val_loss: 0.0330\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0095 - val_loss: 0.0329\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0086 - val_loss: 0.0329\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0076 - val_loss: 0.0328\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0080 - val_loss: 0.0327\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0071 - val_loss: 0.0327\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0072 - val_loss: 0.0326\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0067 - val_loss: 0.0326\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0063 - val_loss: 0.0325\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0058 - val_loss: 0.0324\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0056 - val_loss: 0.0323\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0052 - val_loss: 0.0321\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0049 - val_loss: 0.0320\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0048 - val_loss: 0.0319\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0048 - val_loss: 0.0318\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0045 - val_loss: 0.0317\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0041 - val_loss: 0.0316\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0044 - val_loss: 0.0314\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0042 - val_loss: 0.0313\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0042 - val_loss: 0.0313\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0038 - val_loss: 0.0315\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cb382040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2776 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 0.4842 - val_loss: 0.1880\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2104 - val_loss: 0.1448\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0812 - val_loss: 0.1135\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0330 - val_loss: 0.0897\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0266 - val_loss: 0.0717\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0201 - val_loss: 0.0577\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0186 - val_loss: 0.0472\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0153 - val_loss: 0.0398\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0131 - val_loss: 0.0350\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0134 - val_loss: 0.0324\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0129 - val_loss: 0.0314\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0127 - val_loss: 0.0312\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0120 - val_loss: 0.0317\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0118 - val_loss: 0.0328\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3b22843a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2777 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 0.0606 - val_loss: 0.0351\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0336 - val_loss: 0.0360\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0268 - val_loss: 0.0400\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ad00e820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2778 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.0485 - val_loss: 0.0372\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0190 - val_loss: 0.0377\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0155 - val_loss: 0.0380\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a98649d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2779 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.1433 - val_loss: 0.0385\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0587 - val_loss: 0.0375\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0403 - val_loss: 0.0368\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0213 - val_loss: 0.0367\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0171 - val_loss: 0.0368\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0149 - val_loss: 0.0368\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a88dc700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2780 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0502 - val_loss: 0.0396\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0368 - val_loss: 0.0391\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0223 - val_loss: 0.0394\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0152 - val_loss: 0.0386\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0156 - val_loss: 0.0379\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0134 - val_loss: 0.0381\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0106 - val_loss: 0.0386\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c61c90d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2781 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.1589 - val_loss: 0.0343\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0668 - val_loss: 0.0334\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0513 - val_loss: 0.0327\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0298 - val_loss: 0.0322\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0205 - val_loss: 0.0319\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0183 - val_loss: 0.0320\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0124 - val_loss: 0.0321\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c5e89b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2782 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.0670 - val_loss: 0.0306\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0172 - val_loss: 0.0299\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0162 - val_loss: 0.0297\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0144 - val_loss: 0.0297\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0126 - val_loss: 0.0295\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0106 - val_loss: 0.0299\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0091 - val_loss: 0.0305\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c5340f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2783 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.8679 - val_loss: 0.2310\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2520 - val_loss: 0.2079\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1391 - val_loss: 0.1916\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0933 - val_loss: 0.1771\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0562 - val_loss: 0.1583\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0397 - val_loss: 0.1438\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0336 - val_loss: 0.1309\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0253 - val_loss: 0.1196\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0211 - val_loss: 0.1085\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0195 - val_loss: 0.0982\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0184 - val_loss: 0.0887\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0177 - val_loss: 0.0803\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0173 - val_loss: 0.0735\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0166 - val_loss: 0.0680\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0162 - val_loss: 0.0635\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0154 - val_loss: 0.0596\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0141 - val_loss: 0.0562\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0133 - val_loss: 0.0533\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0123 - val_loss: 0.0508\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0117 - val_loss: 0.0487\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0109 - val_loss: 0.0467\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0105 - val_loss: 0.0449\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0100 - val_loss: 0.0432\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0100 - val_loss: 0.0417\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0099 - val_loss: 0.0403\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0098 - val_loss: 0.0390\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0098 - val_loss: 0.0379\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0096 - val_loss: 0.0369\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0094 - val_loss: 0.0361\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0094 - val_loss: 0.0353\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0092 - val_loss: 0.0345\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0090 - val_loss: 0.0338\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0089 - val_loss: 0.0332\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0087 - val_loss: 0.0326\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0085 - val_loss: 0.0321\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.008 - 0s 30ms/step - loss: 0.0085 - val_loss: 0.0317\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0083 - val_loss: 0.0315\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0081 - val_loss: 0.0316\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0080 - val_loss: 0.0318\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e05595e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2784 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.0605 - val_loss: 0.0435\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0588 - val_loss: 0.0455\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0625 - val_loss: 0.0393\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0908 - val_loss: 0.0349\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0247 - val_loss: 0.0341\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0199 - val_loss: 0.0343\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0187 - val_loss: 0.0354\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e04d9550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2785 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.0633 - val_loss: 0.0359\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0310 - val_loss: 0.0340\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0374 - val_loss: 0.0336\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0229 - val_loss: 0.0335\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0143 - val_loss: 0.0333\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0124 - val_loss: 0.0330\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0114 - val_loss: 0.0329\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0104 - val_loss: 0.0327\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0098 - val_loss: 0.0323\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0095 - val_loss: 0.0321\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0089 - val_loss: 0.0320\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0084 - val_loss: 0.0320\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0078 - val_loss: 0.0322\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3dfe7b1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2786 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.0515 - val_loss: 0.0363\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0193 - val_loss: 0.0348\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0169 - val_loss: 0.0350\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0150 - val_loss: 0.0341\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0138 - val_loss: 0.0337\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0115 - val_loss: 0.0335\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0087 - val_loss: 0.0337\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0077 - val_loss: 0.0343\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d350f040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2787 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.2739 - val_loss: 0.0489\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0363 - val_loss: 0.0487\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0248 - val_loss: 0.0461\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0193 - val_loss: 0.0440\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0197 - val_loss: 0.0410\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0125 - val_loss: 0.0390\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0116 - val_loss: 0.0384\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0112 - val_loss: 0.0379\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0114 - val_loss: 0.0375\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0117 - val_loss: 0.0372\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0112 - val_loss: 0.0369\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0098 - val_loss: 0.0367\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0085 - val_loss: 0.0365\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0082 - val_loss: 0.0364\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0084 - val_loss: 0.0365\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0082 - val_loss: 0.0370\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cd7be1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2788 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.6461 - val_loss: 0.1868\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2462 - val_loss: 0.1354\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1036 - val_loss: 0.0912\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0709 - val_loss: 0.0680\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0603 - val_loss: 0.0609\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0386 - val_loss: 0.0556\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0307 - val_loss: 0.0529\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0260 - val_loss: 0.0522\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0236 - val_loss: 0.0516\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0217 - val_loss: 0.0508\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0200 - val_loss: 0.0499\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0182 - val_loss: 0.0490\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0169 - val_loss: 0.0481\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0162 - val_loss: 0.0473\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0158 - val_loss: 0.0465\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0155 - val_loss: 0.0455\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0153 - val_loss: 0.0447\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0151 - val_loss: 0.0440\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0149 - val_loss: 0.0433\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0147 - val_loss: 0.0427\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0144 - val_loss: 0.0422\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0141 - val_loss: 0.0417\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0139 - val_loss: 0.0413\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0136 - val_loss: 0.0410\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0133 - val_loss: 0.0407\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0131 - val_loss: 0.0404\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0129 - val_loss: 0.0402\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0126 - val_loss: 0.0400\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0124 - val_loss: 0.0398\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0122 - val_loss: 0.0397\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0120 - val_loss: 0.0396\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0118 - val_loss: 0.0395\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0116 - val_loss: 0.0394\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0114 - val_loss: 0.0394\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0112 - val_loss: 0.0393\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0110 - val_loss: 0.0392\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0108 - val_loss: 0.0392\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0106 - val_loss: 0.0391\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0104 - val_loss: 0.0391\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0102 - val_loss: 0.0390\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0100 - val_loss: 0.0390\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0098 - val_loss: 0.0389\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0097 - val_loss: 0.0389\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0095 - val_loss: 0.0388\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0093 - val_loss: 0.0387\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0091 - val_loss: 0.0386\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0089 - val_loss: 0.0384\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0087 - val_loss: 0.0382\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0085 - val_loss: 0.0380\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0083 - val_loss: 0.0378\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cc0a5d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2789 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.5411 - val_loss: 0.3501\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.1310 - val_loss: 0.2720\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0687 - val_loss: 0.2252\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0492 - val_loss: 0.1903\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0280 - val_loss: 0.1595\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0207 - val_loss: 0.1373\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0172 - val_loss: 0.1200\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0152 - val_loss: 0.1053\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0130 - val_loss: 0.0934\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0120 - val_loss: 0.0837\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0113 - val_loss: 0.0756\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0107 - val_loss: 0.0694\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0101 - val_loss: 0.0648\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0093 - val_loss: 0.0608\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0089 - val_loss: 0.0573\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0089 - val_loss: 0.0544\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0088 - val_loss: 0.0519\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0086 - val_loss: 0.0499\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0084 - val_loss: 0.0483\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0082 - val_loss: 0.0470\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0078 - val_loss: 0.0461\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0075 - val_loss: 0.0454\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0074 - val_loss: 0.0449\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0074 - val_loss: 0.0445\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0072 - val_loss: 0.0442\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0069 - val_loss: 0.0439\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0068 - val_loss: 0.0435\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0067 - val_loss: 0.0430\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0065 - val_loss: 0.0425\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0063 - val_loss: 0.0422\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0062 - val_loss: 0.0420\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0059 - val_loss: 0.0419\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0057 - val_loss: 0.0418\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0055 - val_loss: 0.0416\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0052 - val_loss: 0.0415\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0050 - val_loss: 0.0415\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0050 - val_loss: 0.0417\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0046 - val_loss: 0.0418\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cb832430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2790 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.3033 - val_loss: 0.0384\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0901 - val_loss: 0.0363\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0308 - val_loss: 0.0351\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0230 - val_loss: 0.0354\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0214 - val_loss: 0.0354\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cad0a9d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2791 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 1.1462 - val_loss: 0.0479\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.1949 - val_loss: 0.0442\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0974 - val_loss: 0.0436\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0622 - val_loss: 0.0426\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0453 - val_loss: 0.0415\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0346 - val_loss: 0.0405\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0304 - val_loss: 0.0400\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0207 - val_loss: 0.0393\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0169 - val_loss: 0.0387\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0141 - val_loss: 0.0384\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0118 - val_loss: 0.0385\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0102 - val_loss: 0.0388\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e619b940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2792 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.0610 - val_loss: 0.0349\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0346 - val_loss: 0.0342\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0295 - val_loss: 0.0346\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0243 - val_loss: 0.0347\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3f7af49d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2793 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.0283 - val_loss: 0.0366\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0185 - val_loss: 0.0362\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0157 - val_loss: 0.0360\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0145 - val_loss: 0.0360\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0142 - val_loss: 0.0360\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4279b7b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2794 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.4542 - val_loss: 0.0921\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.1652 - val_loss: 0.0809\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0462 - val_loss: 0.0676\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0202 - val_loss: 0.0608\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0132 - val_loss: 0.0541\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0128 - val_loss: 0.0509\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0137 - val_loss: 0.0475\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0138 - val_loss: 0.0456\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0147 - val_loss: 0.0448\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0144 - val_loss: 0.0447\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0138 - val_loss: 0.0449\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0121 - val_loss: 0.0447\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ef0a2ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2795 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.0765 - val_loss: 0.0410\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0497 - val_loss: 0.0421\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0210 - val_loss: 0.0423\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb404192b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2796 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.0620 - val_loss: 0.0354\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0486 - val_loss: 0.0359\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0215 - val_loss: 0.0373\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb45af79f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2797 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 418ms/step - loss: 0.3115 - val_loss: 0.0593\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2015 - val_loss: 0.0436\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1293 - val_loss: 0.0400\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0730 - val_loss: 0.0387\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0358 - val_loss: 0.0379\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0255 - val_loss: 0.0368\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0212 - val_loss: 0.0362\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0177 - val_loss: 0.0355\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0153 - val_loss: 0.0346\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0133 - val_loss: 0.0339\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0121 - val_loss: 0.0335\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0115 - val_loss: 0.0332\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0108 - val_loss: 0.0330\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0101 - val_loss: 0.0329\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0097 - val_loss: 0.0329\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0093 - val_loss: 0.0329\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0091 - val_loss: 0.0329\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0088 - val_loss: 0.0328\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0085 - val_loss: 0.0328\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0083 - val_loss: 0.0329\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb418c11670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2798 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0419 - val_loss: 0.0455\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0225 - val_loss: 0.0391\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0113 - val_loss: 0.0344\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0109 - val_loss: 0.0328\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0090 - val_loss: 0.0320\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0081 - val_loss: 0.0320\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0079 - val_loss: 0.0325\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0077 - val_loss: 0.0328\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb43b057af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2799 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.1460 - val_loss: 0.0709\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0606 - val_loss: 0.0587\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0230 - val_loss: 0.0497\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0163 - val_loss: 0.0442\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0143 - val_loss: 0.0403\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0130 - val_loss: 0.0376\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0118 - val_loss: 0.0355\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0104 - val_loss: 0.0351\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0091 - val_loss: 0.0365\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0084 - val_loss: 0.0389\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb42f0ac5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2800 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.2280 - val_loss: 0.0922\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1520 - val_loss: 0.0742\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0766 - val_loss: 0.0619\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0396 - val_loss: 0.0545\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0254 - val_loss: 0.0501\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0182 - val_loss: 0.0481\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0144 - val_loss: 0.0465\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0135 - val_loss: 0.0450\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0135 - val_loss: 0.0444\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0125 - val_loss: 0.0423\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0102 - val_loss: 0.0396\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0085 - val_loss: 0.0372\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0086 - val_loss: 0.0356\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0089 - val_loss: 0.0348\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0091 - val_loss: 0.0342\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0091 - val_loss: 0.0336\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0088 - val_loss: 0.0330\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0083 - val_loss: 0.0324\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0078 - val_loss: 0.0319\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0073 - val_loss: 0.0316\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0069 - val_loss: 0.0314\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0066 - val_loss: 0.0312\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0063 - val_loss: 0.0310\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0061 - val_loss: 0.0308\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0057 - val_loss: 0.0306\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0053 - val_loss: 0.0305\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0050 - val_loss: 0.0305\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0050 - val_loss: 0.0303\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0044 - val_loss: 0.0301\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0042 - val_loss: 0.0299\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0041 - val_loss: 0.0297\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0040 - val_loss: 0.0295\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0039 - val_loss: 0.0293\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0038 - val_loss: 0.0292\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0038 - val_loss: 0.0291\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0035 - val_loss: 0.0290\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0034 - val_loss: 0.0290\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0034 - val_loss: 0.0290\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0032 - val_loss: 0.0291\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4464f0940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2801 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.5322 - val_loss: 0.0808\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1538 - val_loss: 0.0595\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0267 - val_loss: 0.0462\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0243 - val_loss: 0.0397\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0288 - val_loss: 0.0379\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0242 - val_loss: 0.0394\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0207 - val_loss: 0.0405\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4760d8f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2802 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.0210 - val_loss: 0.0333\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0105 - val_loss: 0.0333\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0096 - val_loss: 0.0329\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0064 - val_loss: 0.0326\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0061 - val_loss: 0.0324\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0050 - val_loss: 0.0325\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0040 - val_loss: 0.0325\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4ce5cc790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2803 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.1046 - val_loss: 0.0370\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0255 - val_loss: 0.0418\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0388 - val_loss: 0.0419\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb43302b160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2804 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.1480 - val_loss: 0.0309\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0579 - val_loss: 0.0322\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0275 - val_loss: 0.0328\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb427770940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2805 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.8887 - val_loss: 0.1228\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4613 - val_loss: 0.0942\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1859 - val_loss: 0.0701\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1218 - val_loss: 0.0482\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0568 - val_loss: 0.0366\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0255 - val_loss: 0.0319\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0183 - val_loss: 0.0306\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0166 - val_loss: 0.0298\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0155 - val_loss: 0.0296\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0149 - val_loss: 0.0297\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0144 - val_loss: 0.0298\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb40ff9caf0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2806 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.2845 - val_loss: 0.0877\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0519 - val_loss: 0.0616\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0226 - val_loss: 0.0506\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0163 - val_loss: 0.0441\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0117 - val_loss: 0.0399\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0119 - val_loss: 0.0368\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0119 - val_loss: 0.0353\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0116 - val_loss: 0.0345\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0105 - val_loss: 0.0342\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0091 - val_loss: 0.0342\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0084 - val_loss: 0.0341\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0076 - val_loss: 0.0341\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0067 - val_loss: 0.0340\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0061 - val_loss: 0.0339\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0057 - val_loss: 0.0338\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0055 - val_loss: 0.0339\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0053 - val_loss: 0.0341\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ffd4a3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2807 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.0891 - val_loss: 0.0557\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0406 - val_loss: 0.0421\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0215 - val_loss: 0.0357\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0130 - val_loss: 0.0349\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0117 - val_loss: 0.0363\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0085 - val_loss: 0.0390\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb400e90940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2808 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 392ms/step - loss: 0.2805 - val_loss: 0.0535\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0236 - val_loss: 0.0454\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0120 - val_loss: 0.0418\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0095 - val_loss: 0.0382\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0081 - val_loss: 0.0356\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0092 - val_loss: 0.0332\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.006 - 0s 33ms/step - loss: 0.0065 - val_loss: 0.0318\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0063 - val_loss: 0.0309\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0052 - val_loss: 0.0303\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0054 - val_loss: 0.0301\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0052 - val_loss: 0.0300\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0048 - val_loss: 0.0303\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0044 - val_loss: 0.0306\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4afa40700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2809 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.0861 - val_loss: 0.1420\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0277 - val_loss: 0.1227\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0401 - val_loss: 0.1171\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0172 - val_loss: 0.1121\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0107 - val_loss: 0.1062\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0082 - val_loss: 0.1000\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0092 - val_loss: 0.0942\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0083 - val_loss: 0.0884\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0077 - val_loss: 0.0823\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0081 - val_loss: 0.0767\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0076 - val_loss: 0.0725\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0068 - val_loss: 0.0693\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0063 - val_loss: 0.0673\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0053 - val_loss: 0.0660\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0049 - val_loss: 0.0655\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0045 - val_loss: 0.0646\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0042 - val_loss: 0.0635\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0040 - val_loss: 0.0628\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0039 - val_loss: 0.0622\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0038 - val_loss: 0.0619\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0036 - val_loss: 0.0624\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0035 - val_loss: 0.0633\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ffab4b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2810 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.3372 - val_loss: 0.0444\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0963 - val_loss: 0.0380\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0433 - val_loss: 0.0360\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0310 - val_loss: 0.0347\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0241 - val_loss: 0.0339\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0220 - val_loss: 0.0334\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0207 - val_loss: 0.0331\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0196 - val_loss: 0.0331\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0185 - val_loss: 0.0331\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0176 - val_loss: 0.0331\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0167 - val_loss: 0.0331\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0161 - val_loss: 0.0329\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0155 - val_loss: 0.0326\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0149 - val_loss: 0.0324\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0144 - val_loss: 0.0323\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0140 - val_loss: 0.0322\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0134 - val_loss: 0.0322\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0130 - val_loss: 0.0322\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0128 - val_loss: 0.0323\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3f5107550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2811 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.5127 - val_loss: 0.2039\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2495 - val_loss: 0.1209\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0604 - val_loss: 0.0763\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0314 - val_loss: 0.0573\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0220 - val_loss: 0.0511\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0213 - val_loss: 0.0464\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0211 - val_loss: 0.0435\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0179 - val_loss: 0.0417\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0147 - val_loss: 0.0406\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0136 - val_loss: 0.0400\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0120 - val_loss: 0.0396\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0109 - val_loss: 0.0397\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0096 - val_loss: 0.0403\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d9e6d8b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2812 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.3692 - val_loss: 0.0826\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.1683 - val_loss: 0.0638\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0701 - val_loss: 0.0485\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0473 - val_loss: 0.0414\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0251 - val_loss: 0.0378\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0174 - val_loss: 0.0365\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0134 - val_loss: 0.0359\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0118 - val_loss: 0.0358\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0104 - val_loss: 0.0360\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0081 - val_loss: 0.0360\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb446655e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2813 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.1448 - val_loss: 0.0503\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.1365 - val_loss: 0.0513\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0334 - val_loss: 0.0548\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4b5cb1b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2814 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.0741 - val_loss: 0.0288\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0586 - val_loss: 0.0323\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0183 - val_loss: 0.0340\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cbcf8790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2815 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.4248 - val_loss: 0.0976\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0486 - val_loss: 0.0953\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0320 - val_loss: 0.0956\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0191 - val_loss: 0.0985\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ccc77940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2816 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.4553 - val_loss: 0.1046\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2275 - val_loss: 0.0715\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0670 - val_loss: 0.0619\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0484 - val_loss: 0.0562\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0340 - val_loss: 0.0566\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0213 - val_loss: 0.0611\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d3766af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2817 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.1323 - val_loss: 0.1892\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0623 - val_loss: 0.1849\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0566 - val_loss: 0.1690\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0208 - val_loss: 0.1458\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0243 - val_loss: 0.1154\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0163 - val_loss: 0.0937\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0094 - val_loss: 0.0773\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0110 - val_loss: 0.0639\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0069 - val_loss: 0.0535\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0066 - val_loss: 0.0459\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0065 - val_loss: 0.0409\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0060 - val_loss: 0.0378\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0055 - val_loss: 0.0358\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0051 - val_loss: 0.0344\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0050 - val_loss: 0.0334\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0048 - val_loss: 0.0329\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0046 - val_loss: 0.0329\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0045 - val_loss: 0.0330\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e23e4f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2818 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.1499 - val_loss: 0.0609\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0510 - val_loss: 0.0530\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0226 - val_loss: 0.0459\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0145 - val_loss: 0.0388\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0102 - val_loss: 0.0334\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0092 - val_loss: 0.0312\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0075 - val_loss: 0.0308\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0058 - val_loss: 0.0316\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0052 - val_loss: 0.0319\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3b996dd30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2819 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.1204 - val_loss: 0.0365\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0511 - val_loss: 0.0384\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0221 - val_loss: 0.0375\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3b9d76e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2820 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.0715 - val_loss: 0.0329\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0153 - val_loss: 0.0325\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0201 - val_loss: 0.0324\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0125 - val_loss: 0.0326\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0104 - val_loss: 0.0330\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3bcb39550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2821 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.0647 - val_loss: 0.0742\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0286 - val_loss: 0.0607\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0256 - val_loss: 0.0504\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0142 - val_loss: 0.0445\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0106 - val_loss: 0.0401\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0087 - val_loss: 0.0371\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0078 - val_loss: 0.0352\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0074 - val_loss: 0.0338\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0072 - val_loss: 0.0330\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0068 - val_loss: 0.0325\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0065 - val_loss: 0.0319\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0062 - val_loss: 0.0315\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0059 - val_loss: 0.0312\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0056 - val_loss: 0.0310\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0054 - val_loss: 0.0308\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0052 - val_loss: 0.0306\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0050 - val_loss: 0.0304\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0046 - val_loss: 0.0301\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0043 - val_loss: 0.0298\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0040 - val_loss: 0.0296\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0037 - val_loss: 0.0294\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0036 - val_loss: 0.0291\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0034 - val_loss: 0.0289\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0033 - val_loss: 0.0288\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0031 - val_loss: 0.0286\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0028 - val_loss: 0.0283\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0026 - val_loss: 0.0282\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0025 - val_loss: 0.0279\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0024 - val_loss: 0.0276\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0023 - val_loss: 0.0274\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0022 - val_loss: 0.0271\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0021 - val_loss: 0.0268\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0020 - val_loss: 0.0264\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0020 - val_loss: 0.0261\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0019 - val_loss: 0.0260\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0019 - val_loss: 0.0260\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0019 - val_loss: 0.0261\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0018 - val_loss: 0.0262\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4ca206af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2822 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.0365 - val_loss: 0.0215\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0157 - val_loss: 0.0230\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0268 - val_loss: 0.0211\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0116 - val_loss: 0.0208\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0102 - val_loss: 0.0232\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0094 - val_loss: 0.0246\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb466baef70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2823 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 1.0721 - val_loss: 0.0217\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4079 - val_loss: 0.0207\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1478 - val_loss: 0.0237\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0791 - val_loss: 0.0299\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c1817ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2824 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.5480 - val_loss: 0.1470\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1368 - val_loss: 0.0926\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1008 - val_loss: 0.0621\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0691 - val_loss: 0.0473\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0496 - val_loss: 0.0395\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0355 - val_loss: 0.0348\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0275 - val_loss: 0.0325\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0212 - val_loss: 0.0318\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0128 - val_loss: 0.0318\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.0093 - val_loss: 0.0318\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0096 - val_loss: 0.0321\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ae4421f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2825 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.1709 - val_loss: 0.1003\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0370 - val_loss: 0.0700\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0237 - val_loss: 0.0470\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0159 - val_loss: 0.0364\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0130 - val_loss: 0.0309\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0112 - val_loss: 0.0258\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0102 - val_loss: 0.0215\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0105 - val_loss: 0.0186\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0094 - val_loss: 0.0172\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0082 - val_loss: 0.0167\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0069 - val_loss: 0.0168\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0065 - val_loss: 0.0170\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c592cc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2826 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.1251 - val_loss: 0.0167\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0284 - val_loss: 0.0181\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0183 - val_loss: 0.0196\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c3a48430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2827 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.0474 - val_loss: 0.0229\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0196 - val_loss: 0.0228\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0146 - val_loss: 0.0222\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0142 - val_loss: 0.0208\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0128 - val_loss: 0.0193\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0125 - val_loss: 0.0182\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0107 - val_loss: 0.0173\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0093 - val_loss: 0.0166\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0076 - val_loss: 0.0161\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0062 - val_loss: 0.0158\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0054 - val_loss: 0.0155\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0049 - val_loss: 0.0153\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0044 - val_loss: 0.0150\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0041 - val_loss: 0.0149\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0038 - val_loss: 0.0151\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.003 - 0s 19ms/step - loss: 0.0036 - val_loss: 0.0154\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c42a6040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2828 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.1392 - val_loss: 0.0594\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0643 - val_loss: 0.0454\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0315 - val_loss: 0.0348\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0185 - val_loss: 0.0299\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0210 - val_loss: 0.0263\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0114 - val_loss: 0.0239\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0136 - val_loss: 0.0223\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0128 - val_loss: 0.0210\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0112 - val_loss: 0.0197\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0094 - val_loss: 0.0188\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0081 - val_loss: 0.0179\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0074 - val_loss: 0.0172\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0068 - val_loss: 0.0168\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0066 - val_loss: 0.0164\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0065 - val_loss: 0.0162\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0063 - val_loss: 0.0160\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0059 - val_loss: 0.0159\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0058 - val_loss: 0.0158\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0058 - val_loss: 0.0158\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0055 - val_loss: 0.0157\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0052 - val_loss: 0.0158\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0050 - val_loss: 0.0158\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c4861ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2829 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.0749 - val_loss: 0.0192\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0234 - val_loss: 0.0244\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0155 - val_loss: 0.0290\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c5569160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2830 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.1266 - val_loss: 0.0348\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0510 - val_loss: 0.0338\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0199 - val_loss: 0.0318\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0202 - val_loss: 0.0283\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0166 - val_loss: 0.0268\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0131 - val_loss: 0.0259\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0111 - val_loss: 0.0252\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0105 - val_loss: 0.0246\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0112 - val_loss: 0.0241\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0088 - val_loss: 0.0235\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0086 - val_loss: 0.0233\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0072 - val_loss: 0.0234\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0058 - val_loss: 0.0238\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb397e9fc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2831 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.1584 - val_loss: 0.0277\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0461 - val_loss: 0.0221\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0204 - val_loss: 0.0195\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0286 - val_loss: 0.0192\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0422 - val_loss: 0.0176\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0144 - val_loss: 0.0175\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0106 - val_loss: 0.0176\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0102 - val_loss: 0.0169\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0095 - val_loss: 0.0162\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0083 - val_loss: 0.0151\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0073 - val_loss: 0.0139\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0067 - val_loss: 0.0130\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0065 - val_loss: 0.0131\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0060 - val_loss: 0.0135\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3991a1c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2832 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.1672 - val_loss: 0.0279\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0901 - val_loss: 0.0372\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0526 - val_loss: 0.0390\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb39a906550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2833 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.1466 - val_loss: 0.0863\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0776 - val_loss: 0.0666\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0242 - val_loss: 0.0516\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0190 - val_loss: 0.0404\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0167 - val_loss: 0.0332\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0152 - val_loss: 0.0277\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0140 - val_loss: 0.0237\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0133 - val_loss: 0.0208\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0125 - val_loss: 0.0199\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0114 - val_loss: 0.0195\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0103 - val_loss: 0.0190\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0092 - val_loss: 0.0183\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0083 - val_loss: 0.0175\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0075 - val_loss: 0.0168\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0072 - val_loss: 0.0161\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0069 - val_loss: 0.0154\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0063 - val_loss: 0.0149\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0057 - val_loss: 0.0144\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0053 - val_loss: 0.0141\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0051 - val_loss: 0.0138\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0050 - val_loss: 0.0136\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0049 - val_loss: 0.0135\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0048 - val_loss: 0.0134\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0047 - val_loss: 0.0134\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0045 - val_loss: 0.0134\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0044 - val_loss: 0.0134\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0044 - val_loss: 0.0134\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a11f6550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2834 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 429ms/step - loss: 0.1193 - val_loss: 0.0728\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0408 - val_loss: 0.0695\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0146 - val_loss: 0.0685\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0100 - val_loss: 0.0613\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0100 - val_loss: 0.0538\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0098 - val_loss: 0.0464\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0086 - val_loss: 0.0396\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0079 - val_loss: 0.0333\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0068 - val_loss: 0.0278\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0062 - val_loss: 0.0232\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0054 - val_loss: 0.0194\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0048 - val_loss: 0.0164\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0046 - val_loss: 0.0146\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0045 - val_loss: 0.0143\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0044 - val_loss: 0.0144\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0042 - val_loss: 0.0145\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb42d72c4c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2835 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.0338 - val_loss: 0.0164\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0305 - val_loss: 0.0160\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0198 - val_loss: 0.0160\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0133 - val_loss: 0.0163\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0103 - val_loss: 0.0167\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4516b2280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2836 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.0501 - val_loss: 0.0155\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0380 - val_loss: 0.0183\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0176 - val_loss: 0.0213\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb430bc7670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2837 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.7394 - val_loss: 0.0500\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.1974 - val_loss: 0.0333\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0655 - val_loss: 0.0226\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0353 - val_loss: 0.0171\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0198 - val_loss: 0.0145\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0172 - val_loss: 0.0131\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0125 - val_loss: 0.0122\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0153 - val_loss: 0.0117\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0170 - val_loss: 0.0115\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0177 - val_loss: 0.0115\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0175 - val_loss: 0.0114\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0171 - val_loss: 0.0113\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0158 - val_loss: 0.0112\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0140 - val_loss: 0.0113\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0122 - val_loss: 0.0112\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0106 - val_loss: 0.0110\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0099 - val_loss: 0.0108\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0093 - val_loss: 0.0107\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0087 - val_loss: 0.0106\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0082 - val_loss: 0.0105\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0080 - val_loss: 0.0104\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0076 - val_loss: 0.0103\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0073 - val_loss: 0.0101\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0070 - val_loss: 0.0101\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0068 - val_loss: 0.0100\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0066 - val_loss: 0.0099\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0065 - val_loss: 0.0097\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0063 - val_loss: 0.0096\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0061 - val_loss: 0.0094\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0058 - val_loss: 0.0092\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0055 - val_loss: 0.0090\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0052 - val_loss: 0.0090\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0050 - val_loss: 0.0091\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb44e4a79d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2838 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 0.0634 - val_loss: 0.0203\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0303 - val_loss: 0.0188\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0152 - val_loss: 0.0161\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0132 - val_loss: 0.0136\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0113 - val_loss: 0.0124\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0102 - val_loss: 0.0122\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0090 - val_loss: 0.0122\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0082 - val_loss: 0.0123\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0066 - val_loss: 0.0124\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a3324c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2839 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.0619 - val_loss: 0.0146\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0457 - val_loss: 0.0147\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0239 - val_loss: 0.0165\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3795c7310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2840 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.0360 - val_loss: 0.0171\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0465 - val_loss: 0.0211\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0251 - val_loss: 0.0247\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3799d39d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2841 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 428ms/step - loss: 0.0581 - val_loss: 0.0356\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0329 - val_loss: 0.0396\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0254 - val_loss: 0.0382\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb379ddec10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2842 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:5 out of the last 19 calls to <function Model.make_train_function.<locals>.train_function at 0x7fb379e87e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1580WARNING:tensorflow:5 out of the last 19 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb37b18c280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.1580 - val_loss: 0.0290\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0366 - val_loss: 0.0226\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0198 - val_loss: 0.0199\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0132 - val_loss: 0.0190\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0115 - val_loss: 0.0182\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0091 - val_loss: 0.0174\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0079 - val_loss: 0.0168\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0077 - val_loss: 0.0163\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0071 - val_loss: 0.0158\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0066 - val_loss: 0.0154\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0064 - val_loss: 0.0152\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0061 - val_loss: 0.0150\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0058 - val_loss: 0.0149\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0051 - val_loss: 0.0147\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0047 - val_loss: 0.0146\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0045 - val_loss: 0.0147\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0041 - val_loss: 0.0147\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb37b28b160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2843 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0524 - val_loss: 0.0338\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0259 - val_loss: 0.0383\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0171 - val_loss: 0.0391\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb37b5d6790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2844 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 358ms/step - loss: 0.4954 - val_loss: 0.0125\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1918 - val_loss: 0.0154\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0956 - val_loss: 0.0197\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb37ba6d160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2845 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.0360 - val_loss: 0.0135\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0171 - val_loss: 0.0134\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0196 - val_loss: 0.0140\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0126 - val_loss: 0.0163\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb37bd7fdc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2846 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.1196 - val_loss: 0.0236\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0469 - val_loss: 0.0184\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0337 - val_loss: 0.0148\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0195 - val_loss: 0.0133\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0143 - val_loss: 0.0131\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0122 - val_loss: 0.0130\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0108 - val_loss: 0.0126\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0084 - val_loss: 0.0122\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0075 - val_loss: 0.0122\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0073 - val_loss: 0.0121\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0070 - val_loss: 0.0121\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0063 - val_loss: 0.0121\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0056 - val_loss: 0.0121\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0053 - val_loss: 0.0121\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0050 - val_loss: 0.0121\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0048 - val_loss: 0.0121\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0046 - val_loss: 0.0121\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb37d172c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2847 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.1640 - val_loss: 0.0473\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0410 - val_loss: 0.0536\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0303 - val_loss: 0.0540\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb37d54f670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2848 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 407ms/step - loss: 0.1395 - val_loss: 0.0167\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0489 - val_loss: 0.0202\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0226 - val_loss: 0.0189\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb37f026550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2849 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.2342 - val_loss: 0.0126\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1438 - val_loss: 0.0135\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0469 - val_loss: 0.0178\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb381274040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2850 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:5 out of the last 28 calls to <function Model.make_train_function.<locals>.train_function at 0x7fb37f8d60d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1415WARNING:tensorflow:5 out of the last 28 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb38207c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 394ms/step - loss: 0.1415 - val_loss: 0.0119\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0532 - val_loss: 0.0154\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0321 - val_loss: 0.0185\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb38207cc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2851 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7fb38207caf0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0661WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb382c4c820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.0661 - val_loss: 0.0373\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0565 - val_loss: 0.0302\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0324 - val_loss: 0.0278\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0212 - val_loss: 0.0252\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0131 - val_loss: 0.0229\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0094 - val_loss: 0.0215\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0099 - val_loss: 0.0211\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0081 - val_loss: 0.0218\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0081 - val_loss: 0.0219\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb383159af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2852 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 758ms/step - loss: 0.1697 - val_loss: 0.0205\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0415 - val_loss: 0.0147\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0238 - val_loss: 0.0153\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0187 - val_loss: 0.0166\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb384385d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2853 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 709ms/step - loss: 0.1929 - val_loss: 0.0284\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0670 - val_loss: 0.0263\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0482 - val_loss: 0.0247\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0448 - val_loss: 0.0237\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0352 - val_loss: 0.0229\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0309 - val_loss: 0.0224\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0276 - val_loss: 0.0217\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0255 - val_loss: 0.0210\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0248 - val_loss: 0.0205\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0229 - val_loss: 0.0198\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0218 - val_loss: 0.0192\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0211 - val_loss: 0.0188\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0201 - val_loss: 0.0183\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0195 - val_loss: 0.0179\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0190 - val_loss: 0.0175\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.018 - 0s 75ms/step - loss: 0.0185 - val_loss: 0.0173\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0181 - val_loss: 0.0170\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0176 - val_loss: 0.0168\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0172 - val_loss: 0.0166\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.0168 - val_loss: 0.0164\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0163 - val_loss: 0.0163\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0159 - val_loss: 0.0161\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0155 - val_loss: 0.0159\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0151 - val_loss: 0.0158\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0147 - val_loss: 0.0156\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0144 - val_loss: 0.0155\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0141 - val_loss: 0.0154\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0138 - val_loss: 0.0153\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0135 - val_loss: 0.0152\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0132 - val_loss: 0.0151\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0129 - val_loss: 0.0150\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0126 - val_loss: 0.0149\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.012 - 0s 34ms/step - loss: 0.0123 - val_loss: 0.0148\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0121 - val_loss: 0.0147\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0118 - val_loss: 0.0147\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0115 - val_loss: 0.0146\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0112 - val_loss: 0.0145\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0110 - val_loss: 0.0145\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0107 - val_loss: 0.0145\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0105 - val_loss: 0.0144\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0103 - val_loss: 0.0144\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0100 - val_loss: 0.0144\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0098 - val_loss: 0.0144\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0096 - val_loss: 0.0144\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0095 - val_loss: 0.0143\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0093 - val_loss: 0.0143\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0091 - val_loss: 0.0144\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb38680d040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2854 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.0525 - val_loss: 0.0158\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0335 - val_loss: 0.0178\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0199 - val_loss: 0.0199\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3878734c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2855 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0447 - val_loss: 0.0256\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0389 - val_loss: 0.0246\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0297 - val_loss: 0.0234\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0257 - val_loss: 0.0222\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0241 - val_loss: 0.0212\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0229 - val_loss: 0.0204\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0218 - val_loss: 0.0198\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0209 - val_loss: 0.0193\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0196 - val_loss: 0.0190\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 0.0186 - val_loss: 0.0187\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0175 - val_loss: 0.0184\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0164 - val_loss: 0.0181\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0154 - val_loss: 0.0179\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0144 - val_loss: 0.0177\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0133 - val_loss: 0.0173\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0124 - val_loss: 0.0169\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0116 - val_loss: 0.0166\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0109 - val_loss: 0.0164\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0104 - val_loss: 0.0162\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0100 - val_loss: 0.0162\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0097 - val_loss: 0.0162\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0095 - val_loss: 0.0162\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0092 - val_loss: 0.0164\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb36981c790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2856 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.0461 - val_loss: 0.0338\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0678 - val_loss: 0.0284\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0319 - val_loss: 0.0216\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0216 - val_loss: 0.0175\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0170 - val_loss: 0.0163\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0137 - val_loss: 0.0166\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0105 - val_loss: 0.0177\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4176ae1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2857 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 0.0518 - val_loss: 0.0249\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0386 - val_loss: 0.0199\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0243 - val_loss: 0.0176\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0232 - val_loss: 0.0166\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0203 - val_loss: 0.0162\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0180 - val_loss: 0.0161\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0165 - val_loss: 0.0160\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0158 - val_loss: 0.0160\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0154 - val_loss: 0.0159\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0151 - val_loss: 0.0156\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0145 - val_loss: 0.0154\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0138 - val_loss: 0.0155\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0131 - val_loss: 0.0155\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e061ec10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2858 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.5065 - val_loss: 0.1756\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1934 - val_loss: 0.1668\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0608 - val_loss: 0.1717\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0384 - val_loss: 0.1350\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0259 - val_loss: 0.1019\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0217 - val_loss: 0.0783\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0188 - val_loss: 0.0645\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0165 - val_loss: 0.0556\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0149 - val_loss: 0.0478\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0135 - val_loss: 0.0417\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0128 - val_loss: 0.0368\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0122 - val_loss: 0.0328\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0116 - val_loss: 0.0293\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0110 - val_loss: 0.0261\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0104 - val_loss: 0.0233\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0100 - val_loss: 0.0210\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0097 - val_loss: 0.0192\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0093 - val_loss: 0.0180\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0091 - val_loss: 0.0173\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0087 - val_loss: 0.0166\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0081 - val_loss: 0.0162\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0076 - val_loss: 0.0159\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0073 - val_loss: 0.0156\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0070 - val_loss: 0.0153\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0067 - val_loss: 0.0151\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0064 - val_loss: 0.0150\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0061 - val_loss: 0.0150\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0059 - val_loss: 0.0151\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3b9d76a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2859 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.3502 - val_loss: 0.0256\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1375 - val_loss: 0.0219\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0409 - val_loss: 0.0190\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0234 - val_loss: 0.0168\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0189 - val_loss: 0.0155\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0161 - val_loss: 0.0150\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0144 - val_loss: 0.0148\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0138 - val_loss: 0.0147\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0128 - val_loss: 0.0148\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0114 - val_loss: 0.0149\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ec307af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2860 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.0392 - val_loss: 0.0263\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0396 - val_loss: 0.0282\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0289 - val_loss: 0.0298\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cc9ecca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2861 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.0747 - val_loss: 0.0239\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0506 - val_loss: 0.0234\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0293 - val_loss: 0.0225\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0178 - val_loss: 0.0216\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0118 - val_loss: 0.0206\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0087 - val_loss: 0.0194\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0077 - val_loss: 0.0185\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0068 - val_loss: 0.0178\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0054 - val_loss: 0.0172\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0049 - val_loss: 0.0168\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0046 - val_loss: 0.0164\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0043 - val_loss: 0.0160\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0043 - val_loss: 0.0158\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0040 - val_loss: 0.0157\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0039 - val_loss: 0.0156\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0038 - val_loss: 0.0153\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0036 - val_loss: 0.0150\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0033 - val_loss: 0.0149\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0031 - val_loss: 0.0149\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0029 - val_loss: 0.0150\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4fee27b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2862 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.0406 - val_loss: 0.0200\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0196 - val_loss: 0.0195\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0147 - val_loss: 0.0179\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0113 - val_loss: 0.0162\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0095 - val_loss: 0.0157\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0092 - val_loss: 0.0150\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0077 - val_loss: 0.0146\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0071 - val_loss: 0.0146\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0060 - val_loss: 0.0144\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0051 - val_loss: 0.0142\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0048 - val_loss: 0.0143\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0043 - val_loss: 0.0144\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4cab91550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2863 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 0.1013 - val_loss: 0.0359\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0405 - val_loss: 0.0377\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0216 - val_loss: 0.0359\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0189 - val_loss: 0.0319\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0174 - val_loss: 0.0308\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0107 - val_loss: 0.0292\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0102 - val_loss: 0.0252\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0091 - val_loss: 0.0225\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0079 - val_loss: 0.0201\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0070 - val_loss: 0.0195\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0062 - val_loss: 0.0188\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0061 - val_loss: 0.0184\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0060 - val_loss: 0.0184\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0057 - val_loss: 0.0185\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0051 - val_loss: 0.0186\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3943a40d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2864 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 0.4863 - val_loss: 0.0359\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1547 - val_loss: 0.0258\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1155 - val_loss: 0.0243\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0615 - val_loss: 0.0236\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0348 - val_loss: 0.0231\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0240 - val_loss: 0.0226\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0197 - val_loss: 0.0222\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0174 - val_loss: 0.0219\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0162 - val_loss: 0.0217\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0151 - val_loss: 0.0215\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0139 - val_loss: 0.0214\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0127 - val_loss: 0.0213\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0117 - val_loss: 0.0212\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0108 - val_loss: 0.0211\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0094 - val_loss: 0.0210\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0084 - val_loss: 0.0210\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0081 - val_loss: 0.0209\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0080 - val_loss: 0.0209\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0081 - val_loss: 0.0209\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0080 - val_loss: 0.0209\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0077 - val_loss: 0.0208\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0074 - val_loss: 0.0208\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0072 - val_loss: 0.0208\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0070 - val_loss: 0.0208\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0208\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb383bcdb80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2865 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.0507 - val_loss: 0.0279\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0188 - val_loss: 0.0244\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0176 - val_loss: 0.0230\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0135 - val_loss: 0.0223\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0120 - val_loss: 0.0232\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0082 - val_loss: 0.0246\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb37f026d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2866 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.0766 - val_loss: 0.0345\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0465 - val_loss: 0.0365\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0298 - val_loss: 0.0384\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb37bb54c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2867 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.2233 - val_loss: 0.1160\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0731 - val_loss: 0.1074\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0268 - val_loss: 0.1033\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0159 - val_loss: 0.0981\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0134 - val_loss: 0.0903\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0118 - val_loss: 0.0804\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0114 - val_loss: 0.0719\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0112 - val_loss: 0.0662\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0107 - val_loss: 0.0629\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0102 - val_loss: 0.0605\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0101 - val_loss: 0.0579\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0100 - val_loss: 0.0552\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0098 - val_loss: 0.0520\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0094 - val_loss: 0.0487\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0090 - val_loss: 0.0452\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0085 - val_loss: 0.0425\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0080 - val_loss: 0.0404\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0076 - val_loss: 0.0385\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0073 - val_loss: 0.0367\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0071 - val_loss: 0.0352\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0069 - val_loss: 0.0338\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0068 - val_loss: 0.0326\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0066 - val_loss: 0.0316\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0064 - val_loss: 0.0307\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0062 - val_loss: 0.0298\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0060 - val_loss: 0.0290\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0058 - val_loss: 0.0282\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0055 - val_loss: 0.0275\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0052 - val_loss: 0.0269\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0050 - val_loss: 0.0264\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0048 - val_loss: 0.0260\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0046 - val_loss: 0.0256\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0044 - val_loss: 0.0253\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0043 - val_loss: 0.0251\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0042 - val_loss: 0.0249\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0041 - val_loss: 0.0247\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0040 - val_loss: 0.0247\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0039 - val_loss: 0.0246\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0038 - val_loss: 0.0246\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0037 - val_loss: 0.0246\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0036 - val_loss: 0.0245\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0036 - val_loss: 0.0244\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0035 - val_loss: 0.0243\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0034 - val_loss: 0.0242\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0034 - val_loss: 0.0241\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0033 - val_loss: 0.0240\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0033 - val_loss: 0.0239\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0032 - val_loss: 0.0238\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0032 - val_loss: 0.0238\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0031 - val_loss: 0.0238\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb37b4cf700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2868 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.0670 - val_loss: 0.0476\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0228 - val_loss: 0.0400\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0150 - val_loss: 0.0309\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0188 - val_loss: 0.0289\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0104 - val_loss: 0.0277\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0095 - val_loss: 0.0269\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0090 - val_loss: 0.0264\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0083 - val_loss: 0.0258\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0075 - val_loss: 0.0253\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0070 - val_loss: 0.0250\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0062 - val_loss: 0.0246\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0061 - val_loss: 0.0242\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0060 - val_loss: 0.0236\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0057 - val_loss: 0.0231\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0056 - val_loss: 0.0228\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0054 - val_loss: 0.0227\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0052 - val_loss: 0.0226\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0050 - val_loss: 0.0226\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0048 - val_loss: 0.0227\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0047 - val_loss: 0.0227\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb379f41ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2869 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.5859 - val_loss: 0.2563\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1305 - val_loss: 0.1392\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0572 - val_loss: 0.0811\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0336 - val_loss: 0.0530\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0214 - val_loss: 0.0389\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0174 - val_loss: 0.0311\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0153 - val_loss: 0.0264\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0141 - val_loss: 0.0236\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0122 - val_loss: 0.0222\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0116 - val_loss: 0.0216\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0111 - val_loss: 0.0215\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0107 - val_loss: 0.0220\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0104 - val_loss: 0.0229\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3798c88b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2870 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.0851 - val_loss: 0.1574\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0363 - val_loss: 0.1537\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0230 - val_loss: 0.1468\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0141 - val_loss: 0.1282\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0134 - val_loss: 0.1081\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0104 - val_loss: 0.0967\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0076 - val_loss: 0.0894\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0069 - val_loss: 0.0826\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0067 - val_loss: 0.0768\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0064 - val_loss: 0.0702\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0063 - val_loss: 0.0626\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0061 - val_loss: 0.0549\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0060 - val_loss: 0.0478\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0056 - val_loss: 0.0424\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0052 - val_loss: 0.0392\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0049 - val_loss: 0.0375\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0045 - val_loss: 0.0363\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0044 - val_loss: 0.0348\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0043 - val_loss: 0.0332\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0041 - val_loss: 0.0318\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0038 - val_loss: 0.0307\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0037 - val_loss: 0.0298\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0034 - val_loss: 0.0289\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0033 - val_loss: 0.0280\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0032 - val_loss: 0.0269\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0030 - val_loss: 0.0257\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0029 - val_loss: 0.0245\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0028 - val_loss: 0.0234\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0027 - val_loss: 0.0225\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0026 - val_loss: 0.0219\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0026 - val_loss: 0.0217\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0025 - val_loss: 0.0216\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0024 - val_loss: 0.0216\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0024 - val_loss: 0.0216\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0024 - val_loss: 0.0216\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0023 - val_loss: 0.0217\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb37942f430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2871 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.0415 - val_loss: 0.1161\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0248 - val_loss: 0.1126\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0174 - val_loss: 0.1063\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0137 - val_loss: 0.0999\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0110 - val_loss: 0.0945\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0082 - val_loss: 0.0903\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0075 - val_loss: 0.0849\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0062 - val_loss: 0.0777\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0057 - val_loss: 0.0726\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0047 - val_loss: 0.0653\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0048 - val_loss: 0.0545\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0040 - val_loss: 0.0437\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0035 - val_loss: 0.0352\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0035 - val_loss: 0.0298\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0033 - val_loss: 0.0269\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0032 - val_loss: 0.0246\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0030 - val_loss: 0.0229\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0026 - val_loss: 0.0219\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0024 - val_loss: 0.0212\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0024 - val_loss: 0.0206\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0024 - val_loss: 0.0199\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0023 - val_loss: 0.0190\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0022 - val_loss: 0.0186\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0021 - val_loss: 0.0185\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0020 - val_loss: 0.0185\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0020 - val_loss: 0.0184\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0020 - val_loss: 0.0181\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0018 - val_loss: 0.0181\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0018 - val_loss: 0.0181\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0017 - val_loss: 0.0181\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb49aa35550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2872 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.3213 - val_loss: 0.0739\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1197 - val_loss: 0.0739\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0288 - val_loss: 0.0768\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0130 - val_loss: 0.0765\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4a5c9e5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2873 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0805 - val_loss: 0.0403\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0284 - val_loss: 0.0330\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0170 - val_loss: 0.0285\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0142 - val_loss: 0.0264\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0149 - val_loss: 0.0261\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0104 - val_loss: 0.0262\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0095 - val_loss: 0.0266\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3f4d0e0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2874 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0789 - val_loss: 0.0957\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0362 - val_loss: 0.0800\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0188 - val_loss: 0.0601\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0163 - val_loss: 0.0444\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0145 - val_loss: 0.0393\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0086 - val_loss: 0.0358\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0071 - val_loss: 0.0338\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0077 - val_loss: 0.0340\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0061 - val_loss: 0.0350\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4afcb2f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2875 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.0924 - val_loss: 0.0458\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0667 - val_loss: 0.0431\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0157 - val_loss: 0.0394\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0108 - val_loss: 0.0376\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0101 - val_loss: 0.0363\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0086 - val_loss: 0.0358\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0077 - val_loss: 0.0349\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0070 - val_loss: 0.0339\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0064 - val_loss: 0.0331\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0059 - val_loss: 0.0327\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0056 - val_loss: 0.0323\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0053 - val_loss: 0.0321\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0051 - val_loss: 0.0318\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0057 - val_loss: 0.0317\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0049 - val_loss: 0.0316\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0049 - val_loss: 0.0316\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0046 - val_loss: 0.0316\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a0499d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2876 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.0888 - val_loss: 0.0296\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0325 - val_loss: 0.0289\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0201 - val_loss: 0.0282\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0148 - val_loss: 0.0281\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0112 - val_loss: 0.0299\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0107 - val_loss: 0.0325\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb39a03aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2877 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.1607 - val_loss: 0.2020\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0293 - val_loss: 0.1513\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0137 - val_loss: 0.1131\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0143 - val_loss: 0.0877\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0140 - val_loss: 0.0754\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0106 - val_loss: 0.0716\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0088 - val_loss: 0.0703\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0076 - val_loss: 0.0687\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0077 - val_loss: 0.0646\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0078 - val_loss: 0.0597\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0073 - val_loss: 0.0547\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0070 - val_loss: 0.0501\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0063 - val_loss: 0.0462\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0058 - val_loss: 0.0431\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0056 - val_loss: 0.0404\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0053 - val_loss: 0.0379\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0051 - val_loss: 0.0357\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0050 - val_loss: 0.0340\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0048 - val_loss: 0.0327\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0046 - val_loss: 0.0319\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0045 - val_loss: 0.0310\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0043 - val_loss: 0.0304\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0042 - val_loss: 0.0301\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0041 - val_loss: 0.0300\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0039 - val_loss: 0.0297\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0039 - val_loss: 0.0297\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0038 - val_loss: 0.0297\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0037 - val_loss: 0.0297\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0037 - val_loss: 0.0299\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb399111550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2878 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.1162 - val_loss: 0.0408\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0420 - val_loss: 0.0704\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0184 - val_loss: 0.0928\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c5569790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2879 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0553 - val_loss: 0.0410\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0244 - val_loss: 0.0406\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0139 - val_loss: 0.0406\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0145 - val_loss: 0.0408\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c4861700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2880 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.0510 - val_loss: 0.0393\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0237 - val_loss: 0.0414\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0175 - val_loss: 0.0434\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c40da5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2881 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.2070 - val_loss: 0.0875\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0704 - val_loss: 0.0871\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0174 - val_loss: 0.0766\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0146 - val_loss: 0.0705\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0107 - val_loss: 0.0694\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0085 - val_loss: 0.0713\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0069 - val_loss: 0.0722\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c3a13280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2882 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.0277 - val_loss: 0.0448\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0178 - val_loss: 0.0460\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0132 - val_loss: 0.0485\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c55c3d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2883 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.0669 - val_loss: 0.1023\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0157 - val_loss: 0.0891\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0137 - val_loss: 0.0901\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0106 - val_loss: 0.0893\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c33111f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2884 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.0359 - val_loss: 0.0281\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0192 - val_loss: 0.0305\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0186 - val_loss: 0.0311\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4142f5160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2885 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.1140 - val_loss: 0.0302\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0723 - val_loss: 0.0415\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0198 - val_loss: 0.0455\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3bcb39790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2886 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.0804 - val_loss: 0.0340\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0286 - val_loss: 0.0379\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0159 - val_loss: 0.0396\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3bb6f69d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2887 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_train_function.<locals>.train_function at 0x7fb3b9deeaf0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1767WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb3b7de10d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.1767 - val_loss: 0.0676\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0581 - val_loss: 0.0630\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0216 - val_loss: 0.0565\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0199 - val_loss: 0.0530\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0186 - val_loss: 0.0519\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0143 - val_loss: 0.0499\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0107 - val_loss: 0.0446\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0095 - val_loss: 0.0384\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0088 - val_loss: 0.0357\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0084 - val_loss: 0.0342\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0079 - val_loss: 0.0334\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0074 - val_loss: 0.0332\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0070 - val_loss: 0.0332\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0069 - val_loss: 0.0335\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3b9b27c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2888 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.1360 - val_loss: 0.0700\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1014 - val_loss: 0.0617\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0405 - val_loss: 0.0540\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0298 - val_loss: 0.0489\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0256 - val_loss: 0.0439\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0207 - val_loss: 0.0409\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0172 - val_loss: 0.0395\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0140 - val_loss: 0.0385\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0120 - val_loss: 0.0372\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0112 - val_loss: 0.0360\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0104 - val_loss: 0.0352\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0097 - val_loss: 0.0345\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0086 - val_loss: 0.0340\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0082 - val_loss: 0.0335\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0079 - val_loss: 0.0331\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0075 - val_loss: 0.0327\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0070 - val_loss: 0.0325\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0067 - val_loss: 0.0323\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0063 - val_loss: 0.0321\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0061 - val_loss: 0.0319\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0060 - val_loss: 0.0316\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0058 - val_loss: 0.0313\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0056 - val_loss: 0.0311\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0054 - val_loss: 0.0310\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0052 - val_loss: 0.0310\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0051 - val_loss: 0.0309\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0050 - val_loss: 0.0310\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0048 - val_loss: 0.0310\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e09d5ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2889 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 411ms/step - loss: 0.3909 - val_loss: 0.0811\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0945 - val_loss: 0.0517\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0311 - val_loss: 0.0352\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0377 - val_loss: 0.0278\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0284 - val_loss: 0.0251\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0185 - val_loss: 0.0285\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0142 - val_loss: 0.0341\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d4c1ed30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2890 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.5460 - val_loss: 0.1116\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2618 - val_loss: 0.0889\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1160 - val_loss: 0.0764\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0667 - val_loss: 0.0682\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0460 - val_loss: 0.0613\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0301 - val_loss: 0.0567\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0267 - val_loss: 0.0527\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0248 - val_loss: 0.0491\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0225 - val_loss: 0.0459\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0225 - val_loss: 0.0434\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0203 - val_loss: 0.0416\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0187 - val_loss: 0.0405\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0184 - val_loss: 0.0399\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0180 - val_loss: 0.0395\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0175 - val_loss: 0.0392\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0170 - val_loss: 0.0388\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0162 - val_loss: 0.0386\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0157 - val_loss: 0.0383\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0148 - val_loss: 0.0381\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0135 - val_loss: 0.0379\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0120 - val_loss: 0.0377\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0106 - val_loss: 0.0375\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0100 - val_loss: 0.0372\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0099 - val_loss: 0.0370\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0098 - val_loss: 0.0369\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0093 - val_loss: 0.0368\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.008 - 0s 24ms/step - loss: 0.0084 - val_loss: 0.0368\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0075 - val_loss: 0.0368\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0070 - val_loss: 0.0368\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0069 - val_loss: 0.0369\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c7cdc550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2891 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.0435 - val_loss: 0.0304\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0406 - val_loss: 0.0301\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0169 - val_loss: 0.0314\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0133 - val_loss: 0.0316\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e2674ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2892 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.8332 - val_loss: 0.2283\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3238 - val_loss: 0.1326\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1790 - val_loss: 0.1056\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0771 - val_loss: 0.0762\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0439 - val_loss: 0.0569\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0293 - val_loss: 0.0451\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0210 - val_loss: 0.0380\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0175 - val_loss: 0.0329\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0142 - val_loss: 0.0299\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0129 - val_loss: 0.0285\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0125 - val_loss: 0.0279\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0128 - val_loss: 0.0280\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0129 - val_loss: 0.0281\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d93c9af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2893 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 0.0633 - val_loss: 0.0541\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0418 - val_loss: 0.0595\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0180 - val_loss: 0.0664\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3f01f2280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2894 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.1090 - val_loss: 0.0372\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0292 - val_loss: 0.0338\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0192 - val_loss: 0.0319\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0137 - val_loss: 0.0304\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0117 - val_loss: 0.0292\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.009 - 0s 19ms/step - loss: 0.0096 - val_loss: 0.0289\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0081 - val_loss: 0.0295\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0074 - val_loss: 0.0296\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3f9eceaf0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2895 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0974 - val_loss: 0.0266\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0274 - val_loss: 0.0302\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0277 - val_loss: 0.0341\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb400da85e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2896 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.1811 - val_loss: 0.0575\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0493 - val_loss: 0.0360\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0243 - val_loss: 0.0310\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0171 - val_loss: 0.0280\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0143 - val_loss: 0.0272\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0079 - val_loss: 0.0265\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0049 - val_loss: 0.0259\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0058 - val_loss: 0.0256\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0048 - val_loss: 0.0254\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0047 - val_loss: 0.0252\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0045 - val_loss: 0.0248\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0045 - val_loss: 0.0246\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0045 - val_loss: 0.0244\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0042 - val_loss: 0.0244\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0037 - val_loss: 0.0244\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0035 - val_loss: 0.0243\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0033 - val_loss: 0.0240\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0032 - val_loss: 0.0237\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0030 - val_loss: 0.0235\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0029 - val_loss: 0.0232\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0029 - val_loss: 0.0229\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0029 - val_loss: 0.0227\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0028 - val_loss: 0.0224\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0027 - val_loss: 0.0222\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0027 - val_loss: 0.0220\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0026 - val_loss: 0.0218\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0026 - val_loss: 0.0217\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0026 - val_loss: 0.0216\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0025 - val_loss: 0.0216\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0024 - val_loss: 0.0215\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0024 - val_loss: 0.0215\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0023 - val_loss: 0.0215\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0023 - val_loss: 0.0214\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0023 - val_loss: 0.0214\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0022 - val_loss: 0.0213\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0022 - val_loss: 0.0211\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0022 - val_loss: 0.0210\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0021 - val_loss: 0.0209\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0021 - val_loss: 0.0208\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0021 - val_loss: 0.0208\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0020 - val_loss: 0.0208\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0020 - val_loss: 0.0207\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0020 - val_loss: 0.0207\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0019 - val_loss: 0.0207\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0019 - val_loss: 0.0207\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0019 - val_loss: 0.0206\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0019 - val_loss: 0.0206\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0018 - val_loss: 0.0205\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0018 - val_loss: 0.0204\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0018 - val_loss: 0.0204\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ff9a9280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2897 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.3318 - val_loss: 0.0497\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2171 - val_loss: 0.0387\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0914 - val_loss: 0.0337\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0345 - val_loss: 0.0294\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0239 - val_loss: 0.0267\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0166 - val_loss: 0.0252\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0128 - val_loss: 0.0244\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0130 - val_loss: 0.0239\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0129 - val_loss: 0.0238\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0117 - val_loss: 0.0235\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0106 - val_loss: 0.0235\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0100 - val_loss: 0.0236\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0092 - val_loss: 0.0239\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb40b6abc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2898 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.1220 - val_loss: 0.0730\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0558 - val_loss: 0.0509\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0263 - val_loss: 0.0413\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0170 - val_loss: 0.0355\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0151 - val_loss: 0.0321\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0137 - val_loss: 0.0296\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0118 - val_loss: 0.0277\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0107 - val_loss: 0.0265\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0094 - val_loss: 0.0262\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0086 - val_loss: 0.0260\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0076 - val_loss: 0.0256\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0073 - val_loss: 0.0255\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0068 - val_loss: 0.0258\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0062 - val_loss: 0.0252\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0057 - val_loss: 0.0248\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0055 - val_loss: 0.0245\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0053 - val_loss: 0.0245\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0050 - val_loss: 0.0248\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0047 - val_loss: 0.0248\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb43302bd30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2899 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.0931 - val_loss: 0.1246\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0551 - val_loss: 0.0991\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0391 - val_loss: 0.0907\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0221 - val_loss: 0.0870\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0244 - val_loss: 0.0774\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0165 - val_loss: 0.0701\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0138 - val_loss: 0.0659\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0127 - val_loss: 0.0641\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0114 - val_loss: 0.0632\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0096 - val_loss: 0.0623\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0090 - val_loss: 0.0608\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0089 - val_loss: 0.0583\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0083 - val_loss: 0.0547\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0079 - val_loss: 0.0508\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0075 - val_loss: 0.0473\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0069 - val_loss: 0.0442\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0066 - val_loss: 0.0415\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0067 - val_loss: 0.0391\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0065 - val_loss: 0.0373\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0061 - val_loss: 0.0363\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0058 - val_loss: 0.0358\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0055 - val_loss: 0.0357\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0052 - val_loss: 0.0357\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0049 - val_loss: 0.0354\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0049 - val_loss: 0.0344\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0047 - val_loss: 0.0331\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0044 - val_loss: 0.0318\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0041 - val_loss: 0.0306\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0040 - val_loss: 0.0298\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0040 - val_loss: 0.0296\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0039 - val_loss: 0.0295\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0038 - val_loss: 0.0296\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0036 - val_loss: 0.0300\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb43a2bdc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2900 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.1375 - val_loss: 0.0449\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0349 - val_loss: 0.0544\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0218 - val_loss: 0.0583\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4760d8430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2901 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 0.0915 - val_loss: 0.0590\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0301 - val_loss: 0.0537\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0138 - val_loss: 0.0512\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0170 - val_loss: 0.0502\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0105 - val_loss: 0.0496\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0093 - val_loss: 0.0489\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0091 - val_loss: 0.0479\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0082 - val_loss: 0.0467\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0070 - val_loss: 0.0459\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0058 - val_loss: 0.0451\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0052 - val_loss: 0.0440\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0049 - val_loss: 0.0429\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0045 - val_loss: 0.0417\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0041 - val_loss: 0.0408\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0040 - val_loss: 0.0406\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0038 - val_loss: 0.0408\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0034 - val_loss: 0.0410\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb42d7738b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2902 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.3082 - val_loss: 0.0315\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0901 - val_loss: 0.0417\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0427 - val_loss: 0.0393\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb41a501040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2903 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.0398 - val_loss: 0.0353\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0181 - val_loss: 0.0361\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0131 - val_loss: 0.0361\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4176348b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2904 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.2293 - val_loss: 0.0363\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0575 - val_loss: 0.0361\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0290 - val_loss: 0.0337\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0154 - val_loss: 0.0313\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0139 - val_loss: 0.0297\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0130 - val_loss: 0.0286\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0137 - val_loss: 0.0280\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0130 - val_loss: 0.0277\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0119 - val_loss: 0.0277\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0106 - val_loss: 0.0276\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0099 - val_loss: 0.0274\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0096 - val_loss: 0.0270\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0091 - val_loss: 0.0265\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0084 - val_loss: 0.0263\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0078 - val_loss: 0.0264\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0075 - val_loss: 0.0266\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb400222dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2905 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.0348 - val_loss: 0.0539\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0315 - val_loss: 0.0529\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0304 - val_loss: 0.0519\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0297 - val_loss: 0.0510\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0293 - val_loss: 0.0501\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0283 - val_loss: 0.0492\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0277 - val_loss: 0.0483\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0274 - val_loss: 0.0475\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0270 - val_loss: 0.0467\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0267 - val_loss: 0.0459\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0263 - val_loss: 0.0452\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0259 - val_loss: 0.0445\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0256 - val_loss: 0.0438\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0253 - val_loss: 0.0431\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0250 - val_loss: 0.0425\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0247 - val_loss: 0.0419\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0244 - val_loss: 0.0413\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0242 - val_loss: 0.0407\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0240 - val_loss: 0.0402\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.023 - 0s 21ms/step - loss: 0.0239 - val_loss: 0.0397\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0238 - val_loss: 0.0392\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0237 - val_loss: 0.0388\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0236 - val_loss: 0.0384\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0235 - val_loss: 0.0380\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0235 - val_loss: 0.0376\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0234 - val_loss: 0.0372\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0234 - val_loss: 0.0369\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0233 - val_loss: 0.0366\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0233 - val_loss: 0.0363\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0232 - val_loss: 0.0360\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0232 - val_loss: 0.0358\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0232 - val_loss: 0.0355\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0232 - val_loss: 0.0353\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0231 - val_loss: 0.0351\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0231 - val_loss: 0.0349\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0231 - val_loss: 0.0347\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0231 - val_loss: 0.0346\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0231 - val_loss: 0.0344\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0231 - val_loss: 0.0343\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0231 - val_loss: 0.0342\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0231 - val_loss: 0.0341\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0231 - val_loss: 0.0340\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0231 - val_loss: 0.0339\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0231 - val_loss: 0.0339\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0231 - val_loss: 0.0338\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0231 - val_loss: 0.0338\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0231 - val_loss: 0.0337\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0231 - val_loss: 0.0337\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0231 - val_loss: 0.0337\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0231 - val_loss: 0.0337\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb406697ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2906 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.3272 - val_loss: 0.1049\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0683 - val_loss: 0.0926\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0343 - val_loss: 0.0801\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0258 - val_loss: 0.0702\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0233 - val_loss: 0.0627\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0239 - val_loss: 0.0572\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0230 - val_loss: 0.0523\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0204 - val_loss: 0.0495\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0187 - val_loss: 0.0490\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0179 - val_loss: 0.0483\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0172 - val_loss: 0.0466\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0166 - val_loss: 0.0443\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0159 - val_loss: 0.0419\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0153 - val_loss: 0.0399\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0149 - val_loss: 0.0383\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0138 - val_loss: 0.0370\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0133 - val_loss: 0.0359\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0127 - val_loss: 0.0350\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0121 - val_loss: 0.0344\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0115 - val_loss: 0.0341\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0109 - val_loss: 0.0338\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0103 - val_loss: 0.0336\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0095 - val_loss: 0.0334\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0089 - val_loss: 0.0333\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0082 - val_loss: 0.0332\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0074 - val_loss: 0.0334\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0068 - val_loss: 0.0335\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3f207d0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2907 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.7830 - val_loss: 0.0644\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3459 - val_loss: 0.0631\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0945 - val_loss: 0.0600\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0547 - val_loss: 0.0567\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0404 - val_loss: 0.0536\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0340 - val_loss: 0.0498\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0296 - val_loss: 0.0460\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0251 - val_loss: 0.0427\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0225 - val_loss: 0.0407\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0207 - val_loss: 0.0394\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0195 - val_loss: 0.0383\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0188 - val_loss: 0.0372\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0182 - val_loss: 0.0363\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0175 - val_loss: 0.0355\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0170 - val_loss: 0.0348\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0164 - val_loss: 0.0341\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0157 - val_loss: 0.0335\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0151 - val_loss: 0.0329\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0144 - val_loss: 0.0324\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0135 - val_loss: 0.0319\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0131 - val_loss: 0.0314\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0125 - val_loss: 0.0310\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0120 - val_loss: 0.0307\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0116 - val_loss: 0.0303\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0114 - val_loss: 0.0300\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0111 - val_loss: 0.0298\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0108 - val_loss: 0.0295\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0105 - val_loss: 0.0293\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0101 - val_loss: 0.0290\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0097 - val_loss: 0.0288\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0094 - val_loss: 0.0287\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0093 - val_loss: 0.0285\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0092 - val_loss: 0.0284\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0092 - val_loss: 0.0282\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0090 - val_loss: 0.0280\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0087 - val_loss: 0.0279\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0085 - val_loss: 0.0277\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0084 - val_loss: 0.0276\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0082 - val_loss: 0.0275\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0081 - val_loss: 0.0274\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0080 - val_loss: 0.0273\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0079 - val_loss: 0.0272\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0078 - val_loss: 0.0271\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0077 - val_loss: 0.0270\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0075 - val_loss: 0.0270\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0074 - val_loss: 0.0269\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0072 - val_loss: 0.0269\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0071 - val_loss: 0.0268\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0070 - val_loss: 0.0268\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0069 - val_loss: 0.0268\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3fbda0820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2908 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.0597 - val_loss: 0.0317\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0353 - val_loss: 0.0320\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0234 - val_loss: 0.0313\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0179 - val_loss: 0.0307\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0159 - val_loss: 0.0302\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0135 - val_loss: 0.0303\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0108 - val_loss: 0.0306\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3fde18790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2909 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 727ms/step - loss: 0.3067 - val_loss: 0.1216\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.1579 - val_loss: 0.1025\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0929 - val_loss: 0.0874\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0603 - val_loss: 0.0795\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.0471 - val_loss: 0.0726\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0354 - val_loss: 0.0663\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0293 - val_loss: 0.0603\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0245 - val_loss: 0.0545\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0215 - val_loss: 0.0493\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0199 - val_loss: 0.0445\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0189 - val_loss: 0.0407\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0180 - val_loss: 0.0379\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0172 - val_loss: 0.0358\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0166 - val_loss: 0.0342\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0160 - val_loss: 0.0327\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0152 - val_loss: 0.0313\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0143 - val_loss: 0.0301\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0134 - val_loss: 0.0292\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0124 - val_loss: 0.0286\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0119 - val_loss: 0.0282\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0115 - val_loss: 0.0277\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0112 - val_loss: 0.0272\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0107 - val_loss: 0.0268\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0103 - val_loss: 0.0264\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0098 - val_loss: 0.0262\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0094 - val_loss: 0.0260\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0090 - val_loss: 0.0259\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0088 - val_loss: 0.0257\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0086 - val_loss: 0.0256\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0085 - val_loss: 0.0256\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0084 - val_loss: 0.0255\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0083 - val_loss: 0.0255\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0082 - val_loss: 0.0254\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0079 - val_loss: 0.0254\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0077 - val_loss: 0.0254\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0075 - val_loss: 0.0255\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cb1f88b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2910 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 546ms/step - loss: 0.0275 - val_loss: 0.0478\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0202 - val_loss: 0.0488\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0163 - val_loss: 0.0498\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cbe68670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2911 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 609ms/step - loss: 0.0386 - val_loss: 0.0548\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0442 - val_loss: 0.0585\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0211 - val_loss: 0.0591\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cca6f0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2912 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 568ms/step - loss: 0.1252 - val_loss: 0.0246\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0627 - val_loss: 0.0282\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0293 - val_loss: 0.0307\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d3fb38b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2913 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:5 out of the last 46 calls to <function Model.make_train_function.<locals>.train_function at 0x7fb3d4cfc430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2525WARNING:tensorflow:5 out of the last 46 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb482486820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2525 - val_loss: 0.0460\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.0372 - val_loss: 0.0423\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.0268 - val_loss: 0.0380\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.0178 - val_loss: 0.0359\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0136 - val_loss: 0.0341\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.0134 - val_loss: 0.0328\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 0.0106 - val_loss: 0.0322\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.0090 - val_loss: 0.0321\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.0089 - val_loss: 0.0320\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 352ms/step - loss: 0.0085 - val_loss: 0.0320\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 0.0080 - val_loss: 0.0322\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.0075 - val_loss: 0.0324\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3dacf0a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2914 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 517ms/step - loss: 0.1021 - val_loss: 0.2034\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0353 - val_loss: 0.1658\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0209 - val_loss: 0.1368\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0159 - val_loss: 0.1149\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0118 - val_loss: 0.0999\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0118 - val_loss: 0.0922\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0106 - val_loss: 0.0884\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0095 - val_loss: 0.0868\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0086 - val_loss: 0.0853\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0079 - val_loss: 0.0836\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0070 - val_loss: 0.0815\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0063 - val_loss: 0.0795\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0061 - val_loss: 0.0778\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0063 - val_loss: 0.0762\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0060 - val_loss: 0.0745\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0056 - val_loss: 0.0726\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.0055 - val_loss: 0.0706\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.0052 - val_loss: 0.0686\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.0049 - val_loss: 0.0668\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.0049 - val_loss: 0.0652\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 402ms/step - loss: 0.0047 - val_loss: 0.0636\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.0044 - val_loss: 0.0621\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.0044 - val_loss: 0.0607\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.0043 - val_loss: 0.0595\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 0.0041 - val_loss: 0.0584\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 0.0040 - val_loss: 0.0571\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.0039 - val_loss: 0.0559\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.0037 - val_loss: 0.0549\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0036 - val_loss: 0.0541\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.0035 - val_loss: 0.0537\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.0034 - val_loss: 0.0533\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0032 - val_loss: 0.0529\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.0031 - val_loss: 0.0524\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.0030 - val_loss: 0.0517\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 0.0030 - val_loss: 0.0511\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0028 - val_loss: 0.0505\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.0027 - val_loss: 0.0495\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0027 - val_loss: 0.0483\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0026 - val_loss: 0.0469\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.0025 - val_loss: 0.0455\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0024 - val_loss: 0.0440\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 0.0023 - val_loss: 0.0428\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.0022 - val_loss: 0.0418\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.0022 - val_loss: 0.0409\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0021 - val_loss: 0.0402\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0021 - val_loss: 0.0397\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0021 - val_loss: 0.0393\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0020 - val_loss: 0.0390\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0019 - val_loss: 0.0387\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0019 - val_loss: 0.0383\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c07c75e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2915 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0999 - val_loss: 0.1315\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.0460 - val_loss: 0.1218\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.0245 - val_loss: 0.1191\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0206 - val_loss: 0.1024\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0344 - val_loss: 0.1024\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.0138 - val_loss: 0.1028\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c54304c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2916 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 520ms/step - loss: 0.2474 - val_loss: 0.1076\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.1070 - val_loss: 0.1045\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0596 - val_loss: 0.1039\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0373 - val_loss: 0.1016\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0278 - val_loss: 0.0989\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0225 - val_loss: 0.0946\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0202 - val_loss: 0.0898\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0180 - val_loss: 0.0850\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0170 - val_loss: 0.0799\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0155 - val_loss: 0.0753\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0134 - val_loss: 0.0714\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0118 - val_loss: 0.0679\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0108 - val_loss: 0.0642\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0097 - val_loss: 0.0602\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0090 - val_loss: 0.0566\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0086 - val_loss: 0.0533\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0082 - val_loss: 0.0506\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0081 - val_loss: 0.0485\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0078 - val_loss: 0.0469\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0077 - val_loss: 0.0456\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0073 - val_loss: 0.0446\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0071 - val_loss: 0.0438\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0068 - val_loss: 0.0433\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0062 - val_loss: 0.0429\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0058 - val_loss: 0.0428\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0055 - val_loss: 0.0427\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0053 - val_loss: 0.0427\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0050 - val_loss: 0.0426\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.0047 - val_loss: 0.0425\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.0052 - val_loss: 0.0424\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0049 - val_loss: 0.0423\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0044 - val_loss: 0.0423\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0043 - val_loss: 0.0424\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c5c30040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2917 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 864ms/step - loss: 0.0787 - val_loss: 0.0761\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0227 - val_loss: 0.0695\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0210 - val_loss: 0.0671\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0119 - val_loss: 0.0663\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0092 - val_loss: 0.0676\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0086 - val_loss: 0.0681\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a7fbe790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2918 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 616ms/step - loss: 0.0975 - val_loss: 0.0211\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0300 - val_loss: 0.0347\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0526 - val_loss: 0.0441\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a993d4c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2919 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 781ms/step - loss: 0.3706 - val_loss: 0.0238\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.0829 - val_loss: 0.0317\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0391 - val_loss: 0.0496\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ad57c3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2920 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 564ms/step - loss: 0.0519 - val_loss: 0.0627\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0260 - val_loss: 0.0617\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0180 - val_loss: 0.0613\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0171 - val_loss: 0.0606\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0138 - val_loss: 0.0595\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0123 - val_loss: 0.0585\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0125 - val_loss: 0.0570\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0124 - val_loss: 0.0547\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0115 - val_loss: 0.0525\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0109 - val_loss: 0.0503\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0103 - val_loss: 0.0487\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0100 - val_loss: 0.0471\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0098 - val_loss: 0.0456\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0094 - val_loss: 0.0443\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0090 - val_loss: 0.0436\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0085 - val_loss: 0.0432\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0080 - val_loss: 0.0423\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0072 - val_loss: 0.0415\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0065 - val_loss: 0.0414\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0057 - val_loss: 0.0420\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0050 - val_loss: 0.0423\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3b27bf940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2921 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0639 - val_loss: 0.0743\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0302 - val_loss: 0.0468\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0234 - val_loss: 0.0404\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0137 - val_loss: 0.0387\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0124 - val_loss: 0.0365\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0122 - val_loss: 0.0335\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0115 - val_loss: 0.0323\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0106 - val_loss: 0.0323\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0092 - val_loss: 0.0321\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0082 - val_loss: 0.0314\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0076 - val_loss: 0.0302\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0069 - val_loss: 0.0290\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0068 - val_loss: 0.0274\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.0060 - val_loss: 0.0262\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0062 - val_loss: 0.0253\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0059 - val_loss: 0.0249\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0056 - val_loss: 0.0247\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0052 - val_loss: 0.0247\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0050 - val_loss: 0.0248\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb44cfc78b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2922 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 621ms/step - loss: 0.0733 - val_loss: 0.0418\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0534 - val_loss: 0.0407\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0214 - val_loss: 0.0380\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0171 - val_loss: 0.0361\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0133 - val_loss: 0.0352\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0128 - val_loss: 0.0348\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0104 - val_loss: 0.0351\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0084 - val_loss: 0.0355\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb47195e9d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2923 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 618ms/step - loss: 0.0767 - val_loss: 0.0598\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0364 - val_loss: 0.0725\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0173 - val_loss: 0.0724\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c3820820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2924 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 463ms/step - loss: 0.0356 - val_loss: 0.0416\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0270 - val_loss: 0.0331\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0292 - val_loss: 0.0262\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0186 - val_loss: 0.0215\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0137 - val_loss: 0.0199\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0142 - val_loss: 0.0219\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0122 - val_loss: 0.0235\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c47fe8b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2925 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0924 - val_loss: 0.0416\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.0479 - val_loss: 0.0512\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0401 - val_loss: 0.0490\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c7343c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2926 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 588ms/step - loss: 0.1467 - val_loss: 0.0403\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0325 - val_loss: 0.0288\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0338 - val_loss: 0.0260\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0265 - val_loss: 0.0229\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0158 - val_loss: 0.0221\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0146 - val_loss: 0.0210\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0106 - val_loss: 0.0206\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0111 - val_loss: 0.0203\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0101 - val_loss: 0.0198\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0081 - val_loss: 0.0197\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0078 - val_loss: 0.0197\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0065 - val_loss: 0.0195\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0058 - val_loss: 0.0194\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0056 - val_loss: 0.0196\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0049 - val_loss: 0.0201\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c4e355e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2927 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 567ms/step - loss: 0.0474 - val_loss: 0.0407\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0354 - val_loss: 0.0443\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0283 - val_loss: 0.0526\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a521b1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2928 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 531ms/step - loss: 0.1307 - val_loss: 0.0639\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.0596 - val_loss: 0.0626\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0475 - val_loss: 0.0593\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0368 - val_loss: 0.0565\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0330 - val_loss: 0.0552\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0233 - val_loss: 0.0529\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0180 - val_loss: 0.0494\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0169 - val_loss: 0.0460\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0160 - val_loss: 0.0408\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0147 - val_loss: 0.0369\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0126 - val_loss: 0.0346\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0110 - val_loss: 0.0327\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0105 - val_loss: 0.0309\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0102 - val_loss: 0.0293\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0095 - val_loss: 0.0279\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0084 - val_loss: 0.0267\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0079 - val_loss: 0.0262\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0074 - val_loss: 0.0262\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0069 - val_loss: 0.0261\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0068 - val_loss: 0.0256\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0066 - val_loss: 0.0245\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0063 - val_loss: 0.0230\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0060 - val_loss: 0.0215\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0059 - val_loss: 0.0202\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0056 - val_loss: 0.0194\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0053 - val_loss: 0.0186\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0052 - val_loss: 0.0181\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0049 - val_loss: 0.0179\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0047 - val_loss: 0.0177\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0045 - val_loss: 0.0176\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0043 - val_loss: 0.0175\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0042 - val_loss: 0.0174\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0041 - val_loss: 0.0174\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0040 - val_loss: 0.0173\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0038 - val_loss: 0.0173\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0036 - val_loss: 0.0172\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0035 - val_loss: 0.0172\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0034 - val_loss: 0.0173\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a58da0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2929 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 818ms/step - loss: 0.4123 - val_loss: 0.0660\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.1072 - val_loss: 0.0637\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0554 - val_loss: 0.0496\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0450 - val_loss: 0.0430\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0377 - val_loss: 0.0397\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0323 - val_loss: 0.0370\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0288 - val_loss: 0.0350\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0264 - val_loss: 0.0334\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0227 - val_loss: 0.0320\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0186 - val_loss: 0.0309\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0162 - val_loss: 0.0299\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0149 - val_loss: 0.0290\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0138 - val_loss: 0.0281\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0127 - val_loss: 0.0272\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0117 - val_loss: 0.0264\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0109 - val_loss: 0.0249\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0101 - val_loss: 0.0233\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0096 - val_loss: 0.0218\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0091 - val_loss: 0.0206\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0086 - val_loss: 0.0198\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.0081 - val_loss: 0.0193\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0078 - val_loss: 0.0190\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0078 - val_loss: 0.0188\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0077 - val_loss: 0.0189\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0075 - val_loss: 0.0189\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a61154c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2930 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 997ms/step - loss: 0.1143 - val_loss: 0.0540\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0408 - val_loss: 0.0482\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0249 - val_loss: 0.0508\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0192 - val_loss: 0.0535\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb387c50ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2931 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 566ms/step - loss: 0.3161 - val_loss: 0.0182\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0858 - val_loss: 0.0203\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.0380 - val_loss: 0.0239\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb389379550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2932 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 919ms/step - loss: 0.2088 - val_loss: 0.0304\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0553 - val_loss: 0.0284\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0796 - val_loss: 0.0275\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0350 - val_loss: 0.0305\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0262 - val_loss: 0.0326\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb390dbbc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2933 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0572 - val_loss: 0.0322\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.0213 - val_loss: 0.0303\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.0124 - val_loss: 0.0278\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.0114 - val_loss: 0.0231\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0107 - val_loss: 0.0221\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0093 - val_loss: 0.0248\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0071 - val_loss: 0.0297\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb39376ab80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2934 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 681ms/step - loss: 0.0656 - val_loss: 0.0253\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0588 - val_loss: 0.0187\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0179 - val_loss: 0.0310\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0192 - val_loss: 0.0400\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3aa9e7940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2935 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 706ms/step - loss: 0.2080 - val_loss: 0.0814\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.1836 - val_loss: 0.0660\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0947 - val_loss: 0.0356\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0501 - val_loss: 0.0199\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0349 - val_loss: 0.0185\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0288 - val_loss: 0.0223\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0248 - val_loss: 0.0293\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d946fb80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2936 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 606ms/step - loss: 0.1094 - val_loss: 0.0949\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0871 - val_loss: 0.0913\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0485 - val_loss: 0.0890\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0441 - val_loss: 0.0816\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0448 - val_loss: 0.0722\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0404 - val_loss: 0.0655\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0392 - val_loss: 0.0614\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0388 - val_loss: 0.0579\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.0379 - val_loss: 0.0553\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0371 - val_loss: 0.0535\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0364 - val_loss: 0.0511\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 0.0357 - val_loss: 0.0472\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.0350 - val_loss: 0.0437\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0345 - val_loss: 0.0403\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0339 - val_loss: 0.0372\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.0330 - val_loss: 0.0345\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0321 - val_loss: 0.0322\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0293 - val_loss: 0.0299\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0277 - val_loss: 0.0277\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0264 - val_loss: 0.0258\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0244 - val_loss: 0.0243\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0232 - val_loss: 0.0231\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0220 - val_loss: 0.0221\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0215 - val_loss: 0.0210\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0212 - val_loss: 0.0199\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0209 - val_loss: 0.0192\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0206 - val_loss: 0.0186\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0204 - val_loss: 0.0182\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0203 - val_loss: 0.0179\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.0198 - val_loss: 0.0179\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.0197 - val_loss: 0.0179\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0196 - val_loss: 0.0178\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.0193 - val_loss: 0.0176\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 0.0194 - val_loss: 0.0175\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.0195 - val_loss: 0.0174\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0190 - val_loss: 0.0174\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0186 - val_loss: 0.0174\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0182 - val_loss: 0.0173\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0173 - val_loss: 0.0175\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0167 - val_loss: 0.0177\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb49dc823a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2937 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 900ms/step - loss: 0.0572 - val_loss: 0.0443\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0393 - val_loss: 0.0433\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0363 - val_loss: 0.0424\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0326 - val_loss: 0.0396\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0315 - val_loss: 0.0364\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0307 - val_loss: 0.0345\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0302 - val_loss: 0.0321\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0296 - val_loss: 0.0306\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0290 - val_loss: 0.0290\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0283 - val_loss: 0.0291\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.0269 - val_loss: 0.0281\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0254 - val_loss: 0.0276\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0233 - val_loss: 0.0269\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0222 - val_loss: 0.0256\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0199 - val_loss: 0.0245\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0184 - val_loss: 0.0238\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0176 - val_loss: 0.0236\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0162 - val_loss: 0.0238\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0151 - val_loss: 0.0242\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb48b4de670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2938 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 881ms/step - loss: 0.1251 - val_loss: 0.0471\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0513 - val_loss: 0.0277\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0341 - val_loss: 0.0334\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0185 - val_loss: 0.0424\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d01c0790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2939 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 609ms/step - loss: 0.1010 - val_loss: 0.0771\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0410 - val_loss: 0.0648\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0224 - val_loss: 0.0568\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0145 - val_loss: 0.0554\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0123 - val_loss: 0.0512\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0109 - val_loss: 0.0466\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0091 - val_loss: 0.0440\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0073 - val_loss: 0.0446\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0066 - val_loss: 0.0456\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3b0ff88b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2940 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 809ms/step - loss: 0.1100 - val_loss: 0.0687\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.0608 - val_loss: 0.0609\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0268 - val_loss: 0.0353\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0210 - val_loss: 0.0230\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.0135 - val_loss: 0.0200\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0115 - val_loss: 0.0187\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.0098 - val_loss: 0.0186\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0092 - val_loss: 0.0191\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0088 - val_loss: 0.0194\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb37dec79d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2941 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 904ms/step - loss: 0.0881 - val_loss: 0.0161\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0501 - val_loss: 0.0157\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0196 - val_loss: 0.0176\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0155 - val_loss: 0.0215\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb37b1b7310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2942 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 758ms/step - loss: 0.0379 - val_loss: 0.0464\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.0233 - val_loss: 0.0409\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0144 - val_loss: 0.0365\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0161 - val_loss: 0.0358\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0114 - val_loss: 0.0345\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.0105 - val_loss: 0.0332\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0095 - val_loss: 0.0327\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0091 - val_loss: 0.0324\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.0087 - val_loss: 0.0322\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0081 - val_loss: 0.0320\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0076 - val_loss: 0.0325\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0071 - val_loss: 0.0333\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb395557700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2943 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 533ms/step - loss: 0.5221 - val_loss: 0.6676\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.1689 - val_loss: 0.6284\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0979 - val_loss: 0.5288\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0549 - val_loss: 0.4472\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0501 - val_loss: 0.3717\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0464 - val_loss: 0.3054\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0418 - val_loss: 0.2466\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0381 - val_loss: 0.1990\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0357 - val_loss: 0.1618\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0345 - val_loss: 0.1348\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0338 - val_loss: 0.1156\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0330 - val_loss: 0.1029\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0322 - val_loss: 0.0910\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0316 - val_loss: 0.0784\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0310 - val_loss: 0.0658\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0301 - val_loss: 0.0558\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0294 - val_loss: 0.0474\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0288 - val_loss: 0.0411\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0282 - val_loss: 0.0381\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0276 - val_loss: 0.0373\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0270 - val_loss: 0.0368\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0264 - val_loss: 0.0363\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0259 - val_loss: 0.0358\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0254 - val_loss: 0.0353\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0249 - val_loss: 0.0349\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0244 - val_loss: 0.0344\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0239 - val_loss: 0.0340\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0234 - val_loss: 0.0336\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0230 - val_loss: 0.0331\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0225 - val_loss: 0.0327\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0220 - val_loss: 0.0323\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0216 - val_loss: 0.0320\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0211 - val_loss: 0.0316\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0207 - val_loss: 0.0312\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0202 - val_loss: 0.0309\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0198 - val_loss: 0.0305\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0194 - val_loss: 0.0302\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0190 - val_loss: 0.0299\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0186 - val_loss: 0.0296\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0182 - val_loss: 0.0293\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0178 - val_loss: 0.0290\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0173 - val_loss: 0.0288\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0169 - val_loss: 0.0286\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0165 - val_loss: 0.0284\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0160 - val_loss: 0.0282\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0156 - val_loss: 0.0279\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.0152 - val_loss: 0.0277\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0146 - val_loss: 0.0274\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0140 - val_loss: 0.0272\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0134 - val_loss: 0.0270\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb37f86c3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2944 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 668ms/step - loss: 0.3156 - val_loss: 0.0714\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0970 - val_loss: 0.0721\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0774 - val_loss: 0.0700\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0523 - val_loss: 0.0670\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0375 - val_loss: 0.0645\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0280 - val_loss: 0.0625\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0196 - val_loss: 0.0600\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0166 - val_loss: 0.0570\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0144 - val_loss: 0.0523\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0127 - val_loss: 0.0484\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0125 - val_loss: 0.0451\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0127 - val_loss: 0.0439\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0119 - val_loss: 0.0434\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0115 - val_loss: 0.0429\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0111 - val_loss: 0.0424\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0109 - val_loss: 0.0420\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0106 - val_loss: 0.0416\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0105 - val_loss: 0.0412\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0104 - val_loss: 0.0408\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0103 - val_loss: 0.0404\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0102 - val_loss: 0.0401\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0101 - val_loss: 0.0397\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0100 - val_loss: 0.0394\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0100 - val_loss: 0.0391\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0099 - val_loss: 0.0389\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0098 - val_loss: 0.0386\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0097 - val_loss: 0.0383\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0096 - val_loss: 0.0381\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0096 - val_loss: 0.0378\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0095 - val_loss: 0.0376\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0094 - val_loss: 0.0373\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0093 - val_loss: 0.0371\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0093 - val_loss: 0.0369\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0093 - val_loss: 0.0367\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0092 - val_loss: 0.0365\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0092 - val_loss: 0.0363\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.0091 - val_loss: 0.0361\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.0090 - val_loss: 0.0360\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0090 - val_loss: 0.0358\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.0089 - val_loss: 0.0356\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.0089 - val_loss: 0.0354\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0089 - val_loss: 0.0353\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0088 - val_loss: 0.0351\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0088 - val_loss: 0.0349\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0088 - val_loss: 0.0348\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0087 - val_loss: 0.0346\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0087 - val_loss: 0.0345\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0087 - val_loss: 0.0343\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0086 - val_loss: 0.0342\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0086 - val_loss: 0.0340\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb39d7d95e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2945 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 711ms/step - loss: 0.0536 - val_loss: 0.0282\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0198 - val_loss: 0.0260\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0149 - val_loss: 0.0248\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0115 - val_loss: 0.0260\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0079 - val_loss: 0.0290\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb36db2adc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2946 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 707ms/step - loss: 0.3734 - val_loss: 0.0615\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.1932 - val_loss: 0.0572\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0884 - val_loss: 0.0519\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0528 - val_loss: 0.0460\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0406 - val_loss: 0.0415\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0316 - val_loss: 0.0369\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0266 - val_loss: 0.0333\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0239 - val_loss: 0.0305\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0221 - val_loss: 0.0282\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0208 - val_loss: 0.0264\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0192 - val_loss: 0.0252\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0159 - val_loss: 0.0242\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0135 - val_loss: 0.0233\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0113 - val_loss: 0.0225\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0098 - val_loss: 0.0217\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0087 - val_loss: 0.0211\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0078 - val_loss: 0.0207\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0074 - val_loss: 0.0205\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0073 - val_loss: 0.0205\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0074 - val_loss: 0.0204\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0073 - val_loss: 0.0205\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0070 - val_loss: 0.0207\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb36e04f1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2947 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 946ms/step - loss: 0.1968 - val_loss: 0.0217\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0832 - val_loss: 0.0198\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0476 - val_loss: 0.0178\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0239 - val_loss: 0.0178\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0154 - val_loss: 0.0181\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb36e37a550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2948 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 926ms/step - loss: 0.2102 - val_loss: 0.0166\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0801 - val_loss: 0.0184\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0362 - val_loss: 0.0214\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb36e762af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2949 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 847ms/step - loss: 0.2457 - val_loss: 0.0433\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 0.1047 - val_loss: 0.0277\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0420 - val_loss: 0.0202\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 0.0231 - val_loss: 0.0199\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0168 - val_loss: 0.0195\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0121 - val_loss: 0.0193\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 0.0097 - val_loss: 0.0196\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0082 - val_loss: 0.0200\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb36eb3fdc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2950 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 784ms/step - loss: 1.6177 - val_loss: 0.2837\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.4522 - val_loss: 0.2264\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.1669 - val_loss: 0.1965\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.1115 - val_loss: 0.1681\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0801 - val_loss: 0.1543\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0559 - val_loss: 0.1451\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.041 - 0s 182ms/step - loss: 0.0413 - val_loss: 0.1281\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0325 - val_loss: 0.1079\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0272 - val_loss: 0.0902\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0236 - val_loss: 0.0759\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0213 - val_loss: 0.0659\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0198 - val_loss: 0.0563\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0183 - val_loss: 0.0475\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0173 - val_loss: 0.0403\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0163 - val_loss: 0.0347\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0154 - val_loss: 0.0309\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0146 - val_loss: 0.0283\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.0141 - val_loss: 0.0263\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0137 - val_loss: 0.0249\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.0133 - val_loss: 0.0239\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.0130 - val_loss: 0.0231\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0126 - val_loss: 0.0226\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0122 - val_loss: 0.0223\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0118 - val_loss: 0.0221\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0113 - val_loss: 0.0221\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0109 - val_loss: 0.0223\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb36f0121f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2951 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0541 - val_loss: 0.0477\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0288 - val_loss: 0.0400\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0186 - val_loss: 0.0351\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0130 - val_loss: 0.0317\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0092 - val_loss: 0.0306\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0076 - val_loss: 0.0309\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0074 - val_loss: 0.0308\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb36f359670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2952 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0276 - val_loss: 0.0478\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.0156 - val_loss: 0.0427\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0128 - val_loss: 0.0429\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.0095 - val_loss: 0.0426\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0081 - val_loss: 0.0449\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0075 - val_loss: 0.0484\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb36f748b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2953 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 511ms/step - loss: 0.1150 - val_loss: 0.0691\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0784 - val_loss: 0.0934\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0284 - val_loss: 0.1091\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb371a81ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2954 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 547ms/step - loss: 0.1748 - val_loss: 0.0229\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.1243 - val_loss: 0.0268\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0263 - val_loss: 0.0315\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3726a00d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2955 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 712ms/step - loss: 0.3349 - val_loss: 0.2199\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0963 - val_loss: 0.1262\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0994 - val_loss: 0.0754\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0436 - val_loss: 0.0557\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0311 - val_loss: 0.0568\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0254 - val_loss: 0.0512\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0192 - val_loss: 0.0471\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0192 - val_loss: 0.0442\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0196 - val_loss: 0.0419\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0190 - val_loss: 0.0401\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0180 - val_loss: 0.0388\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0164 - val_loss: 0.0379\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0139 - val_loss: 0.0370\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0113 - val_loss: 0.0362\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0092 - val_loss: 0.0357\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0080 - val_loss: 0.0353\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0077 - val_loss: 0.0346\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0075 - val_loss: 0.0338\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0071 - val_loss: 0.0334\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0070 - val_loss: 0.0330\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0067 - val_loss: 0.0327\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0060 - val_loss: 0.0326\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0053 - val_loss: 0.0324\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0048 - val_loss: 0.0319\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0043 - val_loss: 0.0313\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0040 - val_loss: 0.0307\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0038 - val_loss: 0.0303\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0037 - val_loss: 0.0301\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0035 - val_loss: 0.0299\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0034 - val_loss: 0.0294\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0033 - val_loss: 0.0285\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0032 - val_loss: 0.0274\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0031 - val_loss: 0.0265\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0031 - val_loss: 0.0252\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0030 - val_loss: 0.0236\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0029 - val_loss: 0.0224\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0028 - val_loss: 0.0218\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0028 - val_loss: 0.0215\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0026 - val_loss: 0.0212\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0025 - val_loss: 0.0210\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0025 - val_loss: 0.0208\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0025 - val_loss: 0.0206\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0024 - val_loss: 0.0205\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0024 - val_loss: 0.0204\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.0023 - val_loss: 0.0205\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0022 - val_loss: 0.0205\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb374401820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2956 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 674ms/step - loss: 0.1063 - val_loss: 0.0214\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0235 - val_loss: 0.0252\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0151 - val_loss: 0.0309\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb375b24c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2957 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 825ms/step - loss: 0.5872 - val_loss: 0.0811\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.1745 - val_loss: 0.0638\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0736 - val_loss: 0.0609\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0536 - val_loss: 0.0551\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0435 - val_loss: 0.0497\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0365 - val_loss: 0.0464\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0280 - val_loss: 0.0446\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0206 - val_loss: 0.0448\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0165 - val_loss: 0.0471\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3773bda60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2958 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 539ms/step - loss: 0.5233 - val_loss: 0.1653\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.1600 - val_loss: 0.0677\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0417 - val_loss: 0.0308\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0207 - val_loss: 0.0214\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0140 - val_loss: 0.0200\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0141 - val_loss: 0.0204\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0141 - val_loss: 0.0214\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb37bf67280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2959 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 600ms/step - loss: 0.0447 - val_loss: 0.0409\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.0240 - val_loss: 0.0360\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0224 - val_loss: 0.0344\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0118 - val_loss: 0.0329\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0119 - val_loss: 0.0312\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0101 - val_loss: 0.0297\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0087 - val_loss: 0.0283\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0080 - val_loss: 0.0271\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0076 - val_loss: 0.0262\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0078 - val_loss: 0.0255\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0076 - val_loss: 0.0251\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.0072 - val_loss: 0.0249\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0070 - val_loss: 0.0249\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0067 - val_loss: 0.0251\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb357c070d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2960 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 581ms/step - loss: 0.1491 - val_loss: 0.1426\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0359 - val_loss: 0.1108\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0150 - val_loss: 0.0835\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0109 - val_loss: 0.0650\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0098 - val_loss: 0.0529\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0074 - val_loss: 0.0451\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0065 - val_loss: 0.0396\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0059 - val_loss: 0.0348\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0061 - val_loss: 0.0308\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0058 - val_loss: 0.0281\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0052 - val_loss: 0.0261\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0052 - val_loss: 0.0249\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0049 - val_loss: 0.0241\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0044 - val_loss: 0.0232\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0042 - val_loss: 0.0223\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0040 - val_loss: 0.0216\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0039 - val_loss: 0.0212\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0038 - val_loss: 0.0208\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0035 - val_loss: 0.0206\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0034 - val_loss: 0.0205\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0033 - val_loss: 0.0205\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0032 - val_loss: 0.0205\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0031 - val_loss: 0.0205\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0029 - val_loss: 0.0206\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3592f6d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2961 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 716ms/step - loss: 0.1723 - val_loss: 0.0354\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0529 - val_loss: 0.0228\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0163 - val_loss: 0.0187\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0206 - val_loss: 0.0188\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0150 - val_loss: 0.0191\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb35ae8c940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2962 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 842ms/step - loss: 0.3674 - val_loss: 0.0174\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.1936 - val_loss: 0.0180\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0956 - val_loss: 0.0211\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d01c03a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2963 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 615ms/step - loss: 0.3999 - val_loss: 0.0811\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1003 - val_loss: 0.0645\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0521 - val_loss: 0.0532\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0316 - val_loss: 0.0465\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0277 - val_loss: 0.0421\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0252 - val_loss: 0.0400\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0228 - val_loss: 0.0386\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0207 - val_loss: 0.0373\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0185 - val_loss: 0.0363\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0174 - val_loss: 0.0351\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0163 - val_loss: 0.0335\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0151 - val_loss: 0.0316\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0138 - val_loss: 0.0298\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0126 - val_loss: 0.0281\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0117 - val_loss: 0.0268\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0106 - val_loss: 0.0258\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0099 - val_loss: 0.0252\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0091 - val_loss: 0.0248\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0086 - val_loss: 0.0247\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0080 - val_loss: 0.0247\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0077 - val_loss: 0.0246\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0072 - val_loss: 0.0246\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0067 - val_loss: 0.0245\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0064 - val_loss: 0.0245\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0063 - val_loss: 0.0245\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0061 - val_loss: 0.0245\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb401bbd790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2964 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2783 - val_loss: 0.0419\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0820 - val_loss: 0.0375\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0408 - val_loss: 0.0353\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.0254 - val_loss: 0.0338\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0202 - val_loss: 0.0330\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0176 - val_loss: 0.0329\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0167 - val_loss: 0.0329\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0144 - val_loss: 0.0329\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0110 - val_loss: 0.0329\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c579e670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2965 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 683ms/step - loss: 0.0300 - val_loss: 0.0340\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0153 - val_loss: 0.0324\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0168 - val_loss: 0.0314\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0111 - val_loss: 0.0302\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0102 - val_loss: 0.0284\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0085 - val_loss: 0.0265\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0076 - val_loss: 0.0247\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0068 - val_loss: 0.0237\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0062 - val_loss: 0.0232\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0057 - val_loss: 0.0228\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0053 - val_loss: 0.0225\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0047 - val_loss: 0.0223\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0044 - val_loss: 0.0222\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0042 - val_loss: 0.0220\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0042 - val_loss: 0.0218\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0041 - val_loss: 0.0216\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0041 - val_loss: 0.0213\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0040 - val_loss: 0.0210\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0039 - val_loss: 0.0207\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0038 - val_loss: 0.0206\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0037 - val_loss: 0.0205\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0036 - val_loss: 0.0204\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0035 - val_loss: 0.0203\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0034 - val_loss: 0.0204\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0034 - val_loss: 0.0204\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3f039f550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2966 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0355 - val_loss: 0.0317\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0246 - val_loss: 0.0294\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0139 - val_loss: 0.0278\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0090 - val_loss: 0.0264\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0077 - val_loss: 0.0249\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.0082 - val_loss: 0.0238\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0073 - val_loss: 0.0231\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0064 - val_loss: 0.0228\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0060 - val_loss: 0.0228\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0061 - val_loss: 0.0228\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0061 - val_loss: 0.0230\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ea6714c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2967 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 679ms/step - loss: 0.4899 - val_loss: 0.1097\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.1519 - val_loss: 0.0769\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0770 - val_loss: 0.0652\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0424 - val_loss: 0.0594\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0344 - val_loss: 0.0556\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0307 - val_loss: 0.0521\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0278 - val_loss: 0.0487\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0281 - val_loss: 0.0459\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0238 - val_loss: 0.0434\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0219 - val_loss: 0.0414\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0192 - val_loss: 0.0398\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0169 - val_loss: 0.0383\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0161 - val_loss: 0.0371\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0156 - val_loss: 0.0361\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0149 - val_loss: 0.0354\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0143 - val_loss: 0.0349\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0137 - val_loss: 0.0344\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0126 - val_loss: 0.0340\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0114 - val_loss: 0.0336\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0103 - val_loss: 0.0333\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0094 - val_loss: 0.0331\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0087 - val_loss: 0.0329\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0083 - val_loss: 0.0327\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0079 - val_loss: 0.0324\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0074 - val_loss: 0.0321\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0069 - val_loss: 0.0318\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0067 - val_loss: 0.0316\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0066 - val_loss: 0.0314\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0064 - val_loss: 0.0312\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0059 - val_loss: 0.0308\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0055 - val_loss: 0.0303\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0053 - val_loss: 0.0298\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0052 - val_loss: 0.0293\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0050 - val_loss: 0.0287\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0047 - val_loss: 0.0282\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0045 - val_loss: 0.0278\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0044 - val_loss: 0.0273\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0042 - val_loss: 0.0267\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0041 - val_loss: 0.0261\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0039 - val_loss: 0.0255\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0037 - val_loss: 0.0248\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0036 - val_loss: 0.0245\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0035 - val_loss: 0.0242\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.0033 - val_loss: 0.0241\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0032 - val_loss: 0.0239\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0032 - val_loss: 0.0237\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0031 - val_loss: 0.0235\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0030 - val_loss: 0.0233\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0030 - val_loss: 0.0232\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0029 - val_loss: 0.0230\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c3311160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2968 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 902ms/step - loss: 0.0313 - val_loss: 0.0353\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0204 - val_loss: 0.0369\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0152 - val_loss: 0.0368\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4a5c9e8b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2969 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.1499 - val_loss: 0.0228\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0755 - val_loss: 0.0201\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0357 - val_loss: 0.0253\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0216 - val_loss: 0.0259\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb40da91040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2970 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 824ms/step - loss: 0.3606 - val_loss: 0.0191\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0878 - val_loss: 0.0219\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0428 - val_loss: 0.0245\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4cc7e2550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2971 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 664ms/step - loss: 0.0780 - val_loss: 0.0647\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0475 - val_loss: 0.0369\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0136 - val_loss: 0.0234\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0122 - val_loss: 0.0195\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0091 - val_loss: 0.0174\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0086 - val_loss: 0.0168\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0071 - val_loss: 0.0171\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0065 - val_loss: 0.0173\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb37e9f4940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2972 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 556ms/step - loss: 0.0677 - val_loss: 0.0286\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.0525 - val_loss: 0.0315\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0191 - val_loss: 0.0333\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3766a0040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2973 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 653ms/step - loss: 0.0301 - val_loss: 0.0314\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0194 - val_loss: 0.0322\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0217 - val_loss: 0.0382\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb373bf41f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2974 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 560ms/step - loss: 0.1835 - val_loss: 0.0186\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.1143 - val_loss: 0.0158\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0516 - val_loss: 0.0167\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0352 - val_loss: 0.0183\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3726a00d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2975 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 560ms/step - loss: 0.8349 - val_loss: 0.3718\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2086 - val_loss: 0.3345\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.1029 - val_loss: 0.3174\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0428 - val_loss: 0.3016\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0219 - val_loss: 0.2841\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0174 - val_loss: 0.2502\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0126 - val_loss: 0.2186\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0108 - val_loss: 0.1917\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0111 - val_loss: 0.1682\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0125 - val_loss: 0.1470\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0129 - val_loss: 0.1299\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0113 - val_loss: 0.1208\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0101 - val_loss: 0.1136\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0091 - val_loss: 0.1071\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0085 - val_loss: 0.1007\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0083 - val_loss: 0.0938\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0081 - val_loss: 0.0852\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0078 - val_loss: 0.0755\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0074 - val_loss: 0.0667\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0064 - val_loss: 0.0591\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0054 - val_loss: 0.0521\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0052 - val_loss: 0.0449\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0052 - val_loss: 0.0381\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0051 - val_loss: 0.0320\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0050 - val_loss: 0.0265\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0046 - val_loss: 0.0225\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0044 - val_loss: 0.0202\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0043 - val_loss: 0.0191\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0041 - val_loss: 0.0187\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0039 - val_loss: 0.0185\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0036 - val_loss: 0.0184\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0037 - val_loss: 0.0182\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0035 - val_loss: 0.0182\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0032 - val_loss: 0.0182\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0032 - val_loss: 0.0183\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb370bbd820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2976 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 748ms/step - loss: 0.0709 - val_loss: 0.0502\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0440 - val_loss: 0.0413\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0282 - val_loss: 0.0351\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0224 - val_loss: 0.0301\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0185 - val_loss: 0.0285\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.0145 - val_loss: 0.0279\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0116 - val_loss: 0.0274\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0096 - val_loss: 0.0271\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0083 - val_loss: 0.0265\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0075 - val_loss: 0.0254\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0073 - val_loss: 0.0244\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0065 - val_loss: 0.0238\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0059 - val_loss: 0.0234\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0057 - val_loss: 0.0233\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0052 - val_loss: 0.0235\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0047 - val_loss: 0.0240\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb36f0df790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2977 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 720ms/step - loss: 0.2049 - val_loss: 0.0253\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0253 - val_loss: 0.0327\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0260 - val_loss: 0.0394\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb36e8a50d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2978 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 579ms/step - loss: 0.1346 - val_loss: 0.1335\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.2050 - val_loss: 0.0474\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0675 - val_loss: 0.0201\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0347 - val_loss: 0.0148\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0301 - val_loss: 0.0157\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0281 - val_loss: 0.0173\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb36e430430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2979 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 725ms/step - loss: 0.2935 - val_loss: 0.0432\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1452 - val_loss: 0.0303\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0464 - val_loss: 0.0253\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.033 - 0s 65ms/step - loss: 0.0336 - val_loss: 0.0239\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0289 - val_loss: 0.0230\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0261 - val_loss: 0.0222\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0242 - val_loss: 0.0214\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0221 - val_loss: 0.0206\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0201 - val_loss: 0.0199\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0179 - val_loss: 0.0192\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0162 - val_loss: 0.0186\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0145 - val_loss: 0.0180\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0131 - val_loss: 0.0175\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0121 - val_loss: 0.0170\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0112 - val_loss: 0.0166\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0104 - val_loss: 0.0161\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0099 - val_loss: 0.0157\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0096 - val_loss: 0.0154\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0094 - val_loss: 0.0151\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0093 - val_loss: 0.0149\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0093 - val_loss: 0.0147\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0093 - val_loss: 0.0145\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0091 - val_loss: 0.0145\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0090 - val_loss: 0.0144\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0086 - val_loss: 0.0144\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0082 - val_loss: 0.0144\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0078 - val_loss: 0.0144\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb36dd108b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2980 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 669ms/step - loss: 0.0442 - val_loss: 0.0256\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0242 - val_loss: 0.0211\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0202 - val_loss: 0.0163\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0170 - val_loss: 0.0156\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0136 - val_loss: 0.0170\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0106 - val_loss: 0.0170\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb379533790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2981 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 926ms/step - loss: 0.0425 - val_loss: 0.0185\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0165 - val_loss: 0.0201\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0133 - val_loss: 0.0209\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb395bad280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2982 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 556ms/step - loss: 0.1504 - val_loss: 0.0239\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0624 - val_loss: 0.0297\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0230 - val_loss: 0.0341\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb401edd550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2983 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 555ms/step - loss: 0.1594 - val_loss: 0.0333\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.0840 - val_loss: 0.0303\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0545 - val_loss: 0.0269\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0420 - val_loss: 0.0248\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0309 - val_loss: 0.0227\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0250 - val_loss: 0.0209\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0214 - val_loss: 0.0193\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0189 - val_loss: 0.0179\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0171 - val_loss: 0.0169\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0160 - val_loss: 0.0162\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0152 - val_loss: 0.0158\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0146 - val_loss: 0.0156\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0137 - val_loss: 0.0156\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0126 - val_loss: 0.0157\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb47efa0550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2984 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 572ms/step - loss: 0.0250 - val_loss: 0.0185\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0250 - val_loss: 0.0247\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.017 - 0s 59ms/step - loss: 0.0176 - val_loss: 0.0260\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a7c6a8b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2985 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 562ms/step - loss: 0.3968 - val_loss: 0.2184\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2080 - val_loss: 0.2004\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.1260 - val_loss: 0.1802\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0813 - val_loss: 0.1326\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0633 - val_loss: 0.0968\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0487 - val_loss: 0.0649\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0376 - val_loss: 0.0389\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0313 - val_loss: 0.0258\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0266 - val_loss: 0.0196\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0232 - val_loss: 0.0162\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0208 - val_loss: 0.0143\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0195 - val_loss: 0.0133\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0183 - val_loss: 0.0130\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0169 - val_loss: 0.0129\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0152 - val_loss: 0.0129\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.0138 - val_loss: 0.0130\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb391b7a310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2986 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 691ms/step - loss: 0.0984 - val_loss: 0.0277\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0376 - val_loss: 0.0256\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0171 - val_loss: 0.0227\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0119 - val_loss: 0.0197\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0136 - val_loss: 0.0176\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0144 - val_loss: 0.0162\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.0100 - val_loss: 0.0153\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0107 - val_loss: 0.0148\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0084 - val_loss: 0.0146\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.0085 - val_loss: 0.0147\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0075 - val_loss: 0.0151\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb38dec4700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2987 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 532ms/step - loss: 0.0362 - val_loss: 0.0156\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0352 - val_loss: 0.0202\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0252 - val_loss: 0.0232\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a65223a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2988 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 570ms/step - loss: 0.1603 - val_loss: 0.1504\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0591 - val_loss: 0.0930\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0241 - val_loss: 0.0595\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0209 - val_loss: 0.0502\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.0187 - val_loss: 0.0429\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0167 - val_loss: 0.0332\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0138 - val_loss: 0.0250\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0122 - val_loss: 0.0192\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0112 - val_loss: 0.0161\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0100 - val_loss: 0.0140\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0093 - val_loss: 0.0124\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0083 - val_loss: 0.0114\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0074 - val_loss: 0.0108\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0065 - val_loss: 0.0103\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0057 - val_loss: 0.0101\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0052 - val_loss: 0.0100\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0048 - val_loss: 0.0099\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0046 - val_loss: 0.0099\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0046 - val_loss: 0.0098\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0046 - val_loss: 0.0099\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0044 - val_loss: 0.0100\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a6015b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2989 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 868ms/step - loss: 0.0626 - val_loss: 0.0113\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0311 - val_loss: 0.0115\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0211 - val_loss: 0.0114\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a567a310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2990 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 531ms/step - loss: 0.0483 - val_loss: 0.0134\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0816 - val_loss: 0.0105\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0257 - val_loss: 0.0090\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0127 - val_loss: 0.0094\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0118 - val_loss: 0.0105\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a4f11940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2991 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0533 - val_loss: 0.0423\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0322 - val_loss: 0.0386\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0396 - val_loss: 0.0354\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0168 - val_loss: 0.0332\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0098 - val_loss: 0.0320\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0080 - val_loss: 0.0312\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0075 - val_loss: 0.0307\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0072 - val_loss: 0.0300\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0074 - val_loss: 0.0293\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0064 - val_loss: 0.0285\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0058 - val_loss: 0.0279\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0053 - val_loss: 0.0275\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0047 - val_loss: 0.0272\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0045 - val_loss: 0.0272\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0044 - val_loss: 0.0276\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c48eedc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2992 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 700ms/step - loss: 0.0680 - val_loss: 0.0231\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0655 - val_loss: 0.0145\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0350 - val_loss: 0.0092\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0273 - val_loss: 0.0073\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0168 - val_loss: 0.0083\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0139 - val_loss: 0.0104\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c35769d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2993 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 609ms/step - loss: 0.2849 - val_loss: 0.0498\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.1426 - val_loss: 0.0237\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0377 - val_loss: 0.0119\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0384 - val_loss: 0.0077\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0271 - val_loss: 0.0081\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0207 - val_loss: 0.0087\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c3820c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2994 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 516ms/step - loss: 0.1990 - val_loss: 0.0974\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0782 - val_loss: 0.0915\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0408 - val_loss: 0.0752\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0340 - val_loss: 0.0578\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0285 - val_loss: 0.0410\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0243 - val_loss: 0.0277\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0215 - val_loss: 0.0187\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0198 - val_loss: 0.0140\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0184 - val_loss: 0.0125\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0170 - val_loss: 0.0112\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0156 - val_loss: 0.0101\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0143 - val_loss: 0.0091\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0132 - val_loss: 0.0085\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0123 - val_loss: 0.0080\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.011 - 0s 107ms/step - loss: 0.0114 - val_loss: 0.0079\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.010 - 0s 66ms/step - loss: 0.0104 - val_loss: 0.0078\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0094 - val_loss: 0.0078\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0088 - val_loss: 0.0077\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0086 - val_loss: 0.0077\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0085 - val_loss: 0.0077\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0083 - val_loss: 0.0077\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ea382700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2995 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 780ms/step - loss: 0.0854 - val_loss: 0.0803\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0702 - val_loss: 0.0628\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0252 - val_loss: 0.0418\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0182 - val_loss: 0.0314\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0115 - val_loss: 0.0225\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0105 - val_loss: 0.0168\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0119 - val_loss: 0.0140\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0108 - val_loss: 0.0130\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0089 - val_loss: 0.0121\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0083 - val_loss: 0.0114\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0074 - val_loss: 0.0109\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0073 - val_loss: 0.0105\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0069 - val_loss: 0.0102\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0063 - val_loss: 0.0099\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0060 - val_loss: 0.0099\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0055 - val_loss: 0.0100\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0051 - val_loss: 0.0103\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d9741670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2996 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7845 - val_loss: 0.0648\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.6721 - val_loss: 0.0589\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.2739 - val_loss: 0.0528\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.1821 - val_loss: 0.0483\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.1225 - val_loss: 0.0419\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0888 - val_loss: 0.0353\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0614 - val_loss: 0.0292\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0443 - val_loss: 0.0244\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0366 - val_loss: 0.0194\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0319 - val_loss: 0.0152\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0273 - val_loss: 0.0122\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0229 - val_loss: 0.0102\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0184 - val_loss: 0.0091\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0145 - val_loss: 0.0085\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0123 - val_loss: 0.0082\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0112 - val_loss: 0.0082\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0111 - val_loss: 0.0083\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0115 - val_loss: 0.0084\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb44d43fc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2997 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 782ms/step - loss: 0.2858 - val_loss: 0.1057\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.1594 - val_loss: 0.0788\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0715 - val_loss: 0.0602\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0390 - val_loss: 0.0417\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0250 - val_loss: 0.0275\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0215 - val_loss: 0.0177\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0212 - val_loss: 0.0121\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0197 - val_loss: 0.0092\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0165 - val_loss: 0.0082\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0140 - val_loss: 0.0078\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0119 - val_loss: 0.0078\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0098 - val_loss: 0.0078\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0092 - val_loss: 0.0079\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ac545700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2998 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 807ms/step - loss: 0.2449 - val_loss: 0.0080\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0603 - val_loss: 0.0085\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0718 - val_loss: 0.0089\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c637aaf0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_2999 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 643ms/step - loss: 0.2547 - val_loss: 0.0235\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.1928 - val_loss: 0.0136\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0760 - val_loss: 0.0089\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0455 - val_loss: 0.0089\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0292 - val_loss: 0.0107\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0223 - val_loss: 0.0134\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c593ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3000 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 668ms/step - loss: 0.0946 - val_loss: 0.0151\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0507 - val_loss: 0.0120\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0227 - val_loss: 0.0096\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0225 - val_loss: 0.0088\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0172 - val_loss: 0.0091\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0133 - val_loss: 0.0096\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c510c820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3001 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 564ms/step - loss: 0.0372 - val_loss: 0.0073\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0208 - val_loss: 0.0076\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0164 - val_loss: 0.0081\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d01998b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3002 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 666ms/step - loss: 0.0955 - val_loss: 0.0570\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0579 - val_loss: 0.0422\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0423 - val_loss: 0.0267\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0198 - val_loss: 0.0188\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0175 - val_loss: 0.0144\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0122 - val_loss: 0.0112\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0095 - val_loss: 0.0098\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0077 - val_loss: 0.0096\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0064 - val_loss: 0.0098\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0061 - val_loss: 0.0100\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d350f3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3003 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 527ms/step - loss: 0.1341 - val_loss: 0.0259\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0874 - val_loss: 0.0235\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0512 - val_loss: 0.0223\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0319 - val_loss: 0.0211\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0258 - val_loss: 0.0201\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0193 - val_loss: 0.0191\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0120 - val_loss: 0.0178\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0102 - val_loss: 0.0162\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0093 - val_loss: 0.0146\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0090 - val_loss: 0.0131\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0085 - val_loss: 0.0120\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0082 - val_loss: 0.0110\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0081 - val_loss: 0.0106\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0076 - val_loss: 0.0102\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0072 - val_loss: 0.0100\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0068 - val_loss: 0.0098\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0064 - val_loss: 0.0098\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0059 - val_loss: 0.0098\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0054 - val_loss: 0.0098\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cc127ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3004 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 734ms/step - loss: 0.9057 - val_loss: 0.0278\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.2206 - val_loss: 0.0230\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0580 - val_loss: 0.0156\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0419 - val_loss: 0.0133\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0315 - val_loss: 0.0116\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0282 - val_loss: 0.0099\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.0244 - val_loss: 0.0090\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0234 - val_loss: 0.0079\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0212 - val_loss: 0.0073\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0191 - val_loss: 0.0071\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0171 - val_loss: 0.0071\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0148 - val_loss: 0.0072\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0129 - val_loss: 0.0072\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3cb1f8430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3005 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 680ms/step - loss: 0.0550 - val_loss: 0.0064\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0836 - val_loss: 0.0071\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0177 - val_loss: 0.0101\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3fde18700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3006 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 630ms/step - loss: 0.1366 - val_loss: 0.0053\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0397 - val_loss: 0.0058\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0327 - val_loss: 0.0070\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4124a0700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3007 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 636ms/step - loss: 0.4383 - val_loss: 0.0376\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1275 - val_loss: 0.0297\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0371 - val_loss: 0.0244\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0304 - val_loss: 0.0169\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0258 - val_loss: 0.0117\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0215 - val_loss: 0.0081\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0181 - val_loss: 0.0073\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0158 - val_loss: 0.0071\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0122 - val_loss: 0.0072\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0099 - val_loss: 0.0073\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ee982c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3008 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 531ms/step - loss: 0.1150 - val_loss: 0.0424\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0549 - val_loss: 0.0352\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0336 - val_loss: 0.0262\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0184 - val_loss: 0.0191\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0112 - val_loss: 0.0144\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0099 - val_loss: 0.0121\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0088 - val_loss: 0.0089\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0083 - val_loss: 0.0070\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0084 - val_loss: 0.0062\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0079 - val_loss: 0.0061\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0072 - val_loss: 0.0061\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0064 - val_loss: 0.0063\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb400222dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3009 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 700ms/step - loss: 0.0436 - val_loss: 0.0060\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0229 - val_loss: 0.0060\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0174 - val_loss: 0.0064\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0205 - val_loss: 0.0066\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4124184c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3010 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 598ms/step - loss: 0.0537 - val_loss: 0.0406\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0193 - val_loss: 0.0322\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0189 - val_loss: 0.0286\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0130 - val_loss: 0.0281\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0107 - val_loss: 0.0282\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0089 - val_loss: 0.0278\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0085 - val_loss: 0.0263\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0079 - val_loss: 0.0242\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0062 - val_loss: 0.0217\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0049 - val_loss: 0.0190\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0044 - val_loss: 0.0161\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0044 - val_loss: 0.0135\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0038 - val_loss: 0.0118\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0034 - val_loss: 0.0112\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0030 - val_loss: 0.0112\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0029 - val_loss: 0.0113\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb42f0ac040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3011 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4887 - val_loss: 0.0578\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0856 - val_loss: 0.0342\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0499 - val_loss: 0.0313\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0338 - val_loss: 0.0305\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0279 - val_loss: 0.0302\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0185 - val_loss: 0.0296\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.0153 - val_loss: 0.0279\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0111 - val_loss: 0.0264\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0096 - val_loss: 0.0260\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0103 - val_loss: 0.0257\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0074 - val_loss: 0.0250\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0063 - val_loss: 0.0243\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0064 - val_loss: 0.0236\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.0063 - val_loss: 0.0227\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.0058 - val_loss: 0.0215\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0053 - val_loss: 0.0204\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0052 - val_loss: 0.0195\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.0052 - val_loss: 0.0187\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0044 - val_loss: 0.0183\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0042 - val_loss: 0.0180\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0040 - val_loss: 0.0178\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0038 - val_loss: 0.0178\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0036 - val_loss: 0.0178\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0034 - val_loss: 0.0181\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb471364310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3012 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.3344 - val_loss: 0.0735\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.1704 - val_loss: 0.0511\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0867 - val_loss: 0.0395\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0575 - val_loss: 0.0321\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0393 - val_loss: 0.0243\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.0250 - val_loss: 0.0177\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.0188 - val_loss: 0.0151\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0159 - val_loss: 0.0130\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.0140 - val_loss: 0.0116\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0131 - val_loss: 0.0104\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0125 - val_loss: 0.0095\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0117 - val_loss: 0.0088\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.0112 - val_loss: 0.0081\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0109 - val_loss: 0.0074\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.0102 - val_loss: 0.0070\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0100 - val_loss: 0.0069\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0097 - val_loss: 0.0069\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0097 - val_loss: 0.0070\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb46c6f0b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3013 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0735 - val_loss: 0.0273\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.0331 - val_loss: 0.0192\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0152 - val_loss: 0.0150\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0121 - val_loss: 0.0131\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0125 - val_loss: 0.0128\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0116 - val_loss: 0.0101\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0107 - val_loss: 0.0079\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0090 - val_loss: 0.0071\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0079 - val_loss: 0.0071\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0073 - val_loss: 0.0072\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb426a7d280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3014 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6792 - val_loss: 0.5954\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1348 - val_loss: 0.4246\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0899 - val_loss: 0.3062\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0580 - val_loss: 0.2586\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0530 - val_loss: 0.2225\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0474 - val_loss: 0.1902\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0412 - val_loss: 0.1614\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0330 - val_loss: 0.1373\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0307 - val_loss: 0.1167\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0286 - val_loss: 0.0979\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0269 - val_loss: 0.0805\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0258 - val_loss: 0.0661\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0247 - val_loss: 0.0544\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0235 - val_loss: 0.0443\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0223 - val_loss: 0.0360\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0214 - val_loss: 0.0297\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0206 - val_loss: 0.0249\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0198 - val_loss: 0.0212\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0192 - val_loss: 0.0182\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0186 - val_loss: 0.0162\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0182 - val_loss: 0.0143\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0176 - val_loss: 0.0124\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0171 - val_loss: 0.0109\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.016 - 0s 88ms/step - loss: 0.0168 - val_loss: 0.0098\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0163 - val_loss: 0.0090\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0161 - val_loss: 0.0084\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0159 - val_loss: 0.0079\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0156 - val_loss: 0.0077\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0152 - val_loss: 0.0075\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0149 - val_loss: 0.0075\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0146 - val_loss: 0.0075\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.014 - 0s 68ms/step - loss: 0.0142 - val_loss: 0.0077\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb40ff9cd30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3015 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 975ms/step - loss: 0.0902 - val_loss: 0.0411\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0344 - val_loss: 0.0280\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0243 - val_loss: 0.0196\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0206 - val_loss: 0.0144\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0151 - val_loss: 0.0111\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0141 - val_loss: 0.0090\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0121 - val_loss: 0.0076\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0102 - val_loss: 0.0069\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0092 - val_loss: 0.0065\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0086 - val_loss: 0.0063\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0083 - val_loss: 0.0064\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0079 - val_loss: 0.0067\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb400da8310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3016 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 726ms/step - loss: 0.0476 - val_loss: 0.0105\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0180 - val_loss: 0.0107\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0120 - val_loss: 0.0098\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0087 - val_loss: 0.0095\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0076 - val_loss: 0.0091\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0062 - val_loss: 0.0090\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0056 - val_loss: 0.0090\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0051 - val_loss: 0.0089\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0044 - val_loss: 0.0087\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0038 - val_loss: 0.0088\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0038 - val_loss: 0.0090\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3f1b94ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3017 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 748ms/step - loss: 0.1340 - val_loss: 0.0505\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0468 - val_loss: 0.0374\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0251 - val_loss: 0.0276\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0161 - val_loss: 0.0215\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0151 - val_loss: 0.0174\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0113 - val_loss: 0.0166\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0101 - val_loss: 0.0149\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0084 - val_loss: 0.0135\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0076 - val_loss: 0.0122\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0071 - val_loss: 0.0106\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0065 - val_loss: 0.0094\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0065 - val_loss: 0.0083\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0064 - val_loss: 0.0076\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0055 - val_loss: 0.0070\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0049 - val_loss: 0.0067\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0048 - val_loss: 0.0067\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0046 - val_loss: 0.0068\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0044 - val_loss: 0.0069\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3eeb5d940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3018 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 748ms/step - loss: 0.5453 - val_loss: 0.0661\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5013 - val_loss: 0.0620\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.1931 - val_loss: 0.0464\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.1337 - val_loss: 0.0338\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.086 - 0s 73ms/step - loss: 0.0864 - val_loss: 0.0254\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0632 - val_loss: 0.0201\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0475 - val_loss: 0.0148\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0453 - val_loss: 0.0114\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0384 - val_loss: 0.0092\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0360 - val_loss: 0.0081\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0333 - val_loss: 0.0074\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0286 - val_loss: 0.0072\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0238 - val_loss: 0.0073\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0206 - val_loss: 0.0077\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e65104c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3019 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 861ms/step - loss: 0.2919 - val_loss: 0.0096\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0696 - val_loss: 0.0193\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0229 - val_loss: 0.0274\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c7cdc670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3020 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 756ms/step - loss: 0.0402 - val_loss: 0.0098\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0313 - val_loss: 0.0077\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0191 - val_loss: 0.0083\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0151 - val_loss: 0.0106\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ccc77ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3021 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 534ms/step - loss: 0.0971 - val_loss: 0.0107\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0475 - val_loss: 0.0097\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0224 - val_loss: 0.0092\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0203 - val_loss: 0.0087\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0156 - val_loss: 0.0083\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0128 - val_loss: 0.0081\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0113 - val_loss: 0.0082\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0098 - val_loss: 0.0084\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e352fdc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3022 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 572ms/step - loss: 0.0780 - val_loss: 0.0873\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0384 - val_loss: 0.0541\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0209 - val_loss: 0.0433\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0108 - val_loss: 0.0340\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0079 - val_loss: 0.0267\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0072 - val_loss: 0.0228\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0056 - val_loss: 0.0195\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0045 - val_loss: 0.0167\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0044 - val_loss: 0.0150\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0047 - val_loss: 0.0146\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0042 - val_loss: 0.0143\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0038 - val_loss: 0.0141\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0036 - val_loss: 0.0141\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0037 - val_loss: 0.0145\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3b9b27f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3023 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 866ms/step - loss: 0.2843 - val_loss: 0.0268\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.1414 - val_loss: 0.0235\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0968 - val_loss: 0.0193\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0663 - val_loss: 0.0152\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0522 - val_loss: 0.0121\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0428 - val_loss: 0.0100\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0370 - val_loss: 0.0087\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0308 - val_loss: 0.0082\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0263 - val_loss: 0.0082\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0215 - val_loss: 0.0085\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3be675700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3024 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 695ms/step - loss: 0.0771 - val_loss: 0.0161\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0303 - val_loss: 0.0101\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0179 - val_loss: 0.0085\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0162 - val_loss: 0.0078\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0149 - val_loss: 0.0080\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0120 - val_loss: 0.0081\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb40dd4f1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3025 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.8466 - val_loss: 0.4823\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 1.0076 - val_loss: 0.3640\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.6678 - val_loss: 0.3026\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4104 - val_loss: 0.2538\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.2351 - val_loss: 0.2209\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.1427 - val_loss: 0.1937\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0941 - val_loss: 0.1667\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.0698 - val_loss: 0.1427\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.0570 - val_loss: 0.1208\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.0517 - val_loss: 0.1016\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0475 - val_loss: 0.0864\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0448 - val_loss: 0.0754\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0427 - val_loss: 0.0660\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0407 - val_loss: 0.0571\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0387 - val_loss: 0.0486\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0369 - val_loss: 0.0406\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0353 - val_loss: 0.0348\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0340 - val_loss: 0.0305\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0329 - val_loss: 0.0276\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0320 - val_loss: 0.0258\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0313 - val_loss: 0.0242\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0306 - val_loss: 0.0226\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0300 - val_loss: 0.0211\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0294 - val_loss: 0.0199\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0288 - val_loss: 0.0189\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0283 - val_loss: 0.0181\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0279 - val_loss: 0.0175\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0275 - val_loss: 0.0169\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0271 - val_loss: 0.0164\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0267 - val_loss: 0.0159\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0262 - val_loss: 0.0153\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0258 - val_loss: 0.0148\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0254 - val_loss: 0.0143\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0249 - val_loss: 0.0140\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0245 - val_loss: 0.0136\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0241 - val_loss: 0.0134\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0236 - val_loss: 0.0131\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0232 - val_loss: 0.0128\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0228 - val_loss: 0.0126\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0224 - val_loss: 0.0124\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0220 - val_loss: 0.0121\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0216 - val_loss: 0.0119\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0212 - val_loss: 0.0117\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0208 - val_loss: 0.0114\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0204 - val_loss: 0.0112\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0201 - val_loss: 0.0110\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0197 - val_loss: 0.0108\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0193 - val_loss: 0.0106\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0190 - val_loss: 0.0104\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0186 - val_loss: 0.0102\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d91813a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3026 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 764ms/step - loss: 0.2202 - val_loss: 0.0082\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0314 - val_loss: 0.0079\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0328 - val_loss: 0.0102\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0183 - val_loss: 0.0146\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3e48e6160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3027 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0665 - val_loss: 0.0465\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0359 - val_loss: 0.0439\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0347 - val_loss: 0.0408\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0172 - val_loss: 0.0359\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0094 - val_loss: 0.0305\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0114 - val_loss: 0.0243\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0095 - val_loss: 0.0202\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0096 - val_loss: 0.0187\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0093 - val_loss: 0.0184\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0076 - val_loss: 0.0187\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0061 - val_loss: 0.0192\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c42a6e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3028 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 791ms/step - loss: 0.0845 - val_loss: 0.0540\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0823 - val_loss: 0.0737\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0206 - val_loss: 0.0744\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c4e59940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3029 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 774ms/step - loss: 0.0467 - val_loss: 0.0238\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0387 - val_loss: 0.0220\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0389 - val_loss: 0.0214\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0287 - val_loss: 0.0199\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0186 - val_loss: 0.0182\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0148 - val_loss: 0.0156\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0103 - val_loss: 0.0130\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0097 - val_loss: 0.0107\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0098 - val_loss: 0.0091\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0089 - val_loss: 0.0082\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0081 - val_loss: 0.0081\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0070 - val_loss: 0.0083\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0059 - val_loss: 0.0085\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb397e9f040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3030 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 754ms/step - loss: 0.0900 - val_loss: 0.0679\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.1241 - val_loss: 0.0456\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0223 - val_loss: 0.0293\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0237 - val_loss: 0.0236\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0142 - val_loss: 0.0190\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0118 - val_loss: 0.0132\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0105 - val_loss: 0.0089\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0097 - val_loss: 0.0070\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0079 - val_loss: 0.0067\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0066 - val_loss: 0.0077\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0057 - val_loss: 0.0092\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb39a03a8b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3031 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 782ms/step - loss: 0.0708 - val_loss: 0.0590\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0364 - val_loss: 0.0382\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0217 - val_loss: 0.0306\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0167 - val_loss: 0.0236\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0119 - val_loss: 0.0210\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0105 - val_loss: 0.0201\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0098 - val_loss: 0.0202\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0082 - val_loss: 0.0207\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb39e76f040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3032 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 734ms/step - loss: 0.2741 - val_loss: 0.0408\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0948 - val_loss: 0.0350\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0556 - val_loss: 0.0315\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0380 - val_loss: 0.0312\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0316 - val_loss: 0.0300\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0295 - val_loss: 0.0297\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0263 - val_loss: 0.0291\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0240 - val_loss: 0.0276\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0219 - val_loss: 0.0243\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0208 - val_loss: 0.0217\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0176 - val_loss: 0.0191\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0159 - val_loss: 0.0173\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0146 - val_loss: 0.0160\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0130 - val_loss: 0.0150\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0120 - val_loss: 0.0143\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0109 - val_loss: 0.0137\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0101 - val_loss: 0.0131\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0093 - val_loss: 0.0127\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0084 - val_loss: 0.0120\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0078 - val_loss: 0.0111\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0071 - val_loss: 0.0102\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0065 - val_loss: 0.0091\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0061 - val_loss: 0.0083\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0058 - val_loss: 0.0076\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0053 - val_loss: 0.0072\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0049 - val_loss: 0.0068\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0044 - val_loss: 0.0068\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0041 - val_loss: 0.0069\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4319a5940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3033 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 781ms/step - loss: 0.0367 - val_loss: 0.0192\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0389 - val_loss: 0.0195\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0237 - val_loss: 0.0201\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb44443e8b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3034 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 748ms/step - loss: 0.0693 - val_loss: 0.0088\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0378 - val_loss: 0.0086\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0280 - val_loss: 0.0091\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0198 - val_loss: 0.0091\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3794c7ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3035 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 630ms/step - loss: 0.5578 - val_loss: 0.0933\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2337 - val_loss: 0.0781\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.1304 - val_loss: 0.0682\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.0447 - val_loss: 0.0584\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0276 - val_loss: 0.0519\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0266 - val_loss: 0.0422\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0240 - val_loss: 0.0341\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0191 - val_loss: 0.0281\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0171 - val_loss: 0.0238\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0168 - val_loss: 0.0206\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0164 - val_loss: 0.0182\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0159 - val_loss: 0.0168\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0145 - val_loss: 0.0161\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0139 - val_loss: 0.0159\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0131 - val_loss: 0.0159\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0125 - val_loss: 0.0159\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0124 - val_loss: 0.0158\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0118 - val_loss: 0.0158\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0108 - val_loss: 0.0159\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb379aa8a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3036 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 806ms/step - loss: 0.2239 - val_loss: 0.0418\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.1451 - val_loss: 0.0210\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0302 - val_loss: 0.0101\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0161 - val_loss: 0.0130\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0210 - val_loss: 0.0214\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb37b28bdc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3037 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 689ms/step - loss: 0.2363 - val_loss: 0.0980\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1265 - val_loss: 0.0787\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0778 - val_loss: 0.0634\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0400 - val_loss: 0.0497\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0359 - val_loss: 0.0382\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0270 - val_loss: 0.0294\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0265 - val_loss: 0.0227\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.023 - 0s 59ms/step - loss: 0.0233 - val_loss: 0.0181\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0207 - val_loss: 0.0151\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0185 - val_loss: 0.0135\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0165 - val_loss: 0.0129\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0162 - val_loss: 0.0126\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0144 - val_loss: 0.0123\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0125 - val_loss: 0.0121\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0119 - val_loss: 0.0120\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0103 - val_loss: 0.0118\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0100 - val_loss: 0.0117\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0094 - val_loss: 0.0116\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0092 - val_loss: 0.0116\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0088 - val_loss: 0.0115\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0082 - val_loss: 0.0114\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0082 - val_loss: 0.0114\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0078 - val_loss: 0.0113\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0075 - val_loss: 0.0113\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0074 - val_loss: 0.0113\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0071 - val_loss: 0.0112\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0067 - val_loss: 0.0112\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0065 - val_loss: 0.0111\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0064 - val_loss: 0.0111\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0061 - val_loss: 0.0110\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0057 - val_loss: 0.0110\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0055 - val_loss: 0.0109\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0054 - val_loss: 0.0109\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0052 - val_loss: 0.0109\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0051 - val_loss: 0.0109\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0050 - val_loss: 0.0109\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0048 - val_loss: 0.0109\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0047 - val_loss: 0.0109\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0045 - val_loss: 0.0109\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0044 - val_loss: 0.0108\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0043 - val_loss: 0.0109\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0042 - val_loss: 0.0109\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb37b957d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3038 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 770ms/step - loss: 0.0549 - val_loss: 0.0094\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0405 - val_loss: 0.0097\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0274 - val_loss: 0.0092\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0321 - val_loss: 0.0127\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0140 - val_loss: 0.0178\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3812744c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3039 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 757ms/step - loss: 0.1069 - val_loss: 0.0549\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0590 - val_loss: 0.0464\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0490 - val_loss: 0.0398\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0438 - val_loss: 0.0380\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0377 - val_loss: 0.0327\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0329 - val_loss: 0.0271\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0284 - val_loss: 0.0240\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0245 - val_loss: 0.0215\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0217 - val_loss: 0.0193\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0199 - val_loss: 0.0173\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0183 - val_loss: 0.0156\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0166 - val_loss: 0.0143\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0150 - val_loss: 0.0134\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0138 - val_loss: 0.0127\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0128 - val_loss: 0.0125\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0129 - val_loss: 0.0124\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0125 - val_loss: 0.0124\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0111 - val_loss: 0.0123\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0104 - val_loss: 0.0123\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0100 - val_loss: 0.0121\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0096 - val_loss: 0.0122\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0094 - val_loss: 0.0124\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb384385700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3040 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1030 - val_loss: 0.0666\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0592 - val_loss: 0.0455\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.0315 - val_loss: 0.0297\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0223 - val_loss: 0.0207\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0201 - val_loss: 0.0165\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0170 - val_loss: 0.0158\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0122 - val_loss: 0.0165\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0098 - val_loss: 0.0163\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ceca5940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3041 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 746ms/step - loss: 0.0459 - val_loss: 0.0174\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.0299 - val_loss: 0.0178\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0222 - val_loss: 0.0189\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4128535e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3042 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 899ms/step - loss: 0.0313 - val_loss: 0.0127\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0232 - val_loss: 0.0131\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0201 - val_loss: 0.0151\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d7cd8ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3043 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 816ms/step - loss: 0.7910 - val_loss: 0.1956\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.6062 - val_loss: 0.1680\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.3726 - val_loss: 0.1550\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2178 - val_loss: 0.1443\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.1194 - val_loss: 0.1320\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0942 - val_loss: 0.1141\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0729 - val_loss: 0.0970\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0581 - val_loss: 0.0811\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0513 - val_loss: 0.0651\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0437 - val_loss: 0.0519\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0376 - val_loss: 0.0417\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0341 - val_loss: 0.0332\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0314 - val_loss: 0.0263\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0288 - val_loss: 0.0210\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0269 - val_loss: 0.0169\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0253 - val_loss: 0.0142\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0236 - val_loss: 0.0126\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0219 - val_loss: 0.0118\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0203 - val_loss: 0.0113\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0191 - val_loss: 0.0109\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0183 - val_loss: 0.0108\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0173 - val_loss: 0.0108\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0164 - val_loss: 0.0110\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb431dce8b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3044 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 796ms/step - loss: 0.2455 - val_loss: 0.0525\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0810 - val_loss: 0.0347\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0368 - val_loss: 0.0298\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0242 - val_loss: 0.0306\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0294 - val_loss: 0.0294\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0229 - val_loss: 0.0289\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.0203 - val_loss: 0.0303\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0187 - val_loss: 0.0311\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d2f2a670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3045 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 947ms/step - loss: 0.1470 - val_loss: 0.1160\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.1040 - val_loss: 0.0442\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0382 - val_loss: 0.0175\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0298 - val_loss: 0.0115\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0329 - val_loss: 0.0116\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0251 - val_loss: 0.0134\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4463f2310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3046 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 811ms/step - loss: 0.5852 - val_loss: 0.0448\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0834 - val_loss: 0.0289\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0444 - val_loss: 0.0199\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0737 - val_loss: 0.0141\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0419 - val_loss: 0.0131\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0210 - val_loss: 0.0153\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0196 - val_loss: 0.0200\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb35bb65550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3047 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 700ms/step - loss: 0.1375 - val_loss: 0.1107\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0454 - val_loss: 0.0763\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0504 - val_loss: 0.0407\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0207 - val_loss: 0.0289\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0160 - val_loss: 0.0212\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0107 - val_loss: 0.0156\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0102 - val_loss: 0.0128\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0092 - val_loss: 0.0118\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0096 - val_loss: 0.0118\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0068 - val_loss: 0.0124\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a512d160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3048 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 598ms/step - loss: 0.0477 - val_loss: 0.0507\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0670 - val_loss: 0.0424\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0305 - val_loss: 0.0380\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0227 - val_loss: 0.0340\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.0147 - val_loss: 0.0315\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0116 - val_loss: 0.0290\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0094 - val_loss: 0.0262\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0079 - val_loss: 0.0231\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0073 - val_loss: 0.0203\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0064 - val_loss: 0.0180\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0059 - val_loss: 0.0165\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0057 - val_loss: 0.0157\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0055 - val_loss: 0.0154\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0052 - val_loss: 0.0153\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0047 - val_loss: 0.0154\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0043 - val_loss: 0.0157\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb36e1da0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3049 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 721ms/step - loss: 0.0691 - val_loss: 0.0399\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0446 - val_loss: 0.0414\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0331 - val_loss: 0.0303\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0185 - val_loss: 0.0213\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0156 - val_loss: 0.0172\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0127 - val_loss: 0.0158\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0087 - val_loss: 0.0158\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0083 - val_loss: 0.0168\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb36e9499d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3050 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 666ms/step - loss: 0.1355 - val_loss: 0.0273\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0808 - val_loss: 0.0283\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0222 - val_loss: 0.0340\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb36e9feca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3051 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 570ms/step - loss: 0.1732 - val_loss: 0.0132\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0171 - val_loss: 0.0164\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0171 - val_loss: 0.0184\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb36e6ca160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3052 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 837ms/step - loss: 0.1762 - val_loss: 0.0294\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0776 - val_loss: 0.0284\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0529 - val_loss: 0.0283\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0327 - val_loss: 0.0269\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0184 - val_loss: 0.0265\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0142 - val_loss: 0.0268\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0145 - val_loss: 0.0268\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb36de72af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3053 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 571ms/step - loss: 0.0608 - val_loss: 0.0239\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0492 - val_loss: 0.0465\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0205 - val_loss: 0.0534\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb373e61280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3054 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 663ms/step - loss: 0.3439 - val_loss: 0.2547\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.1918 - val_loss: 0.1967\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.1392 - val_loss: 0.1560\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0903 - val_loss: 0.1297\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0702 - val_loss: 0.1093\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0530 - val_loss: 0.0946\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0470 - val_loss: 0.0856\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0393 - val_loss: 0.0792\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0343 - val_loss: 0.0738\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0319 - val_loss: 0.0697\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0299 - val_loss: 0.0665\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0281 - val_loss: 0.0636\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0266 - val_loss: 0.0606\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0254 - val_loss: 0.0572\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0243 - val_loss: 0.0538\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0235 - val_loss: 0.0523\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0228 - val_loss: 0.0519\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0221 - val_loss: 0.0513\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0214 - val_loss: 0.0505\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0208 - val_loss: 0.0494\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0202 - val_loss: 0.0483\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0196 - val_loss: 0.0470\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0190 - val_loss: 0.0458\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0184 - val_loss: 0.0445\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0178 - val_loss: 0.0432\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0172 - val_loss: 0.0419\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0166 - val_loss: 0.0405\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0160 - val_loss: 0.0391\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0155 - val_loss: 0.0376\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0149 - val_loss: 0.0362\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0144 - val_loss: 0.0348\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0139 - val_loss: 0.0335\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0133 - val_loss: 0.0320\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0128 - val_loss: 0.0304\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0124 - val_loss: 0.0287\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0119 - val_loss: 0.0271\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0116 - val_loss: 0.0254\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0112 - val_loss: 0.0239\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0109 - val_loss: 0.0225\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0106 - val_loss: 0.0212\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0104 - val_loss: 0.0201\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0101 - val_loss: 0.0190\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0099 - val_loss: 0.0181\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0096 - val_loss: 0.0175\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0093 - val_loss: 0.0173\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0091 - val_loss: 0.0171\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0089 - val_loss: 0.0169\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0086 - val_loss: 0.0167\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0084 - val_loss: 0.0165\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0081 - val_loss: 0.0164\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb360e78a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3055 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 724ms/step - loss: 0.1404 - val_loss: 0.0553\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0334 - val_loss: 0.0589\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0138 - val_loss: 0.0604\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3623350d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3056 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 808ms/step - loss: 0.0857 - val_loss: 0.0359\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0273 - val_loss: 0.0365\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0149 - val_loss: 0.0350\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0132 - val_loss: 0.0336\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0091 - val_loss: 0.0318\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0076 - val_loss: 0.0316\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0063 - val_loss: 0.0317\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0057 - val_loss: 0.0305\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0056 - val_loss: 0.0295\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0053 - val_loss: 0.0285\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0049 - val_loss: 0.0277\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0044 - val_loss: 0.0268\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0038 - val_loss: 0.0262\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0036 - val_loss: 0.0256\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0032 - val_loss: 0.0251\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0031 - val_loss: 0.0246\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0029 - val_loss: 0.0241\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0028 - val_loss: 0.0236\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0027 - val_loss: 0.0231\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0026 - val_loss: 0.0228\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0026 - val_loss: 0.0228\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0026 - val_loss: 0.0231\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0025 - val_loss: 0.0235\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb36267d5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3057 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 723ms/step - loss: 0.2471 - val_loss: 0.0371\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0705 - val_loss: 0.0165\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0342 - val_loss: 0.0120\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0238 - val_loss: 0.0116\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0214 - val_loss: 0.0128\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0186 - val_loss: 0.0142\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb363abf700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3058 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 814ms/step - loss: 0.0293 - val_loss: 0.0392\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0264 - val_loss: 0.0397\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0183 - val_loss: 0.0361\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0130 - val_loss: 0.0346\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0120 - val_loss: 0.0339\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0107 - val_loss: 0.0344\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0092 - val_loss: 0.0354\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb363e47a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3059 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1132 - val_loss: 0.0804\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0376 - val_loss: 0.0735\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0326 - val_loss: 0.0610\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0237 - val_loss: 0.0515\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0239 - val_loss: 0.0439\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0192 - val_loss: 0.0397\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0127 - val_loss: 0.0368\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0105 - val_loss: 0.0353\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0094 - val_loss: 0.0334\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0083 - val_loss: 0.0308\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0075 - val_loss: 0.0290\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0069 - val_loss: 0.0277\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0065 - val_loss: 0.0268\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0063 - val_loss: 0.0263\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0062 - val_loss: 0.0262\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0061 - val_loss: 0.0261\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0058 - val_loss: 0.0262\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0054 - val_loss: 0.0263\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb36437c160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3060 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 942ms/step - loss: 0.2449 - val_loss: 0.1739\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.1422 - val_loss: 0.1536\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0741 - val_loss: 0.1069\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0359 - val_loss: 0.0758\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0198 - val_loss: 0.0574\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0214 - val_loss: 0.0441\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0183 - val_loss: 0.0360\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0144 - val_loss: 0.0315\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0124 - val_loss: 0.0293\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0108 - val_loss: 0.0278\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0090 - val_loss: 0.0269\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0077 - val_loss: 0.0263\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0072 - val_loss: 0.0259\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0065 - val_loss: 0.0256\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0054 - val_loss: 0.0255\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0049 - val_loss: 0.0256\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0048 - val_loss: 0.0257\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb36466d430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3061 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 724ms/step - loss: 0.4228 - val_loss: 0.0334\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0463 - val_loss: 0.0419\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0156 - val_loss: 0.0485\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb365a8b670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3062 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 897ms/step - loss: 0.0548 - val_loss: 0.0730\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0316 - val_loss: 0.0645\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0185 - val_loss: 0.0552\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0129 - val_loss: 0.0524\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0107 - val_loss: 0.0513\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0087 - val_loss: 0.0504\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0071 - val_loss: 0.0497\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0064 - val_loss: 0.0489\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0059 - val_loss: 0.0483\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0056 - val_loss: 0.0477\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0052 - val_loss: 0.0469\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0049 - val_loss: 0.0462\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0046 - val_loss: 0.0457\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0042 - val_loss: 0.0453\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0039 - val_loss: 0.0448\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0036 - val_loss: 0.0445\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0033 - val_loss: 0.0444\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0030 - val_loss: 0.0445\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0029 - val_loss: 0.0444\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0028 - val_loss: 0.0439\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0026 - val_loss: 0.0432\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0025 - val_loss: 0.0425\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0023 - val_loss: 0.0414\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0022 - val_loss: 0.0402\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0020 - val_loss: 0.0392\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0019 - val_loss: 0.0383\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0019 - val_loss: 0.0371\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0018 - val_loss: 0.0359\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0017 - val_loss: 0.0351\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0017 - val_loss: 0.0345\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0016 - val_loss: 0.0337\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0015 - val_loss: 0.0330\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0014 - val_loss: 0.0321\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.0014 - val_loss: 0.0315\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0013 - val_loss: 0.0309\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0012 - val_loss: 0.0302\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0012 - val_loss: 0.0295\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0011 - val_loss: 0.0289\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0011 - val_loss: 0.0283\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 9.9049e-04 - val_loss: 0.0278\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 9.6851e-04 - val_loss: 0.0271\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 9.1354e-04 - val_loss: 0.0263\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 8.7292e-04 - val_loss: 0.0256\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 8.3241e-04 - val_loss: 0.0251\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 7.8762e-04 - val_loss: 0.0248\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 7.5190e-04 - val_loss: 0.0246\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 7.0750e-04 - val_loss: 0.0243\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 6.7467e-04 - val_loss: 0.0238\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 6.5034e-0 - 0s 66ms/step - loss: 6.5034e-04 - val_loss: 0.0234\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 6.0991e-04 - val_loss: 0.0229\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb365e5cc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3063 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 772ms/step - loss: 0.2680 - val_loss: 0.4391\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0588 - val_loss: 0.3189\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0191 - val_loss: 0.2463\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0133 - val_loss: 0.2012\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0127 - val_loss: 0.1659\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0136 - val_loss: 0.1379\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0129 - val_loss: 0.1177\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0107 - val_loss: 0.1031\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0085 - val_loss: 0.0918\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0080 - val_loss: 0.0817\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0073 - val_loss: 0.0723\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0063 - val_loss: 0.0642\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0064 - val_loss: 0.0579\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0063 - val_loss: 0.0532\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0057 - val_loss: 0.0500\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0055 - val_loss: 0.0477\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0054 - val_loss: 0.0461\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0051 - val_loss: 0.0449\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0047 - val_loss: 0.0437\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0048 - val_loss: 0.0424\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0044 - val_loss: 0.0411\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0042 - val_loss: 0.0399\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0040 - val_loss: 0.0389\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0039 - val_loss: 0.0386\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0038 - val_loss: 0.0387\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0036 - val_loss: 0.0390\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3494110d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3064 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 820ms/step - loss: 0.0534 - val_loss: 0.0310\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0197 - val_loss: 0.0358\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0105 - val_loss: 0.0382\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb34a12a280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3065 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 774ms/step - loss: 0.6866 - val_loss: 0.0618\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.1386 - val_loss: 0.0555\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0533 - val_loss: 0.0455\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0206 - val_loss: 0.0386\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0138 - val_loss: 0.0357\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0179 - val_loss: 0.0342\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0209 - val_loss: 0.0334\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0188 - val_loss: 0.0333\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0142 - val_loss: 0.0340\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0101 - val_loss: 0.0351\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb34b316670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3066 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 705ms/step - loss: 0.0355 - val_loss: 0.0120\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0207 - val_loss: 0.0139\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0279 - val_loss: 0.0139\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb34cc348b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3067 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 742ms/step - loss: 0.4299 - val_loss: 0.2085\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.1344 - val_loss: 0.1336\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0532 - val_loss: 0.0977\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0362 - val_loss: 0.0726\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0262 - val_loss: 0.0568\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0184 - val_loss: 0.0451\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0130 - val_loss: 0.0368\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0131 - val_loss: 0.0331\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0135 - val_loss: 0.0312\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0130 - val_loss: 0.0294\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0119 - val_loss: 0.0275\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0104 - val_loss: 0.0260\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0084 - val_loss: 0.0251\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0074 - val_loss: 0.0253\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0069 - val_loss: 0.0255\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb34ef150d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3068 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 907ms/step - loss: 0.0500 - val_loss: 0.0399\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0263 - val_loss: 0.0321\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.024 - 0s 70ms/step - loss: 0.0240 - val_loss: 0.0229\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0114 - val_loss: 0.0137\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0100 - val_loss: 0.0087\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0112 - val_loss: 0.0058\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0083 - val_loss: 0.0059\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0064 - val_loss: 0.0066\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb34f543b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3069 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1116 - val_loss: 0.0976\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0690 - val_loss: 0.0838\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0319 - val_loss: 0.0694\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.0159 - val_loss: 0.0561\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0198 - val_loss: 0.0446\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0099 - val_loss: 0.0378\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0089 - val_loss: 0.0378\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0079 - val_loss: 0.0395\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb350c20820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3070 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3685 - val_loss: 0.0754\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.1034 - val_loss: 0.0635\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0329 - val_loss: 0.0560\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0313 - val_loss: 0.0439\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0214 - val_loss: 0.0353\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.0219 - val_loss: 0.0294\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0165 - val_loss: 0.0253\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0135 - val_loss: 0.0218\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0123 - val_loss: 0.0204\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0119 - val_loss: 0.0193\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0098 - val_loss: 0.0181\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0089 - val_loss: 0.0171\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0083 - val_loss: 0.0163\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0079 - val_loss: 0.0157\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0076 - val_loss: 0.0155\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0074 - val_loss: 0.0154\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0073 - val_loss: 0.0154\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0069 - val_loss: 0.0154\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3520bc0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3071 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 796ms/step - loss: 0.0893 - val_loss: 0.0244\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0524 - val_loss: 0.0309\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0418 - val_loss: 0.0352\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb35415f0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3072 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 615ms/step - loss: 0.0149 - val_loss: 0.0258\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.0154 - val_loss: 0.0232\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0129 - val_loss: 0.0240\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0092 - val_loss: 0.0248\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3dbcd3700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3073 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 743ms/step - loss: 0.1939 - val_loss: 0.2367\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.1034 - val_loss: 0.1900\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0556 - val_loss: 0.1630\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0253 - val_loss: 0.1393\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0137 - val_loss: 0.1176\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0151 - val_loss: 0.0997\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0129 - val_loss: 0.0836\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0109 - val_loss: 0.0697\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0100 - val_loss: 0.0587\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0094 - val_loss: 0.0505\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0086 - val_loss: 0.0448\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0079 - val_loss: 0.0406\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0074 - val_loss: 0.0373\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0071 - val_loss: 0.0347\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0072 - val_loss: 0.0324\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0069 - val_loss: 0.0302\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0061 - val_loss: 0.0285\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0056 - val_loss: 0.0273\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0052 - val_loss: 0.0267\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0047 - val_loss: 0.0266\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0042 - val_loss: 0.0265\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0039 - val_loss: 0.0264\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0037 - val_loss: 0.0262\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0036 - val_loss: 0.0260\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0036 - val_loss: 0.0255\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0035 - val_loss: 0.0248\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0033 - val_loss: 0.0240\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.0032 - val_loss: 0.0232\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0031 - val_loss: 0.0227\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0029 - val_loss: 0.0226\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0027 - val_loss: 0.0229\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0026 - val_loss: 0.0235\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3ae4423a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3074 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0311 - val_loss: 0.0568\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.0180 - val_loss: 0.0446\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0108 - val_loss: 0.0373\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0085 - val_loss: 0.0342\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0063 - val_loss: 0.0325\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0058 - val_loss: 0.0305\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0054 - val_loss: 0.0277\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0038 - val_loss: 0.0250\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0034 - val_loss: 0.0226\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0036 - val_loss: 0.0213\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0034 - val_loss: 0.0209\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0030 - val_loss: 0.0209\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0027 - val_loss: 0.0208\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0025 - val_loss: 0.0205\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0025 - val_loss: 0.0199\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0024 - val_loss: 0.0189\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0023 - val_loss: 0.0179\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.0023 - val_loss: 0.0172\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0022 - val_loss: 0.0169\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0021 - val_loss: 0.0168\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0020 - val_loss: 0.0169\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0019 - val_loss: 0.0171\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb400da8af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3075 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 813ms/step - loss: 0.0352 - val_loss: 0.0589\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0216 - val_loss: 0.0549\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0222 - val_loss: 0.0495\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0153 - val_loss: 0.0457\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0073 - val_loss: 0.0416\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0082 - val_loss: 0.0381\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0070 - val_loss: 0.0357\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0061 - val_loss: 0.0351\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0061 - val_loss: 0.0351\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0055 - val_loss: 0.0357\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0047 - val_loss: 0.0362\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4002224c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3076 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 936ms/step - loss: 0.1425 - val_loss: 0.0744\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0170 - val_loss: 0.0784\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0132 - val_loss: 0.0809\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3c53dc8b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3077 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 839ms/step - loss: 0.0641 - val_loss: 0.0448\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0147 - val_loss: 0.0456\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0109 - val_loss: 0.0451\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3a61b7b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3078 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1156 - val_loss: 0.0495\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.0263 - val_loss: 0.0438\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 0.0188 - val_loss: 0.0419\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.0162 - val_loss: 0.0406\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0148 - val_loss: 0.0394\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0125 - val_loss: 0.0387\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0105 - val_loss: 0.0382\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0088 - val_loss: 0.0376\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0078 - val_loss: 0.0364\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.0072 - val_loss: 0.0349\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0066 - val_loss: 0.0340\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0059 - val_loss: 0.0332\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0052 - val_loss: 0.0326\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0049 - val_loss: 0.0321\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0050 - val_loss: 0.0318\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0048 - val_loss: 0.0316\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0045 - val_loss: 0.0315\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0042 - val_loss: 0.0315\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0039 - val_loss: 0.0315\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0037 - val_loss: 0.0315\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb395564310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3079 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 621ms/step - loss: 0.0728 - val_loss: 0.0169\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0606 - val_loss: 0.0164\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0262 - val_loss: 0.0172\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0142 - val_loss: 0.0193\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb4d27c3310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3080 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 674ms/step - loss: 0.2073 - val_loss: 0.1115\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0689 - val_loss: 0.0869\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0371 - val_loss: 0.0693\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0219 - val_loss: 0.0552\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0178 - val_loss: 0.0478\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0135 - val_loss: 0.0454\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0112 - val_loss: 0.0457\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0100 - val_loss: 0.0418\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0086 - val_loss: 0.0364\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0070 - val_loss: 0.0335\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0061 - val_loss: 0.0338\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0056 - val_loss: 0.0331\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0052 - val_loss: 0.0316\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0044 - val_loss: 0.0297\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0039 - val_loss: 0.0282\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0036 - val_loss: 0.0272\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0034 - val_loss: 0.0265\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0032 - val_loss: 0.0263\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0031 - val_loss: 0.0264\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0030 - val_loss: 0.0263\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb3d01c05e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3081 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 709ms/step - loss: 0.1663 - val_loss: 0.0976\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0508 - val_loss: 0.0582\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0204 - val_loss: 0.0455\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0150 - val_loss: 0.0373\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0141 - val_loss: 0.0319\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0127 - val_loss: 0.0323\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0116 - val_loss: 0.0323\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb35008f1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3082 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 860ms/step - loss: 0.0989 - val_loss: 0.0408\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0375 - val_loss: 0.0340\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0291 - val_loss: 0.0294\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0207 - val_loss: 0.0263\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0215 - val_loss: 0.0241\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0153 - val_loss: 0.0225\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0139 - val_loss: 0.0215\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0141 - val_loss: 0.0212\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0133 - val_loss: 0.0212\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0116 - val_loss: 0.0209\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0101 - val_loss: 0.0206\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0088 - val_loss: 0.0202\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0077 - val_loss: 0.0199\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0067 - val_loss: 0.0196\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0059 - val_loss: 0.0194\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0051 - val_loss: 0.0192\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0046 - val_loss: 0.0190\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0042 - val_loss: 0.0188\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0038 - val_loss: 0.0186\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0034 - val_loss: 0.0185\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0033 - val_loss: 0.0184\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0031 - val_loss: 0.0183\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0030 - val_loss: 0.0182\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0030 - val_loss: 0.0180\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0030 - val_loss: 0.0179\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0028 - val_loss: 0.0177\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0028 - val_loss: 0.0176\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0027 - val_loss: 0.0175\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0025 - val_loss: 0.0175\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0024 - val_loss: 0.0175\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0022 - val_loss: 0.0177\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb34d61c670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3083 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 526ms/step - loss: 0.1491 - val_loss: 0.0963\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0703 - val_loss: 0.0662\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0215 - val_loss: 0.0543\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0187 - val_loss: 0.0514\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0143 - val_loss: 0.0495\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0140 - val_loss: 0.0482\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0143 - val_loss: 0.0466\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0117 - val_loss: 0.0447\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0102 - val_loss: 0.0431\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0091 - val_loss: 0.0419\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0080 - val_loss: 0.0412\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0073 - val_loss: 0.0409\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0066 - val_loss: 0.0407\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0055 - val_loss: 0.0398\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0054 - val_loss: 0.0382\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0055 - val_loss: 0.0369\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0053 - val_loss: 0.0362\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0048 - val_loss: 0.0356\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0042 - val_loss: 0.0352\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0040 - val_loss: 0.0349\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0039 - val_loss: 0.0347\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0038 - val_loss: 0.0346\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0036 - val_loss: 0.0347\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0034 - val_loss: 0.0348\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb34b847ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3084 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5507 - val_loss: 0.3603\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2726 - val_loss: 0.2787\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.1333 - val_loss: 0.2171\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.0891 - val_loss: 0.1737\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0661 - val_loss: 0.1426\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0488 - val_loss: 0.1177\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0377 - val_loss: 0.0969\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.033 - 0s 72ms/step - loss: 0.0330 - val_loss: 0.0828\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0288 - val_loss: 0.0746\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0259 - val_loss: 0.0678\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0236 - val_loss: 0.0617\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0214 - val_loss: 0.0567\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0195 - val_loss: 0.0526\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0180 - val_loss: 0.0481\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0165 - val_loss: 0.0442\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0157 - val_loss: 0.0413\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0150 - val_loss: 0.0389\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0147 - val_loss: 0.0369\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0142 - val_loss: 0.0352\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.013 - 0s 82ms/step - loss: 0.0133 - val_loss: 0.0342\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0125 - val_loss: 0.0335\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.0116 - val_loss: 0.0331\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0113 - val_loss: 0.0326\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0111 - val_loss: 0.0322\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0109 - val_loss: 0.0318\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0107 - val_loss: 0.0314\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0106 - val_loss: 0.0311\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0103 - val_loss: 0.0308\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0098 - val_loss: 0.0305\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0096 - val_loss: 0.0302\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0094 - val_loss: 0.0300\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0092 - val_loss: 0.0297\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0090 - val_loss: 0.0295\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0089 - val_loss: 0.0294\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0087 - val_loss: 0.0293\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0086 - val_loss: 0.0291\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0084 - val_loss: 0.0288\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0082 - val_loss: 0.0285\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0081 - val_loss: 0.0282\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0079 - val_loss: 0.0279\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0077 - val_loss: 0.0276\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0075 - val_loss: 0.0272\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.0073 - val_loss: 0.0270\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0071 - val_loss: 0.0268\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0068 - val_loss: 0.0267\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0066 - val_loss: 0.0266\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0064 - val_loss: 0.0265\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0061 - val_loss: 0.0264\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0059 - val_loss: 0.0264\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0057 - val_loss: 0.0264\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb34a64c790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3085 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0818 - val_loss: 0.0285\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0589 - val_loss: 0.0270\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0340 - val_loss: 0.0268\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0189 - val_loss: 0.0269\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0113 - val_loss: 0.0273\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb349564f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3086 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.0291 - val_loss: 0.0532\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0243 - val_loss: 0.0512\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0138 - val_loss: 0.0428\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0154 - val_loss: 0.0386\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0110 - val_loss: 0.0350\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0094 - val_loss: 0.0324\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0083 - val_loss: 0.0305\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0075 - val_loss: 0.0298\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0066 - val_loss: 0.0294\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0061 - val_loss: 0.0295\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0055 - val_loss: 0.0298\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb365bce160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer layer_normalization_3087 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = []\n",
    "preds = []\n",
    "seed(1)\n",
    "for retrain_idx in range(552):\n",
    "    X = Xl.iloc[t_train_start[retrain_idx]:t_train_end[retrain_idx],:]\n",
    "    X_idx = X.apply(pd.Series.nunique) != 1\n",
    "    X = X.loc[:,X_idx]\n",
    "    X_val = Xl.loc[t_val_start[retrain_idx]:t_val_end[retrain_idx],X_idx]\n",
    "    X_test = Xl.loc[t_test_start[retrain_idx]:t_test_end[retrain_idx],X_idx]\n",
    "    y = y_agg.iloc[t_train_start[retrain_idx]:t_train_end[retrain_idx],:]\n",
    "    y_val = y_agg.loc[t_val_start[retrain_idx]:t_val_end[retrain_idx],:]\n",
    "    y_test = y_agg.loc[t_test_start[retrain_idx]:t_test_end[retrain_idx],:]\n",
    "    model = Sequential()\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(64,activation='tanh'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(32,activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(16,activation='relu'))\n",
    "    model.add(Dense(16,activation='relu'))\n",
    "    model.add(Dense(16,activation='relu'))\n",
    "    model.add(Dense(8,activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    optimizer = Adam(learning_rate=0.003)\n",
    "    model.compile(optimizer=optimizer,loss='mse')\n",
    "    early_stop = EarlyStopping(monitor='val_loss', verbose=0, patience=2)\n",
    "    history = model.fit(x=X,y=y,validation_data=(X_val,y_val), batch_size=128, epochs=50,verbose=1,callbacks=[early_stop])\n",
    "    preds.append(model.predict(X_test))\n",
    "    loss.append(min(history.history['val_loss']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_org = y_agg.loc[t_test_start[0]:t_test_end[551],:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-np.sum((np.squeeze(y_org)-np.squeeze(preds))**2)/np.sum((y_org-np.mean(y_org))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A good R^2 is expected to be between 0 to 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Finance.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
